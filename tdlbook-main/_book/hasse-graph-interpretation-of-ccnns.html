<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>第 8 章 CCNNs的Hasse图解释 | 拓扑深度学习：超越图数据</title>
  <meta name="description" content="一本关于拓扑深度学习的书。" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="第 8 章 CCNNs的Hasse图解释 | 拓扑深度学习：超越图数据" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/figures/unifying.png" />
  <meta property="og:description" content="一本关于拓扑深度学习的书。" />
  <meta name="github-repo" content="pyt-team/tdlbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 8 章 CCNNs的Hasse图解释 | 拓扑深度学习：超越图数据" />
  
  <meta name="twitter:description" content="一本关于拓扑深度学习的书。" />
  <meta name="twitter:image" content="/figures/unifying.png" />

<meta name="author" content="Mustafa Hajij, Theodore Papamarkou, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Michael T. Schaub" />


<meta name="date" content="2024-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="push-forward-pooling-and-unpooling.html"/>
<link rel="next" href="implementation-and-numerical-results.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/glossarybox.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">拓扑深度学习</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>贡献者</a></li>
<li class="chapter" data-level="" data-path="译者.html"><a href="译者.html"><i class="fa fa-check"></i>译者</a></li>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html"><i class="fa fa-check"></i>序言</a>
<ul>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html#编译"><i class="fa fa-check"></i>编译</a></li>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html#致谢"><i class="fa fa-check"></i>致谢</a></li>
</ul></li>
<li class="part"><span><b>第一部分：基础知识</b></span></li>
<li class="chapter" data-level="1" data-path="引言.html"><a href="引言.html"><i class="fa fa-check"></i><b>1</b> 引言</a></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> 研究动机</a>
<ul>
<li class="chapter" data-level="2.1" data-path="motivation.html"><a href="motivation.html#从拓扑空间数据中建模和学习"><i class="fa fa-check"></i><b>2.1</b> 从拓扑空间数据中建模和学习</a></li>
<li class="chapter" data-level="2.2" data-path="motivation.html"><a href="motivation.html#the-utility-of-topology"><i class="fa fa-check"></i><b>2.2</b> 拓扑的有用性</a></li>
<li class="chapter" data-level="2.3" data-path="motivation.html"><a href="motivation.html#深度学习和结构化计算的统一视角"><i class="fa fa-check"></i><b>2.3</b> 深度学习和结构化计算的统一视角</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>3</b> 预备知识</a>
<ul>
<li class="chapter" data-level="3.1" data-path="preliminaries.html"><a href="preliminaries.html#邻域函数和拓扑空间"><i class="fa fa-check"></i><b>3.1</b> 邻域函数和拓扑空间</a></li>
<li class="chapter" data-level="3.2" data-path="preliminaries.html"><a href="preliminaries.html#bridging-the-gap-among-higher-order-networks"><i class="fa fa-check"></i><b>3.2</b> 填补与高阶网络间的代沟</a></li>
<li class="chapter" data-level="3.3" data-path="preliminaries.html"><a href="preliminaries.html#hierarchical-structure-and-set-type-relations"><i class="fa fa-check"></i><b>3.3</b> 层次化结构与集合型关系</a></li>
</ul></li>
<li class="part"><span><b>第二部分:组合复形</b></span></li>
<li class="chapter" data-level="4" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html"><i class="fa fa-check"></i><b>4</b> 组合复形</a>
<ul>
<li class="chapter" data-level="4.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc-definition"><i class="fa fa-check"></i><b>4.1</b> 组合复形定义</a></li>
<li class="chapter" data-level="4.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc-homomorphisms-and-sub-ccs"><i class="fa fa-check"></i><b>4.2</b> CC同态和子CCs</a></li>
<li class="chapter" data-level="4.3" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#motivation-for-ccs"><i class="fa fa-check"></i><b>4.3</b> 引入CCs的动机</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#pooling-operations-on-ccs"><i class="fa fa-check"></i><b>4.3.1</b> CCs上的池化操作</a></li>
<li class="chapter" data-level="4.3.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#structural-advantages-of-ccs"><i class="fa fa-check"></i><b>4.3.2</b> CCs的结构化优势</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#neighbourhood-functions-on-ccs"><i class="fa fa-check"></i><b>4.4</b> CCs上的邻域函数</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#incidence-in-a-cc"><i class="fa fa-check"></i><b>4.4.1</b> CC中的关联关系（Incidence）</a></li>
<li class="chapter" data-level="4.4.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc内的邻接关系adjacency"><i class="fa fa-check"></i><b>4.4.2</b> CC内的邻接关系（Adjacency）</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#data-on-ccs"><i class="fa fa-check"></i><b>4.5</b> CCs上的数据</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html"><i class="fa fa-check"></i><b>5</b> 组合复形神经网络（Combinatorial complex neural networks）</a>
<ul>
<li class="chapter" data-level="5.1" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#building-ccnns-tensor-diagrams"><i class="fa fa-check"></i><b>5.1</b> 构建 CCNN：张量图</a></li>
<li class="chapter" data-level="5.2" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#push-forward-operator-and-merge-node"><i class="fa fa-check"></i><b>5.2</b> 前推操作（Push-forward operator）和聚合节点</a></li>
<li class="chapter" data-level="5.3" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#the-main-three-tensor-operations"><i class="fa fa-check"></i><b>5.3</b> 三种主要的张量操作</a></li>
<li class="chapter" data-level="5.4" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#definition-of-combinatorial-complex-convolutional-networks"><i class="fa fa-check"></i><b>5.4</b> 组合复形卷积网络的定义（combinatorial complex convolutional networks）</a></li>
<li class="chapter" data-level="5.5" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks"><i class="fa fa-check"></i><b>5.5</b> 组合复形注意力神经网络</a></li>
</ul></li>
<li class="part"><span><b>第三部分：高阶消息传递（Higher-order message passing）</b></span></li>
<li class="chapter" data-level="6" data-path="message-passing.html"><a href="message-passing.html"><i class="fa fa-check"></i><b>6</b> 消息传递</a>
<ul>
<li class="chapter" data-level="6.1" data-path="message-passing.html"><a href="message-passing.html#definition-of-higher-order-message-passing"><i class="fa fa-check"></i><b>6.1</b> 高阶消息传递的定义</a></li>
<li class="chapter" data-level="6.2" data-path="message-passing.html"><a href="message-passing.html#higher-order-message-passing-neural-networks-are-ccnns"><i class="fa fa-check"></i><b>6.2</b> 高阶消息传递神经网络就是CCNNs</a></li>
<li class="chapter" data-level="6.3" data-path="message-passing.html"><a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison"><i class="fa fa-check"></i><b>6.3</b> 聚合节点和高阶消息传递：量化比较</a></li>
<li class="chapter" data-level="6.4" data-path="message-passing.html"><a href="message-passing.html#attention-higher-order-message-passing-and-ccanns"><i class="fa fa-check"></i><b>6.4</b> 注意力高阶消息传递和CCANNs</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html"><i class="fa fa-check"></i><b>7</b> 前推、池化和反池化</a>
<ul>
<li class="chapter" data-level="7.1" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#cc-pooling-and-unpooling"><i class="fa fa-check"></i><b>7.1</b> CC池化和反池化</a></li>
<li class="chapter" data-level="7.2" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#formulating-common-pooling-operations-as-cc-pooling"><i class="fa fa-check"></i><b>7.2</b> 将常见的池化操作表述为 CC-pooling</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#graph-pooling-as-cc-pooling"><i class="fa fa-check"></i><b>7.2.1</b> 用CC-pooling表示图池化操作</a></li>
<li class="chapter" data-level="7.2.2" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#image-pooling-as-cc-pooling"><i class="fa fa-check"></i><b>7.2.2</b> 图像池化作为CC-pooing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#pooling-and-unpooling-ccnns"><i class="fa fa-check"></i><b>7.3</b> 池化与反池化CCNNs</a></li>
<li class="chapter" data-level="7.4" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#mapper-and-the-cc-pooling-operation"><i class="fa fa-check"></i><b>7.4</b> 映射器和CC池化操作</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html"><i class="fa fa-check"></i><b>8</b> CCNNs的Hasse图解释</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#hasse-graph-interpretation-of-ccnns"><i class="fa fa-check"></i><b>8.1</b> CCNNs的Hasse图解释</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#ccs-as-hasse-graphs"><i class="fa fa-check"></i><b>8.1.1</b> CCs作为Hasse图</a></li>
<li class="chapter" data-level="8.1.2" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#augmented-hasse-graphs"><i class="fa fa-check"></i><b>8.1.2</b> Augmented Hasse graphs</a></li>
<li class="chapter" data-level="8.1.3" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#reducibility-of-ccnns-to-graph-based-models"><i class="fa fa-check"></i><b>8.1.3</b> Reducibility of CCNNs to graph-based models</a></li>
<li class="chapter" data-level="8.1.4" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#augmented-hasse-graphs-and-cc-pooling"><i class="fa fa-check"></i><b>8.1.4</b> Augmented Hasse graphs and CC-pooling</a></li>
<li class="chapter" data-level="8.1.5" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#augmented-hasse-diagrams-message-passing-and-merge-nodes"><i class="fa fa-check"></i><b>8.1.5</b> Augmented Hasse diagrams, message passing and merge nodes</a></li>
<li class="chapter" data-level="8.1.6" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#higher-order-representation-learning"><i class="fa fa-check"></i><b>8.1.6</b> Higher-order representation learning</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#on-the-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2</b> On the equivariance of CCNNs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#permutation-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2.1</b> Permutation equivariance of CCNNs</a></li>
<li class="chapter" data-level="8.2.2" data-path="hasse-graph-interpretation-of-ccnns.html"><a href="hasse-graph-interpretation-of-ccnns.html#orientation-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2.2</b> Orientation equivariance of CCNNs</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part IV: Applications, literature and conclusions</b></span></li>
<li class="chapter" data-level="9" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html"><i class="fa fa-check"></i><b>9</b> Implementation and numerical results</a>
<ul>
<li class="chapter" data-level="9.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#software-toponetx-topoembedx-and-topomodelx"><i class="fa fa-check"></i><b>9.1</b> Software: TopoNetX, TopoEmbedX, and TopoModelX</a></li>
<li class="chapter" data-level="9.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#datasets"><i class="fa fa-check"></i><b>9.2</b> Datasets</a></li>
<li class="chapter" data-level="9.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#shape-analysis-mesh-segmentation-and-classification"><i class="fa fa-check"></i><b>9.3</b> Shape analysis: mesh segmentation and classification</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-segmentation"><i class="fa fa-check"></i><b>9.3.1</b> Mesh segmentation</a></li>
<li class="chapter" data-level="9.3.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-and-point-cloud-classification"><i class="fa fa-check"></i><b>9.3.2</b> Mesh and point cloud classification</a></li>
<li class="chapter" data-level="9.3.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#graph-classification"><i class="fa fa-check"></i><b>9.3.3</b> Graph classification</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#pooling-with-mapper-on-graphs-and-data-classification"><i class="fa fa-check"></i><b>9.4</b> Pooling with mapper on graphs and data classification</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-classification-cc-pooling-with-input-vertex-and-edge-features"><i class="fa fa-check"></i><b>9.4.1</b> Mesh classification: CC-pooling with input vertex and edge features</a></li>
<li class="chapter" data-level="9.4.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-classification-cc-pooling-with-input-vertex-features-only"><i class="fa fa-check"></i><b>9.4.2</b> Mesh classification: CC-pooling with input vertex features only</a></li>
<li class="chapter" data-level="9.4.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#point-cloud-classification-cc-pooling-with-input-vertex-features-only"><i class="fa fa-check"></i><b>9.4.3</b> Point cloud classification: CC-pooling with input vertex features only</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#ablation-studies"><i class="fa fa-check"></i><b>9.5</b> Ablation studies</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>10</b> Related work</a>
<ul>
<li class="chapter" data-level="10.1" data-path="related-work.html"><a href="related-work.html#graph-based-models"><i class="fa fa-check"></i><b>10.1</b> Graph-based models</a></li>
<li class="chapter" data-level="10.2" data-path="related-work.html"><a href="related-work.html#higher-order-deep-learning-models"><i class="fa fa-check"></i><b>10.2</b> Higher-order deep learning models</a></li>
<li class="chapter" data-level="10.3" data-path="related-work.html"><a href="related-work.html#attention-based-models"><i class="fa fa-check"></i><b>10.3</b> Attention-based models</a></li>
<li class="chapter" data-level="10.4" data-path="related-work.html"><a href="related-work.html#graph-based-pooling"><i class="fa fa-check"></i><b>10.4</b> Graph-based pooling</a></li>
<li class="chapter" data-level="10.5" data-path="related-work.html"><a href="related-work.html#applied-algebraic-topology"><i class="fa fa-check"></i><b>10.5</b> Applied algebraic topology</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>11</b> Conclusions</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>A</b> 术语</a></li>
<li class="chapter" data-level="B" data-path="lifting-maps.html"><a href="lifting-maps.html"><i class="fa fa-check"></i><b>B</b> Lifting maps</a>
<ul>
<li class="chapter" data-level="B.1" data-path="lifting-maps.html"><a href="lifting-maps.html#n-hop-cc-of-a-graph"><i class="fa fa-check"></i><b>B.1</b> n-hop CC of a graph</a></li>
<li class="chapter" data-level="B.2" data-path="lifting-maps.html"><a href="lifting-maps.html#path-based-and-subgraph-based-cc-of-a-graph"><i class="fa fa-check"></i><b>B.2</b> Path-based and subgraph-based CC of a graph</a></li>
<li class="chapter" data-level="B.3" data-path="lifting-maps.html"><a href="lifting-maps.html#loop-based-cc-of-a-graph"><i class="fa fa-check"></i><b>B.3</b> Loop-based CC of a graph</a></li>
<li class="chapter" data-level="B.4" data-path="lifting-maps.html"><a href="lifting-maps.html#coface-cc-of-a-simplicial-complex-or-of-a-cc"><i class="fa fa-check"></i><b>B.4</b> Coface CC of a simplicial complex or of a CC</a></li>
<li class="chapter" data-level="B.5" data-path="lifting-maps.html"><a href="lifting-maps.html#augmentation-of-ccs-by-higher-rank-cells"><i class="fa fa-check"></i><b>B.5</b> Augmentation of CCs by higher-rank cells</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="ccnn-architecture-search-and-topological-quantum-field-theories.html"><a href="ccnn-architecture-search-and-topological-quantum-field-theories.html"><i class="fa fa-check"></i><b>C</b> CCNN architecture search and topological quantum field theories</a></li>
<li class="chapter" data-level="D" data-path="learning-discrete-exterior-calculus-operators-with-ccanns.html"><a href="learning-discrete-exterior-calculus-operators-with-ccanns.html"><i class="fa fa-check"></i><b>D</b> Learning discrete exterior calculus operators with CCANNs</a></li>
<li class="chapter" data-level="E" data-path="a-mapper-induced-topology-preserving-cc-pooling-operation.html"><a href="a-mapper-induced-topology-preserving-cc-pooling-operation.html"><i class="fa fa-check"></i><b>E</b> A mapper-induced topology-preserving CC-pooling operation</a></li>
<li class="chapter" data-level="" data-path="参考文献.html"><a href="参考文献.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">拓扑深度学习：超越图数据</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hasse-graph-interpretation-of-ccnns" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">第 8 章</span> CCNNs的Hasse图解释<a href="hasse-graph-interpretation-of-ccnns.html#hasse-graph-interpretation-of-ccnns" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>在本节中，我们将通过与已有的 GNN研究成果建立联系，仔细研究拓扑学习机的特性。首先将CCs解释为专门的图，例如哈斯图（<em>Hasse graphs</em>），然后针对置换和方向的作用描述它们的等变特性，还将进一步将等变性定义与Hasse图表示法下的传统等变定义联系起来。</p>
<blockquote>
<p>译者注：Hasse graphs，用来表示有限偏序集的一种数学图表，它是一种图形形式的对偏序集的传递简约。具体的说，对于偏序集合<span class="math inline">\((S, ≤)\)</span>，把<span class="math inline">\(S\)</span>的每个元素表示为平面上的顶点，并绘制从<span class="math inline">\(x\)</span>到<span class="math inline">\(y\)</span>向上的线段或弧线，只要<span class="math inline">\(y\)</span> 覆盖<span class="math inline">\(x\)</span>(就是说，只要<span class="math inline">\(x &lt; y\)</span>并且没有<span class="math inline">\(z\)</span>使得<span class="math inline">\(x &lt; z &lt; y\)</span>)。这些弧线可以相互交叉但不能触及任何非其端点的顶点。带有标注的顶点的这种图唯一确定这个集合的偏序。</p>
</blockquote>
<div id="hasse-graph-interpretation-of-ccnns" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> CCNNs的Hasse图解释<a href="hasse-graph-interpretation-of-ccnns.html#hasse-graph-interpretation-of-ccnns" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>我们将首先阐明，每个 CC 都可以规约为一个唯一且特定的图，即 <em>Hasse 图</em>。通过这种规约，就可以用基于图的模型来分析和理解 CCNN 的各种计算和概念。</p>
<div id="ccs-as-hasse-graphs" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> CCs作为Hasse图<a href="hasse-graph-interpretation-of-ccnns.html#ccs-as-hasse-graphs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>定义 <a href="combinatorial-complexes.html#def:maps">4.2</a>表明CC是偏序的（poset）, 是一个部分有序集合，其部分有序关系是集合包含关系。 这也意味着，当且仅当两个 CC 的 偏序关系 等价时，它们才是等价的<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>。 定义<a href="hasse-graph-interpretation-of-ccnns.html#def:hg">8.1</a> 给CC引入了<em>Hasse图</em> <span class="citation">(<a href="#ref-wachs2006poset">Wachs 2006</a>; <a href="#ref-abramenko2008buildings">Abramenko and Brown 2008</a>)</span>，它是和有限偏序关系关联的有向图。</p>
<div class="definition">
<p><span id="def:hg" class="definition"><strong>定义 8.1  (Hasse 图) </strong></span>CC <span class="math inline">\((S, \mathcal{X},\mbox{rk})\)</span> 的<em>Hasse图</em>是有向图 <span class="math inline">\(\mathcal{H}_{\mathcal{X}}= (V (\mathcal{H}_{\mathcal{X}}), E(\mathcal{H}_{\mathcal{X}}) )\)</span>，顶点集为<span class="math inline">\(V (\mathcal{H}_{\mathcal{X}})=\mathcal{X}\)</span>，边集为 <span class="math inline">\(E(\mathcal{H}_{\mathcal{X}})=\{ (x,y) : x\subsetneq y, \mbox{rk}(x)=\mbox{rk}(y)-1 \}\)</span>。</p>
</div>
<p>CC <span class="math inline">\((S, \mathcal{X},\mbox{rk})\)</span>的Hasse图 <span class="math inline">\(\mathcal{H}_{\mathcal{X}}\)</span>的顶点是<span class="math inline">\(\mathcal{X}\)</span>中的胞腔，<span class="math inline">\(\mathcal{H}_{\mathcal{X}}\)</span>的边由胞腔间的直接互连关系（immediate incidence）确定，图 <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-diagram">8.1</a> 提供了一个CC的Hasse图示例。</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hasse-diagram"></span>
<img src="figures/poset.png" alt="Example of the Hasse graph of a CC. (a): CC of a M&quot;{o}bius strip. (b): Hasse graph of the CC, describing the poset structure between cells. (c): Hasse graph augmented with the edges defined via $A_{0,1}$ and $coA_{2,1}$."  />
<p class="caption">
图 8.1: Example of the Hasse graph of a CC. (a): CC of a M”{o}bius strip. (b): Hasse graph of the CC, describing the poset structure between cells. (c): Hasse graph augmented with the edges defined via <span class="math inline">\(A_{0,1}\)</span> and <span class="math inline">\(coA_{2,1}\)</span>.
</p>
</div>
<p>The <em>CC structure class</em> is the set of CCs determined up to isomorphism, according to Definition <a href="combinatorial-complexes.html#def:maps">4.2</a>. Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:structure">8.1</a> provides sufficient criteria for determining CC structure classes. The proof of Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:structure">8.1</a> relies on the observations that CC structure classes are determined by the underlying Hasse graph representation and that the Hasse graph provides the same information as the incidence matrices <span class="math inline">\(\{B_{k,k+1}\}_{k=0}^{\dim(\mathcal{X})-1}\)</span>. Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:vis-structure">8.2</a> supports visually the proofs of parts 2 and 3 in Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:structure">8.1</a>.</p>
<div class="proposition">
<p><span id="prp:structure" class="proposition"><strong>命题 8.1  (Determining a CC structure) </strong></span>Let <span class="math inline">\((S, \mathcal{X},\mbox{rk})\)</span> be a CC. For the CC structure class indicated by <span class="math inline">\((S, \mathcal{X},\mbox{rk})\)</span>, the following sufficient conditions hold:</p>
<ol style="list-style-type: decimal">
<li>The CC structure class is determined by the incidence matrices <span class="math inline">\(\{B_{k,k+1}\}_{k=0}^{ \dim(\mathcal{X}) -1}\)</span>.</li>
<li>The CC structure class is determined by the adjacency matrices <span class="math inline">\(\{A_{k,1}\}_{k=0}^{\dim(\mathcal{X})-1}\)</span>.</li>
<li>The CC structure class is determined by the coadjacency matrices <span class="math inline">\(\{coA_{k,1}\}_{k=1}^{\dim(\mathcal{X})}\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>证明</em>. </span>The proof of the three parts of the proposition follows by noting that the structure of a CC is determined completely by its Hasse graph representation. The first part of the proposition follows from the fact that the edges in the Hasse graph are precisely the non-zero entries of matrices <span class="math inline">\(\{B_{k,k+1}\}_{k=0}^{\dim(\mathcal{X}-1)}\)</span>. The second part follows by observing that two <span class="math inline">\((k-1)\)</span> cells <span class="math inline">\(x^{k-1}\)</span> and <span class="math inline">\(y^{k-1}\)</span> are 1-adjacent if and only if there exists a <span class="math inline">\(k\)</span>-cell <span class="math inline">\(z^k\)</span> that is incident to <span class="math inline">\(x^{k-1}\)</span> and <span class="math inline">\(y^{k-1}\)</span>. The third part is confirmed by noting that two <span class="math inline">\((k+1)\)</span>-cells <span class="math inline">\(x^{k+1}\)</span> and <span class="math inline">\(y^{k+1}\)</span> are 1-coadjacent if and only if there exists a <span class="math inline">\(k\)</span>-cell <span class="math inline">\(z^k\)</span> that is incident to <span class="math inline">\(x^{k+1}\)</span> and <span class="math inline">\(y^{k+1}\)</span>.</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vis-structure"></span>
<img src="figures/prop_structure.png" alt="Relation between immediate incidence and (co)adjacency on the Hasse graph of a CC. (a): Two $(k-1)$ cells $x^{k-1}$ and $y^{k-1}$ (orange vertices) are 1-adjacent if and only if there exists a $k$-cell $z^k$ (pink vertex) that is incident to $x^{k-1}$ and $y^{k-1}$. (b): Two $(k+1)$ cells $x^{k+1}$ and $y^{k+1}$ (blue vertices) are 1-coadjacent if and only if there exists a $k$-cell $z^k$ (pink vertex) that is incident to $x^{k+1}$ and $y^{k+1}$."  />
<p class="caption">
图 8.2: Relation between immediate incidence and (co)adjacency on the Hasse graph of a CC. (a): Two <span class="math inline">\((k-1)\)</span> cells <span class="math inline">\(x^{k-1}\)</span> and <span class="math inline">\(y^{k-1}\)</span> (orange vertices) are 1-adjacent if and only if there exists a <span class="math inline">\(k\)</span>-cell <span class="math inline">\(z^k\)</span> (pink vertex) that is incident to <span class="math inline">\(x^{k-1}\)</span> and <span class="math inline">\(y^{k-1}\)</span>. (b): Two <span class="math inline">\((k+1)\)</span> cells <span class="math inline">\(x^{k+1}\)</span> and <span class="math inline">\(y^{k+1}\)</span> (blue vertices) are 1-coadjacent if and only if there exists a <span class="math inline">\(k\)</span>-cell <span class="math inline">\(z^k\)</span> (pink vertex) that is incident to <span class="math inline">\(x^{k+1}\)</span> and <span class="math inline">\(y^{k+1}\)</span>.
</p>
</div>
</div>
<div id="augmented-hasse-graphs" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Augmented Hasse graphs<a href="hasse-graph-interpretation-of-ccnns.html#augmented-hasse-graphs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Hasse graph of a CC is useful because it shows that computations for a higher-order deep learning model can be reduced to computations for a graph-based model. Particularly, a <span class="math inline">\(k\)</span>-cochain (signal) being processed on a CC <span class="math inline">\(\mathcal{X}\)</span> can be thought as a signal on the corresponding vertices of the associated Hasse graph <span class="math inline">\(\mathcal{H}_{\mathcal{X}}\)</span>. The edges specified by the matrices <span class="math inline">\(B_{k,k+1}\)</span> determine the message-passing structure of a given higher-order model defined on <span class="math inline">\(\mathcal{X}\)</span>. However, the message-passing structure determined via the matrices <span class="math inline">\(A_{r,k}\)</span> is not directly supported on the corresponding edges of <span class="math inline">\(\mathcal{H}_{\mathcal{X}}\)</span>. Thus, it is sometimes desirable to <em>augment the Hasse graph</em> with additional edges other than the ones specified by the poset partial order relation of the CC. Along these lines, Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:ahg">8.2</a> introduces the notion of augmented Hasse graph.</p>
<div class="definition">
<p><span id="def:ahg" class="definition"><strong>定义 8.2  (Augmented Hasse graph) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC, and let <span class="math inline">\(\mathcal{H}_{\mathcal{X}}\)</span> be its Hasse graph with vertex set <span class="math inline">\(V(\mathcal{H}_{\mathcal{X}})\)</span> and edge set <span class="math inline">\(E(\mathcal{H}_{\mathcal{X}})\)</span>. Let <span class="math inline">\(\mathcal{N}=\{\mathcal{N}_1,\ldots,\mathcal{N}_n\}\)</span> be a set of neighborhood functions defined on <span class="math inline">\(\mathcal{X}\)</span>. We say that <span class="math inline">\(\mathcal{H}_{\mathcal{X}}\)</span> has an augmented edge <span class="math inline">\(e_{x,y}\)</span> induced by <span class="math inline">\(\mathcal{N}\)</span> if there exist <span class="math inline">\(\mathcal{N}_i \in \mathcal{N}\)</span> such that <span class="math inline">\(x \in \mathcal{N}_i(y)\)</span> or <span class="math inline">\(y \in \mathcal{N}_i(x)\)</span>. Denote by <span class="math inline">\(E_{\mathcal{N}}\)</span> the set of all augmented edges induced by <span class="math inline">\(\mathcal{N}\)</span>. The <em>augmented Hasse graph</em> of <span class="math inline">\(\mathcal{X}\)</span> induced by <span class="math inline">\(\mathcal{N}\)</span> is defined to be the graph <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathcal{N})= (V(\mathcal{H}_{\mathcal{X}}), E(\mathcal{H}_{\mathcal{X}}) \cup E_{\mathcal{N}})\)</span>.</p>
</div>
<p>It is easier to think of the augmented Hasse graph in Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:ahg">8.2</a> in terms of the matrices <span class="math inline">\(\mathbf{G}=\{G_1,\ldots,G_n\}\)</span> associated with the neighborhood functions <span class="math inline">\(\mathcal{N}=\{\mathcal{N}_1,\ldots,\mathcal{N}_n\}\)</span>. Each augmented edge in <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathcal{N})\)</span> corresponds to a non-zero entry in some <span class="math inline">\(G_i\in \mathbf{G}\)</span>. Since <span class="math inline">\(\mathcal{N}\)</span> and <span class="math inline">\(\mathbf{G}\)</span> store equivalent information, we use <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span> to denote the augmented Hasse graph induced by the edges determined by <span class="math inline">\(\mathbf{G}\)</span>. For instance, the graph given in Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-diagram">8.1</a>(c) is denoted by <span class="math inline">\(\mathcal{H}_{\mathcal{X}}( A_{0,1},coA_{2,1})\)</span>.</p>
</div>
<div id="reducibility-of-ccnns-to-graph-based-models" class="section level3 hasAnchor" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Reducibility of CCNNs to graph-based models<a href="hasse-graph-interpretation-of-ccnns.html#reducibility-of-ccnns-to-graph-based-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we show that any CCNN-based computational model can be realized as a message-passing scheme over a subgraph of the augmented Hasse graph of the underlying CC. Every CCNN is determined via a computational tensor diagram, which can be built using the elementary tensor operations, namely push-forward operations, merge nodes and split nodes. Thus, the reducibility of CCNN-based computations to message-passing schemes over graphs can be achieved by proving that these three tensor operations can be executed on an augmented Hasse graph. Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse-pushforward">8.2</a> states that push-forward operations are executable on augmented Hasse graphs.</p>
<div class="proposition">
<p><span id="prp:hasse-pushforward" class="proposition"><strong>命题 8.2  (Computation over augmented Hasse graph) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC and let <span class="math inline">\(\mathcal{F}_G \colon \mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})\)</span> be a push-forward operator induced by a cochain map <span class="math inline">\(G\colon\mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})\)</span>. Any computation executed via <span class="math inline">\(\mathcal{F}_G\)</span> can be reduced to a corresponding computation over the augmented Hassed graph <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(G)\)</span> of <span class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>证明</em>. </span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC. Let <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(G)\)</span> be the augmented Hasse graph of <span class="math inline">\(\mathcal{X}\)</span> determined by <span class="math inline">\(G\)</span>. The definition of the augmented Hasse graph implies that there is a one-to-one correspondence between the vertices <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(G)\)</span> and the cells in <span class="math inline">\(\mathcal{X}\)</span>. Given a cell <span class="math inline">\(x\in \mathcal{X}\)</span>, let <span class="math inline">\(x^{\prime}\)</span> be the corresponding vertex in <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(G)\)</span>. Let <span class="math inline">\(y\)</span> be a cell in <span class="math inline">\(\mathcal{X}\)</span> with a feature vector <span class="math inline">\(\mathbf{h}_y\)</span> computed via the push-forward operation specified by Equation <a href="combinatorial-complex-neural-networks.html#eq:functional">(5.2)</a>. Recall that the vector <span class="math inline">\(\mathbf{h}_y\)</span> is computed by aggregating all vectors <span class="math inline">\(\mathbf{h}_x\)</span> attached to the neighbors <span class="math inline">\(x \in \mathcal{X}^i\)</span> of <span class="math inline">\(y\)</span> with respect to the neighborhood function <span class="math inline">\(\mathcal{N}_{G^T}\)</span>. Let <span class="math inline">\(m_{x,y}\)</span> be a computation (message) that is executed between two cells <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of <span class="math inline">\(\mathcal{X}\)</span> as a part of the computation of push-forward <span class="math inline">\(\mathcal{F}_G\)</span>. It follows from the augmented Hasse graph definition that the cells <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> must have a corresponding non-zero entry in matrix <span class="math inline">\(G\)</span>. Moreover, this non-zero entry corresponds to an edge in <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(G)\)</span> between <span class="math inline">\(x^{\prime}\)</span> and <span class="math inline">\(y^{\prime}\)</span>. Thus, the computation <span class="math inline">\(m_{x,y}\)</span> between the cells <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of <span class="math inline">\(\mathcal{X}\)</span> can be carried out as the computation (message) <span class="math inline">\(m_{x^{\prime},y^{\prime}}\)</span> between the corresponding vertices <span class="math inline">\(x^{\prime}\)</span> and <span class="math inline">\(y^{\prime}\)</span> of <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(G)\)</span>.</p>
</div>
<p>Similarly, computations on an arbitrary merge node can be characterized in terms of computations on a subgraph of the augmented Hasse graph of the underlying CC. Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse">8.3</a> formalizes this statement.</p>
<div class="proposition">
<p><span id="prp:hasse" class="proposition"><strong>命题 8.3  (Reduction of merge node to augmented Hasse graph) </strong></span>Any computation executed via a merge node <span class="math inline">\(\mathcal{M}_{\mathbf{G},\mathbf{W}}\)</span> as given in Equation <a href="combinatorial-complex-neural-networks.html#eq:sum">(5.1)</a> can be reduced to a corresponding computation over the augmented Hasse graph <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span> of the underlying CC.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>证明</em>. </span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC. Let <span class="math inline">\(\mathbf{G}=\{ G_1,\ldots,G_n\}\)</span> be a sequence of cochain operators defined on <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span> be the augmented Hasse graph determined by <span class="math inline">\(\mathbf{G}\)</span>. By the augmented Hasse graph definition, there is a one-to-one correspondence between the vertices of <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span> and the cells of <span class="math inline">\(\mathcal{X}\)</span>. For each cell <span class="math inline">\(x\in \mathcal{X}\)</span>, let <span class="math inline">\(x^{\prime}\)</span> be the corresponding vertex in <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span>. Let <span class="math inline">\(m_{x,y}\)</span> be a computation (message) that is executed between two cells <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of <span class="math inline">\(\mathcal{X}\)</span> as part of the evaluation of function <span class="math inline">\(\mathcal{M}_{\mathbf{G},W}\)</span>. Hence, the two cells <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> must have a corresponding non-zero entry in a matrix <span class="math inline">\(G_i\in\mathbf{G}\)</span>. By the augmented Hasse graph definition, this non-zero entry corresponds to an edge in <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span> between <span class="math inline">\(x^{\prime}\)</span> and <span class="math inline">\(y^{\prime}\)</span>. Thus, the computation <span class="math inline">\(m_{x,y}\)</span> between the cells <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of <span class="math inline">\(\mathcal{X}\)</span> can be carried out as the computation (message) <span class="math inline">\(m_{x^{\prime},y^{\prime}}\)</span> between the corresponding vertices <span class="math inline">\(x^{\prime}\)</span> and <span class="math inline">\(y^{\prime}\)</span> of <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span>.</p>
</div>
<p>Propositions <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse-pushforward">8.2</a> and <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse">8.3</a> ensure that push-forward and merge node computations can be realized on augmented Hasse graphs. Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:hasse-theorem">8.1</a> generalizes Propositions <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse-pushforward">8.2</a> and <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse">8.3</a>, stating that any computation on tensor diagrams is realizable on augmented Hasse graphs.</p>
<div class="theorem">
<p><span id="thm:hasse-theorem" class="theorem"><strong>定理 8.1  (Reduction of tensor diagram to augmented Hasse graph) </strong></span>Any computation executed via a tensor diagram <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> can be reduced to a corresponding computation on the augmented Hasse graph <span class="math inline">\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>证明</em>. </span>The conclusion follows directly from Propositions <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse">8.3</a> and <a href="hasse-graph-interpretation-of-ccnns.html#prp:hasse-pushforward">8.2</a>, along with the fact that any tensor diagram can be realized in terms of the three elementary tensor operations.</p>
</div>
<p>According to Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:hasse-theorem">8.1</a>, a tensor diagram and its corresponding augmented Hasse graph encode the same computations in alternative forms. Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-diagram-examples">8.3</a> illustrates that the augmented Hasse graph
provides a computational summary of the associated tensor diagram representation of a CCNN.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hasse-diagram-examples"></span>
<img src="figures/augmented_hasse_graph_examples.png" alt="Tensor diagrams of two CCNNs and their corresponding augmented Hasse graphs. Edge labels are dropped from the tensor diagrams to avoid clutter, as they can be inferred from the corresponding augmented Hasse graphs. (a): A tensor diagram obtained from a higher-order message-passing scheme. (b): A tensor diagram obtained by using the three elementary tensor operations."  />
<p class="caption">
图 8.3: Tensor diagrams of two CCNNs and their corresponding augmented Hasse graphs. Edge labels are dropped from the tensor diagrams to avoid clutter, as they can be inferred from the corresponding augmented Hasse graphs. (a): A tensor diagram obtained from a higher-order message-passing scheme. (b): A tensor diagram obtained by using the three elementary tensor operations.
</p>
</div>
</div>
<div id="augmented-hasse-graphs-and-cc-pooling" class="section level3 hasAnchor" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> Augmented Hasse graphs and CC-pooling<a href="hasse-graph-interpretation-of-ccnns.html#augmented-hasse-graphs-and-cc-pooling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Hasse graph and its augmented version are graph representations of the poset structure of the underlying CC. It is instructive to interpret the (un)pooling operations (Definitions <a href="push-forward-pooling-and-unpooling.html#def:pooling-exact-definition">7.1</a> and <a href="push-forward-pooling-and-unpooling.html#def:unpooling-exact-definition">7.2</a>) with respect to these graphs. The CC-pooling operation of Definition <a href="push-forward-pooling-and-unpooling.html#def:pooling-exact-definition">7.1</a> maps a signal in the poset structure from lower-rank cells to higher-rank ones. On the other hand, the CC-unpooling operation of Definition <a href="push-forward-pooling-and-unpooling.html#def:unpooling-exact-definition">7.2</a> maps a signal in the opposite direction. Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-graph-pooling">8.4</a> presents an example of CC (un)-pooling operations visualized over the augmented Hasse graph of the underlying CC.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hasse-graph-pooling"></span>
<img src="figures/hasse_graph_pooling_scaled.png" alt="CC (un)-pooling operations viewed on the augmented Hasse graph of a CC. The vertices in the figure represent skeletons in the underlying CC. Black edges represent edges in the Hasse graph between these vertices, whereas red edges represent edges obtained from the augmented Hasse graph structure. CC-pooling corresponds to pushing a signal in the poset structure from lower-rank to higher-rank vertices, whereas CC-unpooling corresponds to pushing a signal in the poset structure from higher-rank to lower-rank vertices."  />
<p class="caption">
图 8.4: CC (un)-pooling operations viewed on the augmented Hasse graph of a CC. The vertices in the figure represent skeletons in the underlying CC. Black edges represent edges in the Hasse graph between these vertices, whereas red edges represent edges obtained from the augmented Hasse graph structure. CC-pooling corresponds to pushing a signal in the poset structure from lower-rank to higher-rank vertices, whereas CC-unpooling corresponds to pushing a signal in the poset structure from higher-rank to lower-rank vertices.
</p>
</div>
</div>
<div id="augmented-hasse-diagrams-message-passing-and-merge-nodes" class="section level3 hasAnchor" number="8.1.5">
<h3><span class="header-section-number">8.1.5</span> Augmented Hasse diagrams, message passing and merge nodes<a href="hasse-graph-interpretation-of-ccnns.html#augmented-hasse-diagrams-message-passing-and-merge-nodes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The difference between constructing a CCNN using the higher-order message passing paradigm given in Section <a href="message-passing.html#definition-of-higher-order-message-passing">6.1</a> versus using the three elementary tensor operations given in Section <a href="combinatorial-complex-neural-networks.html#the-main-three-tensor-operations">5.3</a> has been demonstrated in Section <a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison">6.3</a>. In particular, Section <a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison">6.3</a> mentions that merge nodes naturally allow for a more flexible computational framework in comparison to the higher-order message-passing paradigm. This flexibility manifests in terms of the underlying tensor diagram as well as the input for the network under consideration. The difference between tensor operations and higher-order message passing can also be highlighted with augmented Hasse graphs, as demonstrated in Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-diagram-examples">8.3</a>. Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-diagram-examples">8.3</a>(a) shows a tensor diagram obtained from a higher-order message-passing scheme on a CCNN. We observe two key properties of this CCNN: the initial input cochains are supported on all cells of all dimensions of the domain, and the CCNN updates all cochains supported on all cells of all dimensions of the domain at every iteration given a predetermined set of neighborhood functions. As a consequence, the corresponding augmented Hasse graph exhibits a uniform topological structure. In contrast, Figure <a href="hasse-graph-interpretation-of-ccnns.html#fig:hasse-diagram-examples">8.3</a>(b) shows a tensor diagram constructed using the three elementary tensor operations. As the higher-order message-passing rules do not impose constraints, the resulting augmented Hasse graph exhibits a more flexible structure.</p>
</div>
<div id="higher-order-representation-learning" class="section level3 hasAnchor" number="8.1.6">
<h3><span class="header-section-number">8.1.6</span> Higher-order representation learning<a href="hasse-graph-interpretation-of-ccnns.html#higher-order-representation-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The relation between augmented Hasse graphs and CCs given by Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:hasse-theorem">8.1</a> suggests that many graph-based deep learning constructions have analogous constructions for CCs. In this section, we demonstrate how <em>higher-order representation learning</em> can be reduced to graph representation learning <span class="citation">(<a href="#ref-hamilton2017representation">Hamilton, Ying, and Leskovec 2017</a>)</span>, as an application of certain CC computations as augmented Hasse graph computations.</p>
<p>The goal of graph representation is to learn a mapping that embeds the vertices, edges or subgraphs of a graph into a Euclidean space, so that the resulting embedding captures useful information about the graph. Similarly, higher-order representation learning <span class="citation">(<a href="#ref-hajijcell">Hajij, Istvan, and Zamzmi 2020</a>)</span> involves learning an embedding of various cells in a given topological domain into a Euclidean space, preserving the main structural properties of the topological domain. More precisely, given a complex <span class="math inline">\(\mathcal{X}\)</span>, higher-order representation learning refers to learning a pair <span class="math inline">\((enc, dec)\)</span> of functions, consisting of the <em>encoder map</em> <span class="math inline">\(enc \colon \mathcal{X}^k \to \mathbb{R}^d\)</span> and the <em>decoder map</em> <span class="math inline">\(dec \colon \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span>. The encoder function associates to every <span class="math inline">\(k\)</span>-cell <span class="math inline">\(x^k\)</span> in <span class="math inline">\(\mathcal{X}\)</span> a feature vector <span class="math inline">\(enc(x^k)\)</span>, which encodes the structure of <span class="math inline">\(x^k\)</span> with respect to the structures of other cells in <span class="math inline">\(\mathcal{X}\)</span>. On the other hand, the decoder function associates to every pair of cell embeddings a measure of similarity, which quantifies some notion of relation between the corresponding cells. We optimize the trainable functions <span class="math inline">\((enc, dec)\)</span> using a context-specific <em>similarity measure</em> <span class="math inline">\(sim \colon \mathcal{X}^k \times \mathcal{X}^k \to \mathbb{R}\)</span> and an objective function
<span class="math display" id="eq:loss">\[\begin{equation}
\mathcal{L}_k=\sum_{ x^k \in \mathcal{X}^k     } l(  dec(  enc(x^{k}), enc(y^{k})),sim(x^{k},y^k)),
\tag{8.1}
\end{equation}\]</span>
where <span class="math inline">\(l \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}\)</span> is a loss function. The precise relation between higher-order and graph representation learning is given by Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:convert-graphtocc">8.4</a>.</p>
<div class="proposition">
<p><span id="prp:convert-graphtocc" class="proposition"><strong>命题 8.4  (Higher-order representation learning as graph representation learning) </strong></span>Higher-order representation learning can be reduced to graph representation learning.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>证明</em>. </span>Let <span class="math inline">\(sim\colon \mathcal{X}^k \times \mathcal{X}^k \to \mathbb{R}\)</span> be a similarity measure. The graph <span class="math inline">\(\mathcal{G}_{\mathcal{X}^k}\)</span> is defined as the graph whose vertex set corresponds to cells in <span class="math inline">\(\mathcal{X}^k\)</span> and whose edges correspond to cell pairs in <span class="math inline">\(\mathcal{X}^k \times \mathcal{X}^k\)</span> mapped to non-zero values by the function <span class="math inline">\(sim\)</span>. Thus, the pair <span class="math inline">\((enc, dec)\)</span> corresponds to a pair <span class="math inline">\((enc_{\mathcal{G}}, dec_{\mathcal{G}})\)</span> of the form <span class="math inline">\(enc_{\mathcal{G}}\colon \mathcal{G}_{\mathcal{X}^k} \to \mathbb{R}\)</span> and <span class="math inline">\(dec_{\mathcal{G}}\colon \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span>. Thereby, learning the pair <span class="math inline">\((enc, dec)\)</span> is reduced to learning the pair <span class="math inline">\((enc_{\mathcal{G}}, dec_{\mathcal{G}})\)</span>.</p>
</div>
<p><a href="https://github.com/pyt-team/TopoEmbedX">TopoEmbedX</a>, one of our three contributed software packages, supports higher-order representation learning on cell complexes, simplicial complexes, and CCs. The main computational principle underlying TopoEmbedX is Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:convert-graphtocc">8.4</a>. Specifically, TopoEmbedX converts a given higher-order domain into a subgraph of the corresponding augmented Hasse graph, and then utilizes existing graph representation learning algorithms to compute the embedding of elements of this subgraph. Given the correspondence between the elements of the augmented Hasse graph and the original higher-order domain, this results in obtaining embeddings for the higher-order domain.</p>
<div class="remark">
<p><span id="unlabeled-div-17" class="remark"><em>备注</em>. </span>Following our discussion on Hasse graphs, and particularly the ability to transform computations on a CCNN to computations on a (Hasse) graph, one may argue that GNNs are sufficient and that there is no need for CCNNs. However, this is a misleading clue, in the sense that any computation can be represented by a computational graph. Applying a standard GNN over the augmented Hasse graph of a CC is not equivalent to applying a CCNN. This point will become clearer in Section <a href="hasse-graph-interpretation-of-ccnns.html#on-the-equivariance-of-ccnns">8.2</a>, where we introduce CCNN <em>equivariances</em>.</p>
</div>
</div>
</div>
<div id="on-the-equivariance-of-ccnns" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> On the equivariance of CCNNs<a href="hasse-graph-interpretation-of-ccnns.html#on-the-equivariance-of-ccnns" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Analogous to their graph counterparts, higher-order deep learning models, and CCNNs in particular, should always be considered in conjunction with their underlying <em>equivariance</em> <span class="citation">(<a href="#ref-bronstein2021geometric">Bronstein et al. 2021</a>)</span>. We now provide novel definitions for <em>permutation</em> and <em>orientation equivariance for CCNNs</em> and draw attention to their relations with conventional notions of equivariance defined for GNNs.</p>
<div id="permutation-equivariance-of-ccnns" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Permutation equivariance of CCNNs<a href="hasse-graph-interpretation-of-ccnns.html#permutation-equivariance-of-ccnns" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Motivated by Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:structure">8.1</a>, which characterizes the structure of a CC, this section introduces permutation-equivariant CCNNs. We first define the action of the permutation group on the space of cochain maps.</p>
<div class="definition">
<p><span id="def:perm" class="definition"><strong>定义 8.3  (Permutation action on space of cochain maps) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC. Define <span class="math inline">\(\mbox{Sym}(\mathcal{X}) = \prod_{i=0}^{\dim(\mathcal{X})} \mbox{Sym}(\mathcal{X}^k)\)</span> the group of rank-preserving permutations of the cells of <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(\mathbf{G}=\{G_k\}\)</span> be a sequence of cochain maps defined on <span class="math inline">\(\mathcal{X}\)</span> with <span class="math inline">\(G_k  \colon \mathcal{C}^{i_k}\to \mathcal{C}^{j_k}\)</span>, <span class="math inline">\(0\leq i_k,j_k\leq \dim(\mathcal{X})\)</span>. Let <span class="math inline">\(\mathcal{P}=(\mathbf{P}_i)_{i=0}^{\dim(\mathcal{X})} \in \mbox{Sym}(\mathcal{X})\)</span>. Define the <em>permutation (group) action</em> of <span class="math inline">\(\mathcal{P}\)</span> on <span class="math inline">\(\mathbf{G}\)</span> by <span class="math inline">\(\mathcal{P}(\mathbf{G}) = (\mathbf{P}_{j_k} G_{k} \mathbf{P}_{i_k}^T )_{i=0}^{\dim(\mathcal{X})}\)</span>.</p>
</div>
<p>We introduce permutation-equivariant CCNNs in Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a>, using the group action given in Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:perm">8.3</a>. Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a> generalizes the relevant definitions in <span class="citation">(<a href="#ref-roddenberry2021principled">T. Mitchell Roddenberry, Glaze, and Segarra 2021</a>; <a href="#ref-schaub2021signal">Schaub et al. 2021</a>)</span>. We refer the reader to <span class="citation">(<a href="#ref-joglwe2022">Jogl 2022</a>; <a href="#ref-velivckovic2022message">Veličković 2022</a>)</span> for a related discussion. Hereafter, we use <span class="math inline">\(\mbox{Proj}_k \colon \mathcal{C}^1\times \cdots \times \mathcal{C}^m \to \mathcal{C}^k\)</span> to denote the standard <span class="math inline">\(k\)</span>-th projection for <span class="math inline">\(1\leq k \leq m\)</span>, defined via <span class="math inline">\(\mbox{Proj}_k ( \mathbf{H}_{1},\ldots, \mathbf{H}_{k},\ldots,\mathbf{H}_{m})= \mathbf{H}_{k}\)</span>.</p>
<div class="definition">
<p><span id="def:eqv" class="definition"><strong>定义 8.4  (Permutation-equivariant CCNN) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC and let <span class="math inline">\(\mathbf{G}= \{G_k\}\)</span> be a finite sequence of cochain maps defined on <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(\mathcal{P}=(\mathbf{P}_i)_{i=0}^{\dim(\mathcal{X})} \in \mbox{Sym}(\mathcal{X})\)</span>. A CCNN of the form
<span class="math display">\[\begin{equation*}
\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}
\end{equation*}\]</span>
is called a <em>permutation-equivariant CCNN</em> if
<span class="math display">\[\begin{equation}
\mbox{Proj}_k \circ \mbox{CCNN}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})=
\mathbf{P}_{k} \mbox{Proj}_k \circ
\mbox{CCNN}_{\mathcal{P}(\mathbf{G});\mathbf{W}}(\mathbf{P}_{i_1} \mathbf{H}_{i_1}, \ldots ,\mathbf{P}_{i_m} \mathbf{H}_{i_m})
\end{equation}\]</span>
for all <span class="math inline">\(1 \leq k\leq m\)</span> and for any <span class="math inline">\((\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m}) \in\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\)</span>.</p>
</div>
<p>Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a> generalizes the corresponding notion of permutation equivariance of GNNs. Consider a graph with <span class="math inline">\(n\)</span> vertices and adjacency matrix <span class="math inline">\(A\)</span>. Denote a GNN on this graph by <span class="math inline">\(\mathrm{GNN}_{A;W}\)</span>. Let <span class="math inline">\(H \in \mathbb{R}^{n \times k}\)</span> be vertex features. Then <span class="math inline">\(\mathrm{GNN}_{A;W}\)</span> is permutation equivariant in the sense that for <span class="math inline">\(P \in \mbox{Sym}(n)\)</span> we have <span class="math inline">\(P \,\mathrm{GNN}_{A;W}(H) = \mathrm{GNN}_{PAP^{T};W}(PH)\)</span>.</p>
<p>In general, working with Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a> may be cumbersome. It is easier to characterize the equivariance in terms of merge nodes. To this end, recall that the height of a tensor diagram is the longest path from any source node to any target node. Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:simple">8.5</a> allows us to express tensor diagrams of height one in terms of merge nodes.</p>
<div class="proposition">
<p><span id="prp:simple" class="proposition"><strong>命题 8.5  (Tensor diagrams of height one as merge nodes) </strong></span>Let <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)</span> be a CCNN with a tensor diagram of height one. Then
<span class="math display" id="eq:merge-lemma">\[\begin{equation}
        \label{merge_lemma}
        \mbox{CCNN}_{\mathbf{G};\mathbf{W}}=(
        \mathcal{M}_{\mathbf{G}_{j_1};\mathbf{W}_1},\ldots,
        \mathcal{M}_{\mathbf{G}_{j_n};\mathbf{W}_n}),
\tag{8.2}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{G}_k \subseteq \mathbf{G}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>证明</em>. </span>Let <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)</span> be a CCNN with a tensor diagram of height one. Since the codomain of the function <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is <span class="math inline">\(\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times \mathcal{C}^{j_n}\)</span>, then <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is determined by <span class="math inline">\(n\)</span> functions <span class="math inline">\(F_k\colon  \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_k}\)</span> for <span class="math inline">\(1 \leq k \leq n\)</span>. Since the height of the tensor diagram of <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is one, then each function <span class="math inline">\(F_k\)</span> is also of height one and it is thus a merge node by definition. The result follows.</p>
</div>
<p>Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:simple">8.5</a> states that every target node <span class="math inline">\(j_k\)</span> in a tensor diagram of height one is a merge node specified by the operators <span class="math inline">\(\mathbf{G}_{j_k}\)</span> formed by the labels of the edges with target <span class="math inline">\(j_k\)</span>. Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a> introduces the general notion of permutation equivariance of CCNNs. Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:node-equivariance">8.5</a> introduces the notion of permutation-equivariant merge node. Since a merge node is a CCNN, Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:node-equivariance">8.5</a> is a special case of Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a>.</p>
<div class="definition">
<p><span id="def:node-equivariance" class="definition"><strong>定义 8.5  (Permutation-equivariant merge node) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC and let <span class="math inline">\(\mathbf{G}= \{G_k\}\)</span> be a finite sequence of cochain operators defined on <span class="math inline">\(\mathcal{X}\)</span> with <span class="math inline">\(G_k\colon C^{i_k}(\mathcal{X})\to C^{j}(\mathcal{X})\)</span>. Let <span class="math inline">\(\mathcal{P}=(\mathbf{P}_i)_{i=0}^{\dim(\mathcal{X})} \in \mbox{Sym}(\mathcal{X})\)</span>. We say that the merge node given in Equation <a href="combinatorial-complex-neural-networks.html#eq:sum">(5.1)</a> is a <em>permutation-equivariant merge node</em> if
<span class="math display">\[\begin{equation}
\mathcal{M}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})= \mathbf{P}_{j}  \mathcal{M}_{\mathcal{P}(\mathbf{G});\mathbf{W}}(\mathbf{P}_{i_1} \mathbf{H}_{i_1}, \ldots ,\mathbf{P}_{i_1} \mathbf{H}_{i_m})
\end{equation}\]</span>
for any <span class="math inline">\((\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m}) \in \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:height1" class="proposition"><strong>命题 8.6  (Permutation-equivariant CCNN of height one and merge nodes) </strong></span>Let <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)</span> be a CCNN with a tensor diagram of height one. Then <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is permutation equivariant if and only the merge nodes <span class="math inline">\(\mathcal{M}_{\mathbf{G}_{j_k};\mathbf{W}_k}\)</span> given in Equation <a href="hasse-graph-interpretation-of-ccnns.html#eq:merge-lemma">(8.2)</a> are permutation equivariant for <span class="math inline">\(1 \leq k \leq n\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>证明</em>. </span>If a CCNN is of height one, then by Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:simple">8.5</a>, <span class="math inline">\(\mbox{Proj}_k \circ \mbox{CCNN}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})= \mathcal{M}_{\mathbf{G}_{j_k};\mathbf{W}_k}\)</span>. Hence, the result follows from the definition of merge node permutation equivariance (Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:node-equivariance">8.5</a>) and the definition of CCNN permutation equivariance (Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a>).</p>
</div>
<p>Finally, Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:height2">8.2</a> characterizes the permutation equivariance of CCNNs in terms of merge nodes. From this point of view, Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:height2">8.2</a> provides a practical version of permutation equivariance for CCNNs.</p>
<div class="theorem">
<p><span id="thm:height2" class="theorem"><strong>定理 8.2  (Permutation-equivariant CCNN and merge nodes) </strong></span>A <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is permutation equivariant if and only if every merge node in <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is permutation equivariant.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>证明</em>. </span>Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:height1">8.6</a> proves this fact for CCNNs of height one. For CCNNs of height <span class="math inline">\(n\)</span>, it is enough to observe that a CCNN of height <span class="math inline">\(n\)</span> is a composition of <span class="math inline">\(n\)</span> CCNNs of height one and that the composition of two permutation-equivariant networks is a permutation-equivariant network.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-21" class="remark"><em>备注</em>. </span>Our permutation equivariance assumes that all cells in each dimension are independently labeled with indices. However, if we label the cells in a CC with subsets of the powerset <span class="math inline">\(\mathcal{P}(S)\)</span> rather than with indices, then we only need to consider permutations of the powerset that are induced by permutations of the 0-cells in order to ensure permutation equivariance.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-22" class="remark"><em>备注</em>. </span>A GNN is equivariant in that a permutation of the vertex set of the graph and the input signal over the vertex set yields the same permutation of the GNN output. Applying a standard GNN over the augmented Hasse graph of the underlying CC is thus not equivalent to applying a CCNN. Although the message-passing structures are the same, the weight-sharing and permutation equivariance of the standard GNN and CCNN are different. In particular, Definition <a href="combinatorial-complexes.html#def:maps">4.2</a> gives additional structure, which is not preserved by an arbitrary permutation of the vertices in the augmented Hasse graph. Thus, care is required in order to reduce message passing over a CCNN to message passing over the associated augmented Hasse graph. Specifically, one need only consider the subgroup of permutations of vertex labels in the augmented Hasse graph which are induced by permutations of 0-cells in the corresponding CC. Thus, there is merit in adopting the rich notions of topology to think about distributed, structured learning architectures, as topological constructions facilitate reasoning about computation in ways that are not within the scope of graph-based approaches.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-23" class="remark"><em>备注</em>. </span>Note that Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:convert-graphtocc">8.4</a> does not contradict the previous remark. In fact, the computations described in Proposition <a href="hasse-graph-interpretation-of-ccnns.html#prp:convert-graphtocc">8.4</a> are conducted on a particular subgraph of the Hasse graph whose vertices are the <span class="math inline">\(k\)</span>-cells of the underlying complex. Differences between graph-based networks and TDL networks start to emerge particularly once different dimensions are considered simultaneously during computations.</p>
</div>
</div>
<div id="orientation-equivariance-of-ccnns" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Orientation equivariance of CCNNs<a href="hasse-graph-interpretation-of-ccnns.html#orientation-equivariance-of-ccnns" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When CC are reduced to regular cell complexes, then orientation equivariance can also be introduced to CCNNs. Analogous to Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:perm">8.3</a>, we introduce the following definition of orientation actions on CCs.</p>
<div class="definition">
<p><span id="def:orientation" class="definition"><strong>定义 8.6  (Orientation action on space of diagonal-cochain maps) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC. Let <span class="math inline">\(\mathbf{G}=\{G_k\}\)</span> be a sequence of cochain operators defined on <span class="math inline">\(\mathcal{X}\)</span> with <span class="math inline">\(G_k  \colon \mathcal{C}^{i_k}\to \mathcal{C}^{j_k}\)</span>, <span class="math inline">\(0\leq i_k,j_k\leq \dim(\mathcal{X})\)</span>. Let <span class="math inline">\(O(\mathcal{X})\)</span> be the group of tuples <span class="math inline">\(\mathcal{D}=(\mathbf{D}_i)_{i=0}^{\dim(\mathcal{X})}\)</span> of diagonal matrices with diagonals <span class="math inline">\(\pm 1\)</span> of size <span class="math inline">\(|\mathcal{X}^k| \times |\mathcal{X}^k|\)</span> such that <span class="math inline">\(\mathbf{D}_0=I\)</span>. Define the <em>orientation (group) action</em> of <span class="math inline">\(\mathcal{D}\)</span> on <span class="math inline">\(\mathbf{G}\)</span> by <span class="math inline">\(\mathcal{D}(\mathbf{G}) = (\mathbf{D}_{j_k} G_{k} \mathbf{D}_{i_k})_{i=0}^{\dim(\mathcal{X})}\)</span>.</p>
</div>
<p>We introduce orientation-equivariant CCNNs in Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:oe">8.7</a>, using the group action given in Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:orientation">8.6</a>. Orientation equivariance for CCNNs (Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:oe">8.7</a>) is put forward in analogous way to permutation equivariance for CCNNs (Definition <a href="hasse-graph-interpretation-of-ccnns.html#def:eqv">8.4</a>).</p>
<div class="definition">
<p><span id="def:oe" class="definition"><strong>定义 8.7  (Orientation-equivariant CCNN) </strong></span>Let <span class="math inline">\(\mathcal{X}\)</span> be a CC and let <span class="math inline">\(\mathbf{G}= \{G_k\}\)</span> be a finite sequence of cochain operators defined on <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(\mathcal{D} \in O(\mathcal{X})\)</span>. A CCNN of the form
<span class="math display">\[\begin{equation*}
\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}
\end{equation*}\]</span>
is called an <em>orientation-equivariant CCNN</em> if
<span class="math display">\[\begin{equation}
\mbox{Proj}_k \circ \mbox{CCNN}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})=\mathbf{D}_{k} \mbox{Proj}_k \circ \mbox{CCNN}_{\mathcal{D}(\mathbf{G});\mathbf{W}}((\mathbf{D}_{i_1} \mathbf{H}_{i_1}, \ldots ,\mathbf{D}_{i_1} \mathbf{H}_{i_m}))
\end{equation}\]</span>
for all <span class="math inline">\(1 \leq k\leq m\)</span> and for any <span class="math inline">\((\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m}) \in \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\)</span>.</p>
</div>
<p>Propositions <a href="hasse-graph-interpretation-of-ccnns.html#prp:simple">8.5</a> and <a href="hasse-graph-interpretation-of-ccnns.html#prp:height1">8.6</a> can be stated analogously for the orientation equivariance case. We skip stating these facts here, and only state the main theorem that characterizes the orientation equivariance of CCNNs in terms of merge nodes.</p>
<div class="theorem">
<p><span id="thm:height3" class="theorem"><strong>定理 8.3  (Orientation-equivariant CCNN and merge nodes) </strong></span>A <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is orientation equivariant if and only if every merge node in <span class="math inline">\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)</span> is orientation equivariant.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-24" class="proof"><em>证明</em>. </span>The proof of Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:height3">8.3</a> is similar to the proof of Theorem <a href="hasse-graph-interpretation-of-ccnns.html#thm:height2">8.2</a>.</p>
</div>

</div>
</div>
</div>



<h3>参考文献<a href="参考文献.html#参考文献" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-abramenko2008buildings" class="csl-entry">
Abramenko, Peter, and Kenneth S. Brown. 2008. <em>Buildings: Theory and Applications</em>. Vol. 248. Springer Science &amp; Business Media.
</div>
<div id="ref-bronstein2021geometric" class="csl-entry">
Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. 2021. <span>“Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.”</span> <em>arXiv Preprint arXiv:2104.13478</em>.
</div>
<div id="ref-hajijcell" class="csl-entry">
Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. <span>“Cell Complex Neural Networks.”</span> In <em>NeurIPS 2020 Workshop TDA and Beyond</em>.
</div>
<div id="ref-hamilton2017representation" class="csl-entry">
Hamilton, William L., Rex Ying, and Jure Leskovec. 2017. <span>“Representation Learning on Graphs: Methods and Applications.”</span> <em>IEEE Data Engineering Bulletin</em> 40 (3): 52–74.
</div>
<div id="ref-joglwe2022" class="csl-entry">
Jogl, Fabian. 2022. <span>“Do We Need to Improve Message Passing? Improving Graph Neural Networks with Graph Transformations.”</span> PhD thesis, Vienna University of Technology.
</div>
<div id="ref-roddenberry2021principled" class="csl-entry">
Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021. <span>“Principled Simplicial Neural Networks for Trajectory Prediction.”</span> In <em>International Conference on Machine Learning</em>.
</div>
<div id="ref-schaub2021signal" class="csl-entry">
Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. <span>“Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.”</span> <em>Signal Processing</em> 187: 108149.
</div>
<div id="ref-velivckovic2022message" class="csl-entry">
Veličković, Petar. 2022. <span>“Message Passing All the Way Up.”</span> <em>ICLR 2022 Workshop on Geometrical and Topological Representation Learning</em>.
</div>
<div id="ref-wachs2006poset" class="csl-entry">
Wachs, Michelle L. 2006. <span>“Poset Topology: Tools and Applications.”</span> <em>arXiv Preprint Math/0602226</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>对于相关结构 (例如, 单纯形/胞腔/胞腔复形), 偏序关系一般称为 <em>面偏序（face poset）</em> <span class="citation">(<a href="#ref-wachs2006poset">Wachs 2006</a>)</span>。<a href="hasse-graph-interpretation-of-ccnns.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="push-forward-pooling-and-unpooling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="implementation-and-numerical-results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pyt-team/tdlbook/edit/main/rmd/08-hasse-graph-interpretation.rmd",
"text": "编辑"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
