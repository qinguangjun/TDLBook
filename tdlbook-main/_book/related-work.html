<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>第 10 章 相关工作 | 拓扑深度学习：超越图数据</title>
  <meta name="description" content="一本关于拓扑深度学习的书。" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="第 10 章 相关工作 | 拓扑深度学习：超越图数据" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="一本关于拓扑深度学习的书。" />
  <meta name="github-repo" content="pyt-team/tdlbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 10 章 相关工作 | 拓扑深度学习：超越图数据" />
  
  <meta name="twitter:description" content="一本关于拓扑深度学习的书。" />
  

<meta name="author" content="Mustafa Hajij, Theodore Papamarkou, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Michael T. Schaub" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="implementation-and-numerical-results.html"/>
<link rel="next" href="conclusions.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/glossarybox.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">拓扑深度学习</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>贡献者</a></li>
<li class="chapter" data-level="" data-path="翻译说明.html"><a href="翻译说明.html"><i class="fa fa-check"></i>翻译说明</a></li>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html"><i class="fa fa-check"></i>序言</a>
<ul>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html#编译"><i class="fa fa-check"></i>编译</a></li>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html#致谢"><i class="fa fa-check"></i>致谢</a></li>
</ul></li>
<li class="part"><span><b>第一部分：基础知识</b></span></li>
<li class="chapter" data-level="1" data-path="引言.html"><a href="引言.html"><i class="fa fa-check"></i><b>1</b> 引言</a></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> 研究动机</a>
<ul>
<li class="chapter" data-level="2.1" data-path="motivation.html"><a href="motivation.html#从拓扑空间数据中建模和学习"><i class="fa fa-check"></i><b>2.1</b> 从拓扑空间数据中建模和学习</a></li>
<li class="chapter" data-level="2.2" data-path="motivation.html"><a href="motivation.html#the-utility-of-topology"><i class="fa fa-check"></i><b>2.2</b> 拓扑的有用性</a></li>
<li class="chapter" data-level="2.3" data-path="motivation.html"><a href="motivation.html#深度学习和结构化计算的统一视角"><i class="fa fa-check"></i><b>2.3</b> 深度学习和结构化计算的统一视角</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>3</b> 预备知识</a>
<ul>
<li class="chapter" data-level="3.1" data-path="preliminaries.html"><a href="preliminaries.html#邻域函数和拓扑空间"><i class="fa fa-check"></i><b>3.1</b> 邻域函数和拓扑空间</a></li>
<li class="chapter" data-level="3.2" data-path="preliminaries.html"><a href="preliminaries.html#bridging-the-gap-among-higher-order-networks"><i class="fa fa-check"></i><b>3.2</b> 填补与高阶网络间的代沟</a></li>
<li class="chapter" data-level="3.3" data-path="preliminaries.html"><a href="preliminaries.html#hierarchical-structure-and-set-type-relations"><i class="fa fa-check"></i><b>3.3</b> 层次化结构与集合型关系</a></li>
</ul></li>
<li class="part"><span><b>第二部分:组合复形</b></span></li>
<li class="chapter" data-level="4" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html"><i class="fa fa-check"></i><b>4</b> 组合复形</a>
<ul>
<li class="chapter" data-level="4.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc-definition"><i class="fa fa-check"></i><b>4.1</b> 组合复形定义</a></li>
<li class="chapter" data-level="4.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc-homomorphisms-and-sub-ccs"><i class="fa fa-check"></i><b>4.2</b> CC同态和子CCs</a></li>
<li class="chapter" data-level="4.3" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#motivation-for-ccs"><i class="fa fa-check"></i><b>4.3</b> 引入CCs的动机</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#pooling-operations-on-ccs"><i class="fa fa-check"></i><b>4.3.1</b> CCs上的池化操作</a></li>
<li class="chapter" data-level="4.3.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#structural-advantages-of-ccs"><i class="fa fa-check"></i><b>4.3.2</b> CCs的结构化优势</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#neighbourhood-functions-on-ccs"><i class="fa fa-check"></i><b>4.4</b> CCs上的邻域函数</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#incidence-in-a-cc"><i class="fa fa-check"></i><b>4.4.1</b> CC中的关联关系（Incidence）</a></li>
<li class="chapter" data-level="4.4.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc内的邻接关系adjacency"><i class="fa fa-check"></i><b>4.4.2</b> CC内的邻接关系（Adjacency）</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#data-on-ccs"><i class="fa fa-check"></i><b>4.5</b> CCs上的数据</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html"><i class="fa fa-check"></i><b>5</b> 组合复形神经网络（Combinatorial complex neural networks）</a>
<ul>
<li class="chapter" data-level="5.1" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#building-ccnns-tensor-diagrams"><i class="fa fa-check"></i><b>5.1</b> 构建 CCNN：张量图</a></li>
<li class="chapter" data-level="5.2" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#push-forward-operator-and-merge-node"><i class="fa fa-check"></i><b>5.2</b> 前推操作（Push-forward operator）和聚合节点</a></li>
<li class="chapter" data-level="5.3" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#the-main-three-tensor-operations"><i class="fa fa-check"></i><b>5.3</b> 三种主要的张量操作</a></li>
<li class="chapter" data-level="5.4" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#definition-of-combinatorial-complex-convolutional-networks"><i class="fa fa-check"></i><b>5.4</b> 组合复形卷积网络的定义（combinatorial complex convolutional networks）</a></li>
<li class="chapter" data-level="5.5" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks"><i class="fa fa-check"></i><b>5.5</b> 组合复形注意力神经网络</a></li>
</ul></li>
<li class="part"><span><b>第三部分：高阶消息传递（Higher-order message passing）</b></span></li>
<li class="chapter" data-level="6" data-path="message-passing.html"><a href="message-passing.html"><i class="fa fa-check"></i><b>6</b> 消息传递</a>
<ul>
<li class="chapter" data-level="6.1" data-path="message-passing.html"><a href="message-passing.html#definition-of-higher-order-message-passing"><i class="fa fa-check"></i><b>6.1</b> 高阶消息传递的定义</a></li>
<li class="chapter" data-level="6.2" data-path="message-passing.html"><a href="message-passing.html#higher-order-message-passing-neural-networks-are-ccnns"><i class="fa fa-check"></i><b>6.2</b> 高阶消息传递神经网络就是CCNNs</a></li>
<li class="chapter" data-level="6.3" data-path="message-passing.html"><a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison"><i class="fa fa-check"></i><b>6.3</b> 聚合节点和高阶消息传递：量化比较</a></li>
<li class="chapter" data-level="6.4" data-path="message-passing.html"><a href="message-passing.html#attention-higher-order-message-passing-and-ccanns"><i class="fa fa-check"></i><b>6.4</b> 注意力高阶消息传递和CCANNs</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html"><i class="fa fa-check"></i><b>7</b> 前推、池化和反池化</a>
<ul>
<li class="chapter" data-level="7.1" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#cc-pooling-and-unpooling"><i class="fa fa-check"></i><b>7.1</b> CC池化和反池化</a></li>
<li class="chapter" data-level="7.2" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#formulating-common-pooling-operations-as-cc-pooling"><i class="fa fa-check"></i><b>7.2</b> 将常见的池化操作表述为 CC-pooling</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#graph-pooling-as-cc-pooling"><i class="fa fa-check"></i><b>7.2.1</b> 用CC-pooling表示图池化操作</a></li>
<li class="chapter" data-level="7.2.2" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#image-pooling-as-cc-pooling"><i class="fa fa-check"></i><b>7.2.2</b> 图像池化作为CC-pooing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#pooling-and-unpooling-ccnns"><i class="fa fa-check"></i><b>7.3</b> 池化与反池化CCNNs</a></li>
<li class="chapter" data-level="7.4" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#mapper-and-the-cc-pooling-operation"><i class="fa fa-check"></i><b>7.4</b> 映射器和CC池化操作</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html"><i class="fa fa-check"></i><b>8</b> CCNNs的Hasse图解释</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#hasse-graph-interpretation-of-ccnns-2"><i class="fa fa-check"></i><b>8.1</b> CCNNs的Hasse图解释</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#ccs-as-hasse-graphs"><i class="fa fa-check"></i><b>8.1.1</b> CCs作为Hasse图</a></li>
<li class="chapter" data-level="8.1.2" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#augmented-hasse-graphs"><i class="fa fa-check"></i><b>8.1.2</b> 增强的Hasse图</a></li>
<li class="chapter" data-level="8.1.3" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#reducibility-of-ccnns-to-graph-basedmodels"><i class="fa fa-check"></i><b>8.1.3</b> CCNN对图模型的归约能力</a></li>
<li class="chapter" data-level="8.1.4" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#augmented-hasse-graphs-and-cc-pooling"><i class="fa fa-check"></i><b>8.1.4</b> 增强Hasse图和CC-pooling</a></li>
<li class="chapter" data-level="8.1.5" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#augmented-hasse-diagrams-message-passing-and-mergenodes"><i class="fa fa-check"></i><b>8.1.5</b> 增强Hasse图消息传递和聚合节点</a></li>
<li class="chapter" data-level="8.1.6" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#higher-order-representation-learning"><i class="fa fa-check"></i><b>8.1.6</b> 高阶表征学习</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#on-the-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2</b> CCNNs的等变性</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#permutation-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2.1</b> CCNNs的置换等变</a></li>
<li class="chapter" data-level="8.2.2" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#orientation-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2.2</b> CCNNs的方向等变</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>第四部分：应用，文献和结论</b></span></li>
<li class="chapter" data-level="9" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html"><i class="fa fa-check"></i><b>9</b> 实现与实验</a>
<ul>
<li class="chapter" data-level="9.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#software-toponetx-topoembedx-and-topomodelx"><i class="fa fa-check"></i><b>9.1</b> 软件：TopoNetX, TopoEmbedX, and TopoModelX</a></li>
<li class="chapter" data-level="9.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#datasets"><i class="fa fa-check"></i><b>9.2</b> 数据集</a></li>
<li class="chapter" data-level="9.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#shape-analysis-mesh-segmentation-and-classification"><i class="fa fa-check"></i><b>9.3</b> 形状分析：网格分割与分类</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-segmentation"><i class="fa fa-check"></i><b>9.3.1</b> 网格分割</a></li>
<li class="chapter" data-level="9.3.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-and-point-cloud-classification"><i class="fa fa-check"></i><b>9.3.2</b> 网格和点云分类</a></li>
<li class="chapter" data-level="9.3.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#graph-classification"><i class="fa fa-check"></i><b>9.3.3</b> 图分类</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#pooling-with-mapper-on-graphs-and-data-classification"><i class="fa fa-check"></i><b>9.4</b> 在图上用映射器（mapper）算法池化和数据分类</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-classification-cc-pooling-with-input-vertex-and-edge-features"><i class="fa fa-check"></i><b>9.4.1</b> 网格分类</a></li>
<li class="chapter" data-level="9.4.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-classification-cc-pooling-with-input-vertex-features-only"><i class="fa fa-check"></i><b>9.4.2</b> 网格分类：仅带输入顶点特征的CC-pooling</a></li>
<li class="chapter" data-level="9.4.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#point-cloud-classification-cc-pooling-with-input-vertex-features-only"><i class="fa fa-check"></i><b>9.4.3</b> 点云分类：仅带输入顶点特征得CC-pooling</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#ablation-studies"><i class="fa fa-check"></i><b>9.5</b> 消融实验</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>10</b> 相关工作</a>
<ul>
<li class="chapter" data-level="10.1" data-path="related-work.html"><a href="related-work.html#graph-based-models"><i class="fa fa-check"></i><b>10.1</b> 基于图的模型</a></li>
<li class="chapter" data-level="10.2" data-path="related-work.html"><a href="related-work.html#attention-based-models"><i class="fa fa-check"></i><b>10.2</b> 基于注意力的模型</a></li>
<li class="chapter" data-level="10.3" data-path="related-work.html"><a href="related-work.html#graph-based-pooling"><i class="fa fa-check"></i><b>10.3</b> 基于图的池化</a></li>
<li class="chapter" data-level="10.4" data-path="related-work.html"><a href="related-work.html#applied-algebraic-topology"><i class="fa fa-check"></i><b>10.4</b> 代数拓扑的应用</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>11</b> 结论</a></li>
<li class="appendix"><span><b>附录</b></span></li>
<li class="chapter" data-level="A" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>A</b> 术语</a></li>
<li class="chapter" data-level="B" data-path="lifting-maps.html"><a href="lifting-maps.html"><i class="fa fa-check"></i><b>B</b> 提升映射</a>
<ul>
<li class="chapter" data-level="B.1" data-path="lifting-maps.html"><a href="lifting-maps.html#n-hop-cc-of-a-graph"><i class="fa fa-check"></i><b>B.1</b> 图的n-hop CC</a></li>
<li class="chapter" data-level="B.2" data-path="lifting-maps.html"><a href="lifting-maps.html#path-based-and-subgraph-based-cc-of-a-graph"><i class="fa fa-check"></i><b>B.2</b> 图的基于路径和基于子图的CC</a></li>
<li class="chapter" data-level="B.3" data-path="lifting-maps.html"><a href="lifting-maps.html#loop-based-cc-of-a-graph"><i class="fa fa-check"></i><b>B.3</b> Loop-based CC of a graph</a></li>
<li class="chapter" data-level="B.4" data-path="lifting-maps.html"><a href="lifting-maps.html#coface-cc-of-a-simplicial-complex-or-of-a-cc"><i class="fa fa-check"></i><b>B.4</b> Coface CC of a simplicial complex or of a CC</a></li>
<li class="chapter" data-level="B.5" data-path="lifting-maps.html"><a href="lifting-maps.html#augmentation-of-ccs-by-higher-rank-cells"><i class="fa fa-check"></i><b>B.5</b> Augmentation of CCs by higher-rank cells</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="ccnn-architecture-search-and-topological-quantum-field-theories.html"><a href="ccnn-architecture-search-and-topological-quantum-field-theories.html"><i class="fa fa-check"></i><b>C</b> CCNN architecture search and topological quantum field theories</a></li>
<li class="chapter" data-level="D" data-path="learning-discrete-exterior-calculus-operators-with-ccanns.html"><a href="learning-discrete-exterior-calculus-operators-with-ccanns.html"><i class="fa fa-check"></i><b>D</b> Learning discrete exterior calculus operators with CCANNs</a></li>
<li class="chapter" data-level="E" data-path="a-mapper-induced-topology-preserving-cc-pooling-operation.html"><a href="a-mapper-induced-topology-preserving-cc-pooling-operation.html"><i class="fa fa-check"></i><b>E</b> A mapper-induced topology-preserving CC-pooling operation</a></li>
<li class="chapter" data-level="" data-path="参考文献.html"><a href="参考文献.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">拓扑深度学习：超越图数据</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="related-work" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">第 10 章</span> 相关工作<a href="related-work.html#related-work" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>拓扑深度学习（TDL）是最近出现的一个新的研究前沿，它处于几何和拓扑机器学习以及网络科学等多个领域的交叉点。为了说明拓扑深度学习在现有文献中的位置，我们回顾了之前的大量工作，并将其分为基于图的模型、高阶深度学习模型、基于图的池化、基于注意力的模型和应用代数拓扑。</p>
<div id="graph-based-models" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> 基于图的模型<a href="related-work.html#graph-based-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>基于图的模型已被广泛用于不同系统元素（顶点）之间成对相互作用（边）的建模，包括社会系统（如社会网络分析）和生物系统（如蛋白质-蛋白质相互作用），更多内容参见<span class="citation">(<a href="#ref-knoke2019social">Knoke and Yang 2019</a>; <a href="#ref-jha2022prediction">Jha, Saha, and Singh 2022</a>)</span>。 根据图的边或顶点属性，图可以分为无权图（无权边）、有权图（有权边）、有符号图（有符号边）、无向图或有向图（无向边或有向边）以及时空图（时空顶点），详见<span class="citation">(<a href="#ref-goyal2018graph">Goyal and Ferrara 2018</a>; <a href="#ref-wu2020comprehensive">Zonghan Wu et al. 2020</a>)</span>。这些图类型中的每一种都可以与神经网络相结合，形成图神经网络，并对各种系统中的不同交互作用进行建模 <span class="citation">(<a href="#ref-goyal2018graph">Goyal and Ferrara 2018</a>; <a href="#ref-wu2020comprehensive">Zonghan Wu et al. 2020</a>)</span>。例如，基于非加权和非定向图的模型已被用于 omic 数据映射<span class="citation">(<a href="#ref-amar2014constructing">Amar and Shamir 2014</a>)</span>和社交网络中的相互友谊检测<span class="citation">(<a href="#ref-tabassum2018social">Tabassum et al. 2018</a>)</span>；基于加权图的模型已被广泛应用于交通预测<span class="citation">Q. Zhang et al. (<a href="#ref-zhang2018kernel">2018</a>)</span>和流行病学建模/预测<span class="citation">Manrı́quez, Guerrero-Nancuante, and Taramasco (<a href="#ref-manriquez2021protection">2021</a>)</span>；基于符号图的模型适用于分割<span class="citation">(<a href="#ref-bailoni2022gasp">Bailoni et al. 2022</a>)</span>和聚类<span class="citation">(<a href="#ref-kunegis2010spectral">Kunegis et al. 2010</a>; <a href="#ref-gallier2016spectral">Gallier 2016</a>)</span>等任务；基于时空图的模型可以描述具有时空性质的系统，如人类活动和不同类型的运动<span class="citation">(<a href="#ref-yan2018spatial">Yan, Xiong, and Lin 2018</a>; <a href="#ref-bhattacharya2020step">Bhattacharya et al. 2020</a>; <a href="#ref-plizzari2021spatial">Plizzari, Cannici, and Matteucci 2021</a>)</span>。</p>
<p>由于利用单层（single-layer 或 monolayer）图的基于图的方法无法模拟网络中顶点之间的多种类型关系<span class="citation">(<a href="#ref-goyal2018graph">Goyal and Ferrara 2018</a>; <a href="#ref-wu2020comprehensive">Zonghan Wu et al. 2020</a>)</span>，因此有人提出了多层或多重网络<span class="citation">(<a href="#ref-kivela2014multilayer">Kivelä et al. 2014</a>; <a href="#ref-zhang2020multiplex">W. Zhang et al. 2020</a>; <a href="#ref-chang2022graphrr">Chang et al. 2022</a>)</span>。与单层图类似，多重网络包含顶点和边，但边存在于不同的层中，每一层代表一种特定类型的交互或关系。多层网络已被用于各种应用，包括人脑的多层建模 <span class="citation">(<a href="#ref-de2017multilayer">De Domenico 2017</a>; <a href="#ref-anand2023hodge">Anand and Chung 2023</a>)</span> 和在线游戏 <span class="citation">(<a href="#ref-chang2022graphrr">Chang et al. 2022</a>)</span>。所有这些类型的网络都只能模拟顶点之间的成对关系，因此需要更高阶的网络，这将在第 <a href="motivation.html#the-utility-of-topology">2.2</a>节中讨论。
## 高阶深度学习模型{#higher-order-deep-learning-models}</p>
<p>近年来，人们对高阶网络越来越感兴趣<span class="citation">(<a href="#ref-mendel1991tutorial">Mendel 1991</a>; <a href="#ref-battiston2020networks">Battiston et al. 2020</a>; <a href="#ref-bick2021higher">Bick et al. 2021</a>)</span>，因为这些网络能够充分捕捉高阶交互。在信号处理和深度学习文献中，已经为高阶网络开发了霍奇理论（Hodge-theoretic）方法、信息传递方案和跳转连接。</p>
<p>单纯复形上的霍奇理论方法<span class="citation">(<a href="#ref-lim2020hodge">Lim 2020</a>)</span>已由<span class="citation">(<a href="#ref-barbarossa2020topological">Barbarossa and Sardellitti 2020a</a>; <a href="#ref-schaub2021signal">Schaub et al. 2021</a>)</span>引入。这项工作由<span class="citation">(<a href="#ref-barbarossa2016introduction">Barbarossa and Tsitsvero 2016</a>; <a href="#ref-schaub2021signal">Schaub et al. 2021</a>)</span>扩展到超图，由<span class="citation">(<a href="#ref-roddenberry2021signal">T. Mitchell Roddenberry, Schaub, and Hajij 2022</a>; <a href="#ref-sardellitti2021topological">Sardellitti, Barbarossa, and Testa 2021</a>)</span>扩展到胞腔复形。<span class="citation">T. Mitchell Roddenberry and Segarra (<a href="#ref-roddenberry2019hodgenet">2019</a>)</span>]的工作通过利用1-霍奇拉普拉斯算子（1-Hodge Laplacian operator）进行线性过滤，定义了一种基于边的卷积神经网络<span class="citation">(<a href="#ref-barbarossa2018learning">Barbarossa, Sardellitti, and Ceci 2018</a>; <a href="#ref-schaub2018denoising">Schaub and Segarra 2018</a>; <a href="#ref-barbarossa2020topological">Barbarossa and Sardellitti 2020a</a>, <a href="#ref-barbarossa2020topologicalmag">2020b</a>; <a href="#ref-schaub2021signal">Schaub et al. 2021</a>)</span>。</p>
<p>针对高阶神经网络的卷积算子和消息传递算法已经被开发出来。例如，<span class="citation">J. Jiang et al. (<a href="#ref-jiang2019dynamic">2019</a>)</span>提出了超图上的卷积算子，<span class="citation">Gong, Higham, and Zygalakis (<a href="#ref-gong2023generative">2023</a>)</span>对其进行了进一步研究。 最近，文献<span class="citation">(<a href="#ref-huang2021unignn">Jing Huang and Yang 2021</a>)</span>
提出了在图和超图上学习的统一框架。<span class="citation">Y. Gao et al. (<a href="#ref-gao2022hgnn">2022</a>)</span>]中的作者介绍了所谓的通用超图神经网络，它构成了一个多模式/多类型数据关联建模框架。至于复形上的消息传递，文献<span class="citation">(<a href="#ref-hajijcell">Hajij, Istvan, and Zamzmi 2020</a>)</span>的工作引入了一个高阶消息传递框架，其中包含了<span class="citation">(<a href="#ref-gilmer2017neural">Gilmer et al. 2017</a>; <a href="#ref-bunch2020simplicial">Bunch et al. 2020</a>; <a href="#ref-ebli2020simplicial">Ebli, Defferrard, and Spreemann 2020</a>; <a href="#ref-hayhoe2022stable">Hayhoe et al. 2022</a>)</span>提出的框架，并利用了各种局部邻域聚合方案。 在文献<span class="citation">(<a href="#ref-mitchell2022topological">E. C. Mitchell et al. 2022</a>)</span>中，提出了递归单纯形神经网络，并将其应用于轨迹预测。在<span class="citation">(<a href="#ref-calmon2022higher">Calmon, Schaub, and Bianconi 2022</a>)</span>中，作者通过在高阶网络上引入一种利用狄拉克算子（Dirac operator.）的耦合多信号方法，解决了同时处理多个胞腔维度上支持的信号的难题。 最近还介绍了一些单纯形和胞腔神经网络，包括<span class="citation">(<a href="#ref-burnssimplicial">Burns and Fukai 2023</a>; <a href="#ref-bodnar2021weisfeiler">Bodnar et al. 2021</a>; <a href="#ref-roddenberry2021signal">T. Mitchell Roddenberry, Schaub, and Hajij 2022</a>; <a href="#ref-sardellitti2021topological">Sardellitti, Barbarossa, and Testa 2021</a>; <a href="#ref-sardellitti2022topological">Sardellitti and Barbarossa 2022</a>; <a href="#ref-battiloro2023topological">Battiloro et al. 2023</a>; <a href="#ref-yang2023convolutional">Yang and Isufi 2023</a>)</span>。更多详情，请读者参阅 <span class="citation">(<a href="#ref-mathilde2023">Papillon et al. 2023</a>)</span> 最近关于 TDL 的研究。</p>
<p>文献 <span class="citation">(<a href="#ref-hajij2022high">Hajij, Ramamurthy, et al. 2022</a>)</span>将跨层连接<span class="citation">(<a href="#ref-ronneberger2015u">Ronneberger, Fischer, and Brox 2015</a>; <a href="#ref-he2016deep">He et al. 2016</a>)</span>推广到了介绍了单纯复形上，使得可以训练高阶深度神经网络。文献<span class="citation">(<a href="#ref-morris2019weisfeiler">Morris et al. 2019</a>)</span>中提出了一种高阶图神经网络，它考虑到了多种尺度的高阶图结构。虽然这些方法允许多向分层耦合，但耦合是各向同性的，无法学习特定多向连接内的权重差异。但是，基于注意力的模型可以缓解这些限制。</p>
<p>高阶模型在一些现实世界的应用中取得了可喜的性能，包括连接预测（link prediction）<span class="citation">Yuzhou Chen, Gel, and Poor (<a href="#ref-chen2021bscnets">2022</a>)</span>、动作识别（action recognition）<span class="citation">(<a href="#ref-wang2023survey">C. Wang et al. 2023</a>)</span>、视觉分类（visual classification）<span class="citation">(<a href="#ref-shi2018hypergraph">Shi et al. 2018</a>)</span>、最优同源生成器检测（optimal homology generator detection）<span class="citation">(<a href="#ref-keros2021dist2cycle">Keros, Nanda, and Subr 2022</a>)</span>、时间序列（time series）<span class="citation">(<a href="#ref-santoro2023higher">Andrea Santoro et al. 2023</a>)</span>、动力学系统（dynamical systems ）<span class="citation">(<a href="#ref-majhi2022dynamics">Majhi, Perc, and Ghosh 2022</a>)</span>、谱聚类（ spectral clustering）<span class="citation">(<a href="#ref-reddy2023clustering">Reddy, Chepuri, and Borgnat 2023</a>)</span>、节点分类（node classification）<span class="citation">(<a href="#ref-hajij2022high">Hajij, Ramamurthy, et al. 2022</a>)</span>和轨迹预测（trajectory prediction ）<span class="citation">T. Mitchell Roddenberry, Glaze, and Segarra (<a href="#ref-roddenberry2021principled">2021</a>)</span>。</p>
</div>
<div id="attention-based-models" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> 基于注意力的模型<a href="related-work.html#attention-based-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>现实世界的关系数据庞大、非结构化、稀疏且嘈杂。因此，图神经网络（GNN）可能会学习到次优的数据表示，从而表现出尚可接受的性能<span class="citation">(<a href="#ref-wu2020comprehensive">Zonghan Wu et al. 2020</a>; <a href="#ref-asif2021graph">Asif et al. 2021</a>; <a href="#ref-dai2021nrgnn">Dai, Aggarwal, and Wang 2021</a>)</span>。为了解决这些问题，各种注意力机制<span class="citation">(<a href="#ref-chaudhari2021attentive">Chaudhari et al. 2021</a>)</span>已被纳入 GNN，它们允许学习神经架构，以检测给定图形中最相关的部分，同时忽略不相关的部分。根据所使用的注意机制，现有的图注意方法可分为基于权重的注意、基于相似性的注意和基于注意引导的游走 <span class="citation">(<a href="#ref-boaz2019">J. B. Lee et al. 2019</a>)</span>。</p>
<p>除了<span class="citation">Goh, Bodnar, and Lio (<a href="#ref-goh2022simplicial">2022</a>)</span>之外，大多数基于注意力的机制都是为图数据而设计的。例如，<span class="citation">(<a href="#ref-goh2022simplicial">Goh, Bodnar, and Lio 2022</a>)</span>提出的注意力模型是对<span class="citation">(<a href="#ref-velickovic2017graph">Veličković et al. 2018</a>)</span>的图注意力模型的推广。在<span class="citation">(<a href="#ref-giusti2022simplicial">L. Giusti, Battiloro, Di Lorenzo, et al. 2022</a>)</span>中，作者利用基于霍奇分解（Hodge decomposition）的模型（类似于<span class="citation">(<a href="#ref-roddenberry2021principled">T. Mitchell Roddenberry, Glaze, and Segarra 2021</a>)</span>中提出的模型）提出了单纯复形上的注意力模型。文献<span class="citation">S. Bai, Zhang, and Torr (<a href="#ref-bai2021hypergraph">2021</a>)</span>中引入的超图注意力模型为<span class="citation">(<a href="#ref-velickovic2017graph">Veličković et al. 2018</a>)</span>的图注意力模型提供了另一种泛化选择。上述注意模型既不允许也不结合不同维度实体的高阶注意块。这限制了神经架构的空间和现有注意力模型的应用范围。</p>
</div>
<div id="graph-based-pooling" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> 基于图的池化<a href="related-work.html#graph-based-pooling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>为了在图的背景下模仿基于图像的池化层的成功，已经进行了多次尝试。一些早期工作采用了流行的图聚类算法 <span class="citation">(<a href="#ref-kushnir2006fast">Kushnir, Galun, and Brandt 2006</a>; <a href="#ref-dhillon2007weighted">Dhillon, Guan, and Kulis 2007</a>)</span> 来实现基于图的池化架构 <span class="citation">(<a href="#ref-bruna2013spectral">Bruna et al. 2014</a>)</span>。粗化操作被应用于图，以获得学习任务所需的不变性<span class="citation">(<a href="#ref-ying2018hierarchical">Ying et al. 2018</a>; <a href="#ref-mesquita2020rethinking">Mesquita, Souza, and Kaski 2020</a>; <a href="#ref-gao2021topology">H. Gao, Liu, and Ji 2021</a>)</span>。目前最先进的基于图的池化方法大多依赖于学习任务所需的动态池化学习<span class="citation">(<a href="#ref-grattarola2022understanding">Grattarola et al. 2022</a>)</span>，这包括谱方法（spectral methods ） <span class="citation">(<a href="#ref-ma2019graph">Ma et al. 2019</a>)</span>、聚类方法（如 DiffPool <span class="citation">(<a href="#ref-ying2018hierarchical">Ying et al. 2018</a>)</span> 和 MinCut <span class="citation">(<a href="#ref-bianchi2020spectral">Bianchi, Grattarola, and Alippi 2020</a>)</span>）、top-K 方法 <span class="citation">J. Lee, Lee, and Kang (<a href="#ref-lee2019self">2019</a>)</span> 和分层图池<span class="citation">(<a href="#ref-huang2019attpool">Jingjia Huang et al. 2019</a>; <a href="#ref-lee2019self">J. Lee, Lee, and Kang 2019</a>; <a href="#ref-zhang2019hierarchical">Z. Zhang et al. 2019</a>, <a href="#ref-zhang2021hierarchical">2021</a>; <a href="#ref-li2020graph">J. Li et al. 2020</a>; <a href="#ref-pang2021graph">Pang, Zhao, and Li 2021</a>)</span>。除了文献<span class="citation">(<a href="#ref-cinque2022pooling">Cinque, Battiloro, and Di Lorenzo 2022</a>)</span>按照<span class="citation">(<a href="#ref-grattarola2022understanding">Grattarola et al. 2022</a>)</span>提出的思路开发了一种单纯复形池化策略外，对高阶网络上的池化仍未进行研究。</p>
</div>
<div id="applied-algebraic-topology" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> 代数拓扑的应用<a href="related-work.html#applied-algebraic-topology" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>虽然代数拓扑学<span class="citation">(<a href="#ref-hatcher2005algebraic">Hatcher 2005</a>)</span>是一个相对古老的领域，但该领域的应用直到最近才开始具体化<span class="citation">(<a href="#ref-carlsson2009topology">G. Carlsson 2009</a>; <a href="#ref-edelsbrunner2010computational">Edelsbrunner and Harer 2010</a>)</span>。 事实上，人们发现拓扑结构是解决许多领域长期存在的问题的天然工具。 例如，持久同源（persistent homology）<span class="citation">(<a href="#ref-edelsbrunner2010computational">Edelsbrunner and Harer 2010</a>)</span>已在各种复杂数据问题的解决中被成功应用<span class="citation">Carr, Snoeyink, and Panne (<a href="#ref-carr2004simplifying">2004</a>)</span>。近年来，人们对拓扑学在机器学习和数据科学中的作用越来越感兴趣<span class="citation">(<a href="#ref-hensel2021survey">Hensel, Moor, and Rieck 2021</a>; <a href="#ref-DW22">Dey and Wang 2022b</a>)</span>。</p>
<p>基于拓扑结构的机器学习模型已被应用于许多领域，包括数据的拓扑签名（topological signatures）<span class="citation">(<a href="#ref-biasotti2008describing">Biasotti et al. 2008</a>; <a href="#ref-carlsson2005persistence">G. Carlsson et al. 2005</a>; <a href="#ref-rieck2015persistent">Rieck and Leitte 2015</a>)</span>、神经科学<span class="citation">H. Lee et al. (<a href="#ref-LeeChungKang2011b">2011a</a>)</span>, 生物科学 <span class="citation">Topaz, Ziegelmeier, and Halverson (<a href="#ref-topaz2015topological">2015</a>)</span>、图的研究<span class="citation">Hajij and Rosen (<a href="#ref-hajij2020efficient">2020</a>)</span>、时间序列预测<span class="citation">(<a href="#ref-zeng_topological_2021">Zeng et al. 2021</a>)</span>、木马检测<span class="citation">(<a href="#ref-hu_trigger_2022">Hu et al. 2022</a>)</span>、图像分割<span class="citation">(<a href="#ref-hu_topology-preserving_2019">Hu et al. 2019</a>)</span>、三维重建<span class="citation">(<a href="#ref-waibel_capturing_2022">Waibel et al. 2022</a>)</span>、
以及时变设置（time-varying setups ） <span class="citation">(<a href="#ref-edelsbrunner2004time">Edelsbrunner et al. 2004</a>; <a href="#ref-perea2015sw1pers">Perea et al. 2015</a>; <a href="#ref-maletic2016persistent">Maletić, Zhao, and Rajković 2016</a>; <a href="#ref-rieck_uncovering_2020">Rieck et al. 2020</a>)</span>。</p>
<p>拓扑数据分析（Topological data analysis ，TDA）<span class="citation">Dey and Wang (<a href="#ref-DW22">2022b</a>)</span>已成为一个利用拓扑工具分析数据和开发机器学习算法的科学领域。TDA 在机器学习领域有许多应用，包括增强现有的机器学习模型 <span class="citation">Leventhal et al. (<a href="#ref-leventhal2023exploring">2023</a>)</span>、提高深度学习模型的可解释性<span class="citation">(<a href="#ref-carlsson2020topological">G. Carlsson and Gabrielsson 2020</a>; <a href="#ref-elhamdadi2021affectivetda">Elhamdadi, Canavan, and Rosen 2021</a>; <a href="#ref-love2023topological">Love et al. 2023a</a>)</span>、降维<span class="citation">(<a href="#ref-moor2020topological">Moor et al. 2020</a>)</span>、过滤学习（ filtration learning ）<span class="citation">(<a href="#ref-hofer2020graph">Hofer et al. 2020</a>)</span>和拓扑层构造<span class="citation">(<a href="#ref-kim2020pllay">K. Kim et al. 2020</a>)</span>。一个值得注意的研究趋势是持久图（persistence diagrams）的矢量化，构建持久图的矢量表示是为了在下游机器学习任务中加以利用。这些方法包括贝蒂曲线 （Betti curves）<span class="citation">(<a href="#ref-umeda2017time">Umeda 2017</a>)</span>、持久性景观（persistence landscapes） <span class="citation">(<a href="#ref-bubenik2015statistical">Bubenik 2015</a>)</span>、持久性图像 （persistence images）<span class="citation">(<a href="#ref-adams2017persistence">Adams et al. 2017</a>)</span>，以及其他矢量化构造 <span class="citation">Berry et al. (<a href="#ref-berry2020functional">2020</a>)</span>。最近，文献<span class="citation">(<a href="#ref-carriere_perslay_2020">Carriere et al. 2020</a>)</span>也提出了这些方法的一种统一框架。</p>
<p>我们的工作引入了组合复形（combinatorial complexes，CC），将其作为一种广义的高阶网络，在此网络上可以统一的方式定义和研究深度学习模型。因此，我们的工作扩展了 TDA，用拓扑术语形式化了深度学习概念，并用我们的 TDL 框架实现了 TDA 中的构造，如 mapper <span class="citation">(<a href="#ref-singh2007topological">Singh et al. 2007</a>)</span>。CC 和组合复形神经网络（combinatorial complex neural networks，CCNNs，定义在 CC 上的神经网络），其构建受到代数拓扑学<span class="citation">(<a href="#ref-hatcher2005algebraic">Hatcher 2005</a>)</span>和拓扑量子场论（topological quantum field theory ）<span class="citation">(<a href="#ref-turaev2016quantum">Turaev 2016</a>)</span>中经典概念的启发，也受到 了TDA <span class="citation">G. Carlsson et al. (<a href="#ref-carlsson2005persistence">2005</a>)</span>在机器学习中应用的最新进展的启发<span class="citation">(<a href="#ref-pun2018persistent">Pun, Xia, and Lee 2018</a>; <a href="#ref-DW22">Dey and Wang 2022b</a>)</span>。</p>

</div>
</div>
<h3>参考文献<a href="参考文献.html#参考文献" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-adams2017persistence" class="csl-entry">
Adams, Henry, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. 2017. <span>“Persistence Images: A Stable Vector Representation of Persistent Homology.”</span> <em>Jmlr</em> 18 (1): 218–52.
</div>
<div id="ref-amar2014constructing" class="csl-entry">
Amar, David, and Ron Shamir. 2014. <span>“Constructing Module Maps for Integrated Analysis of Heterogeneous Biological Networks.”</span> <em>Nucleic Acids Research</em> 42 (7): 4208–19.
</div>
<div id="ref-anand2023hodge" class="csl-entry">
Anand, D. V., and Moo K. Chung. 2023. <span>“Hodge <span>L</span>aplacian of Brain Networks.”</span> <em>IEEE Transactions on Medical Imaging</em>.
</div>
<div id="ref-asif2021graph" class="csl-entry">
Asif, Nurul A., Yeahia Sarker, Ripon K. Chakrabortty, Michael J. Ryan, Md. Hafiz Ahamed, Dip K. Saha, Faisal R. Badal, et al. 2021. <span>“Graph Neural Network: A Comprehensive Review on Non-<span>E</span>uclidean Space.”</span> <em>IEEE Access</em> 9: 60588–606. <a href="https://doi.org/10.1109/ACCESS.2021.3071274">https://doi.org/10.1109/ACCESS.2021.3071274</a>.
</div>
<div id="ref-bai2021hypergraph" class="csl-entry">
Bai, Song, Feihu Zhang, and Philip H. S. Torr. 2021. <span>“Hypergraph Convolution and Hypergraph Attention.”</span> <em>Pattern Recognition</em> 110: 107637.
</div>
<div id="ref-bailoni2022gasp" class="csl-entry">
Bailoni, Alberto, Constantin Pape, Nathan Hütsch, Steffen Wolf, Thorsten Beier, Anna Kreshuk, and Fred A Hamprecht. 2022. <span>“<span>GASP</span>, a Generalized Framework for Agglomerative Clustering of Signed Graphs and Its Application to Instance Segmentation.”</span> In <em>Cvpr</em>, 11645–55.
</div>
<div id="ref-barbarossa2020topological" class="csl-entry">
Barbarossa, Sergio, and Stefania Sardellitti. 2020a. <span>“Topological Signal Processing over Simplicial Complexes.”</span> <em>IEEE Transactions on Signal Processing</em> 68: 2992–3007.
</div>
<div id="ref-barbarossa2020topologicalmag" class="csl-entry">
———. 2020b. <span>“Topological Signal Processing: Making Sense of Data Building on Multiway Relations.”</span> <em>IEEE Signal Processing Magazine</em> 37 (6): 174–83.
</div>
<div id="ref-barbarossa2018learning" class="csl-entry">
Barbarossa, Sergio, Stefania Sardellitti, and Elena Ceci. 2018. <span>“Learning from Signals Defined over Simplicial Complexes.”</span> In <em>2018 IEEE Data Science Workshop (DSW)</em>, 51–55. IEEE.
</div>
<div id="ref-barbarossa2016introduction" class="csl-entry">
Barbarossa, Sergio, and Mikhail Tsitsvero. 2016. <span>“An Introduction to Hypergraph Signal Processing.”</span> In <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 6425–29. IEEE.
</div>
<div id="ref-battiloro2023topological" class="csl-entry">
Battiloro, Claudio, Stefania Sardellitti, Sergio Barbarossa, and Paolo Di Lorenzo. 2023. <span>“Topological Signal Processing over Weighted Simplicial Complexes.”</span> <em>arXiv Preprint arXiv:2302.08561</em>.
</div>
<div id="ref-battiston2020networks" class="csl-entry">
Battiston, Federico, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. 2020. <span>“Networks Beyond Pairwise Interactions: Structure and Dynamics.”</span> <em>Physics Reports</em> 874: 1–92.
</div>
<div id="ref-berry2020functional" class="csl-entry">
Berry, Eric, Yen-Chi Chen, Jessi Cisewski-Kehe, and Brittany Terese Fasy. 2020. <span>“Functional Summaries of Persistence Diagrams.”</span> <em>J. Appl. Comput. Topol.</em> 4 (2): 211–62.
</div>
<div id="ref-bhattacharya2020step" class="csl-entry">
Bhattacharya, Uttaran, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, and Dinesh Manocha. 2020. <span>“<span>STEP</span>: Spatial Temporal Graph Convolutional Networks for Emotion Perception from <span>G</span>aits.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 34 (02): 1342–50. <a href="https://doi.org/10.1609/aaai.v34i02.5490">https://doi.org/10.1609/aaai.v34i02.5490</a>.
</div>
<div id="ref-bianchi2020spectral" class="csl-entry">
Bianchi, Filippo Maria, Daniele Grattarola, and Cesare Alippi. 2020. <span>“Spectral Clustering with Graph Neural Networks for Graph Pooling.”</span> In <em>Icml</em>, 874–83. PMLR.
</div>
<div id="ref-biasotti2008describing" class="csl-entry">
Biasotti, Silvia, Leila De Floriani, Bianca Falcidieno, Patrizio Frosini, Daniela Giorgi, Claudia Landi, Laura Papaleo, and Michela Spagnuolo. 2008. <span>“Describing Shapes by Geometrical-Topological Properties of Real Functions.”</span> <em>ACM Computing Surveys (CSUR)</em> 40 (4): 12.
</div>
<div id="ref-bick2021higher" class="csl-entry">
Bick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T Schaub. 2021. <span>“What Are Higher-Order Networks?”</span> <em>arXiv Preprint arXiv:2104.11329</em>.
</div>
<div id="ref-bodnar2021weisfeiler" class="csl-entry">
Bodnar, Cristian, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. 2021. <span>“Weisfeiler and <span>L</span>ehman Go Cellular: <span>CW</span> Networks.”</span> In <em>Advances in Neural Information Processing Systems</em>.
</div>
<div id="ref-bruna2013spectral" class="csl-entry">
Bruna, Joan, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. <span>“Spectral Networks and Locally Connected Networks on Graphs.”</span> In <em>Proceedings of the 2nd International Conference on Learning Representations</em>, edited by Yoshua Bengio and Yann LeCun. <span>ICLR</span> 2014. Banff, AB, Canada.
</div>
<div id="ref-bubenik2015statistical" class="csl-entry">
Bubenik, Peter. 2015. <span>“Statistical Topological Data Analysis Using Persistence Landscapes.”</span> <em>Jmlr</em> 16 (1): 77–102.
</div>
<div id="ref-bunch2020simplicial" class="csl-entry">
Bunch, Eric, Qian You, Glenn Fung, and Vikas Singh. 2020. <span>“Simplicial 2-Complex Convolutional Neural Nets.”</span> <em>NeurIPS Workshop on Topological Data Analysis and Beyond</em>.
</div>
<div id="ref-burnssimplicial" class="csl-entry">
Burns, Thomas F., and Tomoki Fukai. 2023. <span>“Simplicial <span>H</span>opfield Networks.”</span> In <em>The Eleventh International Conference on Learning Representations</em>.
</div>
<div id="ref-calmon2022higher" class="csl-entry">
Calmon, Lucille, Michael T. Schaub, and Ginestra Bianconi. 2022. <span>“Higher-Order Signal Processing with the <span>D</span>irac Operator.”</span> In <em>Asilomar Conference on Signals, Systems, and Computers</em>.
</div>
<div id="ref-carlsson2009topology" class="csl-entry">
Carlsson, Gunnar. 2009. <span>“Topology and Data.”</span> <em>Bulletin of the American Mathematical Society</em> 46 (2): 255–308.
</div>
<div id="ref-carlsson2020topological" class="csl-entry">
Carlsson, Gunnar, and Rickard Brüel Gabrielsson. 2020. <span>“Topological Approaches to Deep Learning.”</span> In <em>Topological Data Analysis: The Abel Symposium 2018</em>, 119–46. Springer; Springer.
</div>
<div id="ref-carlsson2005persistence" class="csl-entry">
Carlsson, Gunnar, Afra Zomorodian, Anne Collins, and Leonidas J Guibas. 2005. <span>“Persistence Barcodes for Shapes.”</span> <em>International Journal of Shape Modeling</em> 11 (02): 149–87.
</div>
<div id="ref-carr2004simplifying" class="csl-entry">
Carr, Hamish, Jack Snoeyink, and Michiel van de Panne. 2004. <span>“Simplifying Flexible Isosurfaces Using Local Geometric Measures.”</span> In <em><span>IEEE</span> Visualization</em>, 497–504. IEEE.
</div>
<div id="ref-carriere_perslay_2020" class="csl-entry">
Carriere, Mathieu, Frederic Chazal, Yuichi Ike, Theo Lacombe, Martin Royer, and Yuhei Umeda. 2020. <span>“<span>PersLay</span>: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures.”</span> In <em>Proceedings of the <span>Twenty</span> <span>Third</span> <span>International</span> <span>Conference</span> on <span>Artificial</span> <span>Intelligence</span> and <span>Statistics</span></em>, 2786–96. PMLR.
</div>
<div id="ref-chang2022graphrr" class="csl-entry">
Chang, Yaomin, Lin Shu, Erxin Du, Chuan Chen, Ziyang Zhang, Zibin Zheng, Yuzhao Huang, and Xingxing Xing. 2022. <span>“Graph<span>RR</span>: A Multiplex Graph Based Reciprocal Friend Recommender System with Applications on Online Gaming Service.”</span> <em>Knowledge-Based Systems</em> 251: 109187.
</div>
<div id="ref-chaudhari2021attentive" class="csl-entry">
Chaudhari, Sneha, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. 2021. <span>“An Attentive Survey of Attention Models.”</span> <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 12 (5): 1–32.
</div>
<div id="ref-chen2021bscnets" class="csl-entry">
Chen, Yuzhou, Yulia R. Gel, and H. Vincent Poor. 2022. <span>“<span>BS</span>c<span>N</span>ets: Block Simplicial Complex Neural Networks.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 36 (6): 6333–41. <a href="https://doi.org/10.1609/aaai.v36i6.20583">https://doi.org/10.1609/aaai.v36i6.20583</a>.
</div>
<div id="ref-cinque2022pooling" class="csl-entry">
Cinque, Domenico Mattia, Claudio Battiloro, and Paolo Di Lorenzo. 2022. <span>“Pooling Strategies for Simplicial Convolutional Networks.”</span> <em>arXiv Preprint arXiv:2210.05490</em>.
</div>
<div id="ref-dai2021nrgnn" class="csl-entry">
Dai, Enyan, Charu Aggarwal, and Suhang Wang. 2021. <span>“<span>NRGNN</span>: Learning a Label Noise Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs.”</span> In <em>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>, 227–36.
</div>
<div id="ref-de2017multilayer" class="csl-entry">
De Domenico, Manlio. 2017. <span>“<span class="nocase">Multilayer modeling and analysis of human brain networks</span>.”</span> <em>GigaScience</em> 6 (5). <a href="https://doi.org/10.1093/gigascience/gix004">https://doi.org/10.1093/gigascience/gix004</a>.
</div>
<div id="ref-DW22" class="csl-entry">
———. 2022b. <em>Computational Topology for Data Analysis</em>. Cambridge University Press.
</div>
<div id="ref-dhillon2007weighted" class="csl-entry">
Dhillon, Inderjit S., Yuqiang Guan, and Brian Kulis. 2007. <span>“Weighted Graph Cuts Without Eigenvectors a Multilevel Approach.”</span> <em>Pami</em> 29 (11): 1944–57.
</div>
<div id="ref-ebli2020simplicial" class="csl-entry">
Ebli, Stefania, Michaël Defferrard, and Gard Spreemann. 2020. <span>“Simplicial Neural Networks.”</span> <em>NeurIPS Workshop on Topological Data Analysis and Beyond</em>.
</div>
<div id="ref-edelsbrunner2010computational" class="csl-entry">
Edelsbrunner, Herbert, and John Harer. 2010. <em>Computational Topology: An Introduction</em>. American Mathematical Soc.
</div>
<div id="ref-edelsbrunner2004time" class="csl-entry">
Edelsbrunner, Herbert, John Harer, Ajith Mascarenhas, and Valerio Pascucci. 2004. <span>“Time-Varying <span>R</span>eeb Graphs for Continuous Space-Time Data.”</span> In <em>Proceedings of the Twentieth Annual Symposium on Computational Geometry</em>, 366–72. ACM.
</div>
<div id="ref-elhamdadi2021affectivetda" class="csl-entry">
Elhamdadi, Hamza, Shaun Canavan, and Paul Rosen. 2021. <span>“Affective<span>TDA</span>: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em> 28 (1): 769–79.
</div>
<div id="ref-gallier2016spectral" class="csl-entry">
Gallier, Jean. 2016. <span>“Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: A Survey.”</span> <em>arXiv Preprint arXiv:1601.04692</em>.
</div>
<div id="ref-gao2021topology" class="csl-entry">
Gao, Hongyang, Yi Liu, and Shuiwang Ji. 2021. <span>“Topology-Aware Graph Pooling Networks.”</span> <em>Pami</em> 43 (12): 4512–18.
</div>
<div id="ref-gao2022hgnn" class="csl-entry">
Gao, Yue, Yifan Feng, Shuyi Ji, and Rongrong Ji. 2022. <span>“<span>HGNN</span>+: General Hypergraph Neural Networks.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.
</div>
<div id="ref-gilmer2017neural" class="csl-entry">
Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. <span>“Neural Message Passing for Quantum Chemistry.”</span> In <em>International Conference on Machine Learning</em>.
</div>
<div id="ref-giusti2022simplicial" class="csl-entry">
Giusti, Lorenzo, Claudio Battiloro, Paolo Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. 2022. <span>“Simplicial Attention Networks.”</span> <em>arXiv Preprint arXiv:2203.07485</em>.
</div>
<div id="ref-goh2022simplicial" class="csl-entry">
Goh, Christopher Wei Jin, Cristian Bodnar, and Pietro Lio. 2022. <span>“Simplicial Attention Networks.”</span> In <em>ICLR 2022 Workshop on Geometrical and Topological Representation Learning</em>.
</div>
<div id="ref-gong2023generative" class="csl-entry">
Gong, Xue, Desmond J. Higham, and Konstantinos Zygalakis. 2023. <span>“Generative Hypergraph Models and Spectral Embedding.”</span> <em>Scientific Reports</em> 13 (1): 540.
</div>
<div id="ref-goyal2018graph" class="csl-entry">
Goyal, Palash, and Emilio Ferrara. 2018. <span>“Graph Embedding Techniques, Applications, and Performance: A Survey.”</span> <em>Knowledge-Based Systems</em> 151: 78–94.
</div>
<div id="ref-grattarola2022understanding" class="csl-entry">
Grattarola, Daniele, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. 2022. <span>“Understanding Pooling in Graph Neural Networks.”</span> <em>IEEE Transactions on Neural Networks and Learning Systems</em>.
</div>
<div id="ref-hajijcell" class="csl-entry">
Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. <span>“Cell Complex Neural Networks.”</span> In <em>NeurIPS 2020 Workshop TDA and Beyond</em>.
</div>
<div id="ref-hajij2022high" class="csl-entry">
Hajij, Mustafa, Karthikeyan Natesan Ramamurthy, Aldo Saenz, and Ghada Zamzmi. 2022. <span>“High Skip Networks: A Higher Order Generalization of Skip Connections.”</span> In <em>ICLR 2022 Workshop on Geometrical and Topological Representation Learning</em>.
</div>
<div id="ref-hajij2020efficient" class="csl-entry">
Hajij, Mustafa, and Paul Rosen. 2020. <span>“An Efficient Data Retrieval Parallel <span>R</span>eeb Graph Algorithm.”</span> <em>Algorithms</em> 13 (10): 258.
</div>
<div id="ref-hatcher2005algebraic" class="csl-entry">
Hatcher, Allen. 2005. <em>Algebraic Topology</em>. Cambridge University Press.
</div>
<div id="ref-hayhoe2022stable" class="csl-entry">
Hayhoe, Mikhail, Hans Riess, Victor M Preciado, and Alejandro Ribeiro. 2022. <span>“Stable and Transferable Hyper-Graph Neural Networks.”</span> <em>arXiv Preprint arXiv:2211.06513</em>.
</div>
<div id="ref-he2016deep" class="csl-entry">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 <span>IEEE</span> <span>C</span>onference on <span>C</span>omputer <span>V</span>ision and <span>P</span>attern <span>R</span>ecognition (<span>CVPR</span>)</em>, 770–78. <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.
</div>
<div id="ref-hensel2021survey" class="csl-entry">
Hensel, Felix, Michael Moor, and Bastian Rieck. 2021. <span>“A Survey of Topological Machine Learning Methods.”</span> <em>Frontiers in Artificial Intelligence</em> 4: 681108.
</div>
<div id="ref-hofer2020graph" class="csl-entry">
Hofer, Christoph, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. 2020. <span>“Graph Filtration Learning.”</span> In <em>International Conference on Machine Learning</em>, 4314–23. PMLR.
</div>
<div id="ref-hu_topology-preserving_2019" class="csl-entry">
Hu, Xiaoling, Fuxin Li, Dimitris Samaras, and Chao Chen. 2019. <span>“Topology-Preserving Deep Image Segmentation.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>. Vol. 32. Curran Associates, Inc.
</div>
<div id="ref-hu_trigger_2022" class="csl-entry">
Hu, Xiaoling, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. 2022. <span>“Trigger Hunting with a Topological Prior for <span>T</span>rojan Detection.”</span> In <em>International Conference on Learning Representations</em>.
</div>
<div id="ref-huang2019attpool" class="csl-entry">
Huang, Jingjia, Zhangheng Li, Nannan Li, Shan Liu, and Ge Li. 2019. <span>“Att<span>P</span>ool: Towards Hierarchical Feature Representation in Graph Convolutional Networks via Attention Mechanism.”</span> In <em>Iccv</em>, 6480–89.
</div>
<div id="ref-huang2021unignn" class="csl-entry">
Huang, Jing, and Jie Yang. 2021. <span>“Uni<span>GNN</span>: A Unified Framework for Graph and Hypergraph Neural Networks.”</span> In <em>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, <span>IJCAI</span></em>.
</div>
<div id="ref-jha2022prediction" class="csl-entry">
Jha, Kanchan, Sriparna Saha, and Hiteshi Singh. 2022. <span>“Prediction of Protein–Protein Interaction Using Graph Neural Networks.”</span> <em>Scientific Reports</em> 12 (1): 1–12.
</div>
<div id="ref-jiang2019dynamic" class="csl-entry">
Jiang, Jianwen, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. 2019. <span>“Dynamic Hypergraph Neural Networks.”</span> In <em>IJCAI</em>, 2635–41.
</div>
<div id="ref-keros2021dist2cycle" class="csl-entry">
Keros, Alexandros D., Vidit Nanda, and Kartic Subr. 2022. <span>“Dist2Cycle: A Simplicial Neural Network for Homology Localization.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 36 (7): 7133–42. <a href="https://doi.org/10.1609/aaai.v36i7.20673">https://doi.org/10.1609/aaai.v36i7.20673</a>.
</div>
<div id="ref-kim2020pllay" class="csl-entry">
Kim, Kwangho, Jisu Kim, Manzil Zaheer, Joon Kim, Frédéric Chazal, and Larry Wasserman. 2020. <span>“Pllay: Efficient Topological Layer Based on Persistent Landscapes.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 15965–77.
</div>
<div id="ref-kivela2014multilayer" class="csl-entry">
Kivelä, Mikko, Alex Arenas, Marc Barthelemy, James P Gleeson, Yamir Moreno, and Mason A Porter. 2014. <span>“Multilayer Networks.”</span> <em>Journal of Complex Networks</em> 2 (3): 203–71.
</div>
<div id="ref-knoke2019social" class="csl-entry">
Knoke, David, and Song Yang. 2019. <em>Social Network Analysis</em>. SAGE publications.
</div>
<div id="ref-kunegis2010spectral" class="csl-entry">
Kunegis, Jérôme, Stephan Schmidt, Andreas Lommatzsch, Jürgen Lerner, Ernesto W De Luca, and Sahin Albayrak. 2010. <span>“Spectral Analysis of Signed Graphs for Clustering, Prediction and Visualization.”</span> In <em>Proceedings of the 2010 SIAM International Conference on Data Mining</em>, 559–70. SIAM.
</div>
<div id="ref-kushnir2006fast" class="csl-entry">
Kushnir, Dan, Meirav Galun, and Achi Brandt. 2006. <span>“Fast Multiscale Clustering and Manifold Identification.”</span> <em>Pattern Recognition</em> 39 (10): 1876–91.
</div>
<div id="ref-LeeChungKang2011b" class="csl-entry">
Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Boong-Nyun Kim, and Dong Soo Lee. 2011a. <span>“Computing the Shape of Brain Networks Using Graph Filtration and Gromov-Hausdorff Metric.”</span> <em>International Conference on Medical Image Computing and Computer Assisted Intervention</em>, 302–9.
</div>
<div id="ref-boaz2019" class="csl-entry">
Lee, John Boaz, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. 2019. <span>“Attention Models in Graphs: A Survey.”</span> <em>ACM Transactions on Knowledge Discovery from Data</em> 13 (6): 1–25.
</div>
<div id="ref-lee2019self" class="csl-entry">
Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. <span>“Self-Attention Graph Pooling.”</span> In <em>Icml</em>, 3734–43. PMLR.
</div>
<div id="ref-leventhal2023exploring" class="csl-entry">
Leventhal, Samuel, Attila Gyulassy, Mark Heimann, and Valerio Pascucci. 2023. <span>“Exploring Classification of Topological Priors with Machine Learning for Feature Extraction.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em>.
</div>
<div id="ref-li2020graph" class="csl-entry">
Li, Juanhui, Yao Ma, Yiqi Wang, Charu Aggarwal, Chang-Dong Wang, and Jiliang Tang. 2020. <span>“Graph Pooling with Representativeness.”</span> In <em>2020 IEEE International Conference on Data Mining (ICDM)</em>, 302–11. IEEE.
</div>
<div id="ref-lim2020hodge" class="csl-entry">
Lim, Lek-Heng. 2020. <span>“<span>H</span>odge <span>L</span>aplacians on Graphs.”</span> <em><span>SIAM</span> Review</em> 62 (3): 685–715.
</div>
<div id="ref-love2023topological" class="csl-entry">
Love, Ephy R., Benjamin Filippenko, Vasileios Maroulas, and Gunnar Carlsson. 2023a. <span>“Topological Convolutional Layers for Deep Learning.”</span> <em>Jmlr</em> 24 (59): 1–35.
</div>
<div id="ref-ma2019graph" class="csl-entry">
Ma, Yao, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019. <span>“Graph Convolutional Networks with Eigenpooling.”</span> In <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, 723–31.
</div>
<div id="ref-majhi2022dynamics" class="csl-entry">
Majhi, Soumen, Matjaž Perc, and Dibakar Ghosh. 2022. <span>“Dynamics on Higher-Order Networks: A Review.”</span> <em>Journal of the Royal Society Interface</em> 19 (188): 20220043.
</div>
<div id="ref-maletic2016persistent" class="csl-entry">
Maletić, Slobodan, Yi Zhao, and Milan Rajković. 2016. <span>“Persistent Topological Features of Dynamical Systems.”</span> <em>Chaos: An Interdisciplinary Journal of Nonlinear Science</em> 26 (5): 053105.
</div>
<div id="ref-manriquez2021protection" class="csl-entry">
Manrı́quez, Ronald, Camilo Guerrero-Nancuante, and Carla Taramasco. 2021. <span>“Protection Strategy Against an Epidemic Disease on Edge-Weighted Graphs Applied to a <span>COVID</span>-19 Case.”</span> <em>Biology</em> 10 (7): 667.
</div>
<div id="ref-mendel1991tutorial" class="csl-entry">
Mendel, Jerry M. 1991. <span>“Tutorial on Higher-Order Statistics (Spectra) in Signal Processing and System Theory: Theoretical Results and Some Applications.”</span> <em>Proceedings of the IEEE</em> 79 (3): 278–305.
</div>
<div id="ref-mesquita2020rethinking" class="csl-entry">
Mesquita, Diego, Amauri Souza, and Samuel Kaski. 2020. <span>“Rethinking Pooling in Graph Neural Networks.”</span> <em>Neurips</em> 33: 2220–31.
</div>
<div id="ref-mitchell2022topological" class="csl-entry">
Mitchell, Edward C., Brittany Story, David Boothe, Piotr J. Franaszczuk, and Vasileios Maroulas. 2022. <span>“A Topological Deep Learning Framework for Neural Spike Decoding.”</span> <em>arXiv Preprint arXiv:2212.05037</em>.
</div>
<div id="ref-moor2020topological" class="csl-entry">
Moor, Michael, Max Horn, Bastian Rieck, and Karsten Borgwardt. 2020. <span>“Topological Autoencoders.”</span> In <em>International Conference on Machine Learning</em>, 7045–54. PMLR.
</div>
<div id="ref-morris2019weisfeiler" class="csl-entry">
Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. <span>“Weisfeiler and <span>L</span>eman Go Neural: Higher-Order Graph Neural Networks.”</span> In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>.
</div>
<div id="ref-pang2021graph" class="csl-entry">
Pang, Yunsheng, Yunxiang Zhao, and Dongsheng Li. 2021. <span>“Graph Pooling via Coarsened Graph Infomax.”</span> In <em>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2177–81.
</div>
<div id="ref-mathilde2023" class="csl-entry">
Papillon, Mathilde, Sophia Sanborn, Mustafa Hajij, and Nina Miolane. 2023. <span>“Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.”</span> <em>arXiv Preprint arXiv:2304.10031</em>.
</div>
<div id="ref-perea2015sw1pers" class="csl-entry">
Perea, Jose A., Anastasia Deckard, Steve B. Haase, and John Harer. 2015. <span>“<span>SW</span>1<span>P</span>er<span>S</span>: Sliding Windows and 1-Persistence Scoring; Discovering Periodicity in Gene Expression Time Series Data.”</span> <em>BMC Bioinformatics</em> 16 (1): 257.
</div>
<div id="ref-plizzari2021spatial" class="csl-entry">
Plizzari, Chiara, Marco Cannici, and Matteo Matteucci. 2021. <span>“Spatial Temporal Transformer Network for Skeleton-Based Action Recognition.”</span> In <em>Icpr</em>, 694–701. Springer.
</div>
<div id="ref-pun2018persistent" class="csl-entry">
Pun, Chi Seng, Kelin Xia, and Si Xian Lee. 2018. <span>“Persistent-Homology-Based Machine Learning and Its Applications–a Survey.”</span> <em>arXiv Preprint arXiv:1811.00252</em>.
</div>
<div id="ref-reddy2023clustering" class="csl-entry">
Reddy, Thummaluru Siddartha, Sundeep Prabhakar Chepuri, and Pierre Borgnat. 2023. <span>“Clustering with Simplicial Complexes.”</span> <em>arXiv Preprint arXiv:2303.07646</em>.
</div>
<div id="ref-rieck2015persistent" class="csl-entry">
Rieck, Bastian, and Heike Leitte. 2015. <span>“Persistent Homology for the Evaluation of Dimensionality Reduction Schemes.”</span> <em>Computer Graphics Forum</em> 34 (3): 431–40.
</div>
<div id="ref-rieck_uncovering_2020" class="csl-entry">
Rieck, Bastian, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy Wolf, Nick Turk-Browne, and Smita Krishnaswamy. 2020. <span>“Uncovering the Topology of Time-Varying <span class="nocase">fMRI</span> Data Using Cubical Persistence.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span> (<span>NeurIPS</span>)</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6900–6912. Curran Associates, Inc.
</div>
<div id="ref-roddenberry2019hodgenet" class="csl-entry">
Roddenberry, T Mitchell, and Santiago Segarra. 2019. <span>“Hodge<span>N</span>et: Graph Neural Networks for Edge Data.”</span> In <em>2019 53rd Asilomar Conference on Signals, Systems, and Computers</em>, 220–24. IEEE.
</div>
<div id="ref-roddenberry2021principled" class="csl-entry">
Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021. <span>“Principled Simplicial Neural Networks for Trajectory Prediction.”</span> In <em>International Conference on Machine Learning</em>.
</div>
<div id="ref-roddenberry2021signal" class="csl-entry">
Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. <span>“Signal Processing on Cell Complexes.”</span> In <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em>.
</div>
<div id="ref-ronneberger2015u" class="csl-entry">
Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. <span>“U-<span>N</span>et: Convolutional Networks for Biomedical Image Segmentation.”</span> In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em>, 234–41. Springer.
</div>
<div id="ref-santoro2023higher" class="csl-entry">
Santoro, Andrea, Federico Battiston, Giovanni Petri, and Enrico Amico. 2023. <span>“Higher-Order Organization of Multivariate Time Series.”</span> <em>Nature Physics</em>, 1–9.
</div>
<div id="ref-sardellitti2022topological" class="csl-entry">
Sardellitti, Stefania, and Sergio Barbarossa. 2022. <span>“Topological Signal Representation and Processing over Cell Complexes.”</span> <em>arXiv Preprint arXiv:2201.08993</em>.
</div>
<div id="ref-sardellitti2021topological" class="csl-entry">
Sardellitti, Stefania, Sergio Barbarossa, and Lucia Testa. 2021. <span>“Topological Signal Processing over Cell Complexes.”</span> <em>Proceeding IEEE Asilomar Conference. Signals, Systems and Computers</em>.
</div>
<div id="ref-schaub2018denoising" class="csl-entry">
Schaub, Michael T., and Santiago Segarra. 2018. <span>“Flow Smoothing and Denoising: Graph Signal Processing in the Edge-Space.”</span> In <em>2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</em>, 735–39.
</div>
<div id="ref-schaub2021signal" class="csl-entry">
Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. <span>“Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.”</span> <em>Signal Processing</em> 187: 108149.
</div>
<div id="ref-shi2018hypergraph" class="csl-entry">
Shi, Heyuan, Yubo Zhang, Zizhao Zhang, Nan Ma, Xibin Zhao, Yue Gao, and Jiaguang Sun. 2018. <span>“Hypergraph-Induced Convolutional Networks for Visual Classification.”</span> <em>IEEE Transactions on Neural Networks and Learning Systems</em> 30 (10): 2963–72.
</div>
<div id="ref-singh2007topological" class="csl-entry">
Singh, Gurjeet, Facundo Mémoli, Gunnar E Carlsson, et al. 2007. <span>“Topological Methods for the Analysis of High Dimensional Data Sets and 3d Object Recognition.”</span> <em>PBG@ Eurographics</em> 2: 091–100.
</div>
<div id="ref-tabassum2018social" class="csl-entry">
Tabassum, Shazia, Fabiola SF Pereira, Sofia Fernandes, and João Gama. 2018. <span>“Social Network Analysis: An Overview.”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em> 8 (5): e1256.
</div>
<div id="ref-topaz2015topological" class="csl-entry">
Topaz, Chad M, Lori Ziegelmeier, and Tom Halverson. 2015. <span>“Topological Data Analysis of Biological Aggregation Models.”</span> <em>PloS One</em> 10 (5): e0126383.
</div>
<div id="ref-turaev2016quantum" class="csl-entry">
Turaev, Vladimir G. 2016. <em>Quantum Invariants of Knots and 3-Manifolds</em>. Vol. 18. Walter de Gruyter GmbH &amp; Co KG.
</div>
<div id="ref-umeda2017time" class="csl-entry">
Umeda, Yuhei. 2017. <span>“Time Series Classification via Topological Data Analysis.”</span> <em>Information and Media Technologies</em> 12: 228–39.
</div>
<div id="ref-velickovic2017graph" class="csl-entry">
Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. <span>“Graph Attention Networks.”</span> In <em>International Conference on Learning Representations</em>.
</div>
<div id="ref-waibel_capturing_2022" class="csl-entry">
Waibel, Dominik J. E., Scott Atwell, Matthias Meier, Carsten Marr, and Bastian Rieck. 2022. <span>“Capturing <span>Shape</span> <span>Information</span> with <span>Multi</span>-<span>Scale</span> <span>Topological</span> <span>Loss</span> <span>Terms</span> for <span>3D</span> <span>Reconstruction</span>.”</span> In <em>Medical <span>Image</span> <span>Computing</span> and <span>Computer</span> <span>Assisted</span> <span>Intervention</span> – <span>MICCAI</span> 2022</em>, edited by Linwei Wang, Qi Dou, P. Thomas Fletcher, Stefanie Speidel, and Shuo Li, 150–59. Lecture <span>Notes</span> in <span>Computer</span> <span>Science</span>. Cham: Springer Nature Switzerland. <a href="https://doi.org/10.1007/978-3-031-16440-8_15">https://doi.org/10.1007/978-3-031-16440-8_15</a>.
</div>
<div id="ref-wang2023survey" class="csl-entry">
Wang, Cheng, Nan Ma, Zhixuan Wu, Jin Zhang, and Yongqiang Yao. 2023. <span>“Survey of Hypergraph Neural Networks and Its Application to Action Recognition.”</span> In <em>Artificial Intelligence: Second CAAI International Conference, CICAI 2022, Beijing, China, August 27–28, 2022, Revised Selected Papers, Part II</em>, 387–98. Springer.
</div>
<div id="ref-wu2020comprehensive" class="csl-entry">
Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. <span>“A Comprehensive Survey on Graph Neural Networks.”</span> <em>IEEE Transactions on Neural Networks and Learning Systems</em> 32 (1): 4–24.
</div>
<div id="ref-yan2018spatial" class="csl-entry">
Yan, Sijie, Yuanjun Xiong, and Dahua Lin. 2018. <span>“Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition.”</span> In <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>.
</div>
<div id="ref-yang2023convolutional" class="csl-entry">
Yang, Maosheng, and Elvin Isufi. 2023. <span>“Convolutional Learning on Simplicial Complexes.”</span> <em>arXiv Preprint arXiv:2301.11163</em>.
</div>
<div id="ref-ying2018hierarchical" class="csl-entry">
Ying, Zhitao, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. <span>“Hierarchical Graph Representation Learning with Differentiable Pooling.”</span> <em>Neurips</em> 31.
</div>
<div id="ref-zeng_topological_2021" class="csl-entry">
Zeng, Sebastian, Florian Graf, Christoph Hofer, and Roland Kwitt. 2021. <span>“Topological Attention for Time Series Forecasting.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, 34:24871–82. Curran Associates, Inc.
</div>
<div id="ref-zhang2018kernel" class="csl-entry">
Zhang, Qi, Qizhao Jin, Jianlong Chang, Shiming Xiang, and Chunhong Pan. 2018. <span>“Kernel-Weighted Graph Convolutional Network: A Deep Learning Approach for Traffic Forecasting.”</span> In <em>2018 24th International Conference on Pattern Recognition (ICPR)</em>, 1018–23. IEEE.
</div>
<div id="ref-zhang2020multiplex" class="csl-entry">
Zhang, Weifeng, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. <span>“Multiplex Graph Neural Networks for Multi-Behavior Recommendation.”</span> In <em>Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>, 2313–16.
</div>
<div id="ref-zhang2021hierarchical" class="csl-entry">
Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Zhao Li, Chengwei Yao, Huifen Dai, Zhi Yu, and Can Wang. 2021. <span>“Hierarchical Multi-View Graph Pooling with Structure Learning.”</span> <em>IEEE Transactions on Knowledge and Data Engineering</em> 35 (1): 545–59.
</div>
<div id="ref-zhang2019hierarchical" class="csl-entry">
Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. 2019. <span>“Hierarchical Graph Pooling with Structure Learning.”</span> <em>arXiv Preprint arXiv:1911.05954</em>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="implementation-and-numerical-results.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pyt-team/tdlbook/edit/main/rmd/10-related-work.rmd",
"text": "编辑"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
