[["index.html", "拓扑深度学习：超越图数据 贡献者", " 拓扑深度学习：超越图数据 Mustafa Hajij, Theodore Papamarkou, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Michael T. Schaub 2024-09-03 贡献者 除了共同作者之外，以下贡献者对本书的撰写做出了很多贡献: Nina Miolane, Aldo Guzmán-Sáenz, Tamal K. Dey, Soham Mukherjee, Shreyas N. Samaga, Neal Livesay, Robin Walters, Paul Rosen. "],["译者.html", "译者", " 译者 秦广军 北京联合大学 智慧城市学院 2024年08月06日开始，xxxx年xx月xx日结束 北京 几何深度学习，这真的是一个很酷的想法，作为其一个分支，拓扑深度学习更是直接将代数拓扑应用到了神经网络中。读过作者的相关文章，本书是作者写的第一本关于拓扑深度学习的书，于是决定把本书翻译了。说它是书，其实更像个小册子，或者蓝皮书，是学习几何深度学习和拓扑深度学习的重要入门材料。 对于深度学习，我不是太了解的那么深入，只是在科研上逐渐开始使用，对于代数拓扑这种十分抽象的专业知识更是初次接触，不确定相关术语和表述翻译是否准确，请参考原文阅读（TDLBook）。 图 0.1: 来自这个网站 问题1. 生成html文件正常，但是生成epub文件里的公式不能正常显示 问题2. 如果生成的HTML中公式显示不正常，重启电脑就好了，不指导为什么 “dsfdsfdsfsfd” 除了本文，下面再给出一些有用的链接： TDL4CV，https://tdl4cv.github.io/ awesome-topological-deep-learning Public， https://github.com/lrnzgiusti/awesome-topological-deep-learning ICML Topological Deep Learning Challenge 2024: Beyond the Graph Domain，https://pyt-team.github.io/packs/challenge.html "],["序言.html", "序言 编译 致谢", " 序言 拓扑深度学习是一个快速发展的领域，用于为拓扑域(如单纯复形、胞腔复形和超图等多种科学计算中常用的数据域)上的数据研发深度学习模型。在本文中，我们提出了一个统一的深度学习框架，该框架涵盖丰富的数据结构，其中包括广泛采用的拓扑域。 具体来说，我们首先介绍组合复形(combinatorial complexes)–一种新型拓扑域。组合复形可以被看作保持某些理想性质的图的一般化，与超图类似，组合复形对关系集不施加任何约束。此外，组合复形可以用于构建分层的高阶关系，类似于单纯复形和胞腔复形中的关系。因此，组合复形涵盖并结合了超图和胞腔复形的有用特征（他们是两种有前途的抽象概念），这有助于将图神经网络推广到拓扑空间。 其次，在组合复形及其丰富的组合和代数结构基础上，我们构造了一类通用的消息传递组合复形神经网络（combinatorial complex neural networks,CCNNs），主要侧重于基于注意力的 CCNNs。我们描述了 CCNNs 的置换和方向等变关系，并详细讨论了 CCNNs 中的池化和反池化操作。 最后，我们评估了 CCNN 在网格形状分析和图形学习任务中的表现。实验表明，与专为相同任务定制的最先进的深度学习模型相比，CCNN 的性能更具竞争力。 编译 本书用knitr包(Xie 2015)和bookdown包(Xie 2024)编辑，书中源代码见TDLBook的github仓库 致谢 M. H. 感谢美国国家科学基金会 DMS-2134231 奖项的支持。 G. Z.目前隶属于美国国立卫生研究院（NIH），但本研究的核心内容是在南佛罗里达大学（USF）工作期间完成的。本文仅代表作者个人观点，不代表美国国立卫生研究院或美国政府的观点。 N. M. 感谢美国国家科学基金会（National Science Foundation）颁发的 DMS-2134241 奖项。 T. B. 感谢工程与物理科学研究委员会 [grant EP/X011364/1] 的支持。 T. K. D. 感谢美国国家科学基金会 CCF 2049010 奖项的资助。 N. L. 感谢 Roux 研究所和 Harold Alfond 基金会的支持。 R. W. 感谢美国国家科学基金会 DMS-2134178 奖项的资助。 P. R. 感谢美国国家科学基金会 IIS-2316496 奖项的资助。 M. T. S. 感谢德国北莱茵-威斯特法伦州文化与科学部（MKW）（NRW Rückkehrprogramm）和欧盟（ERC, HIGH-HOPeS, 101039827）的资助。然而，所表达的观点和意见仅代表 M. T. S. 的观点和意见，并不一定反映欧盟或欧洲研究理事会执行局的观点和意见；欧盟或拨款机构均不对这些观点和意见负责。 作者感谢玛蒂尔德-帕皮隆和索菲亚-桑伯恩帮助改进图4.1，并就张量图的发展进行了深入讨论。 本书作者 参考文献 Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2024. Bookdown: Authoring Books and Technical Documents with R Markdown. https://github.com/rstudio/bookdown. "],["引言.html", "第 1 章 引言", " 第 1 章 引言 最近几年，可用于计算分析的数据量呈指数级增长，包括科学数据以及常见的数据类型，如文本、图像和音频。丰富的数据使物理、化学、计算社会科学和生物学等各个领域都能利用机器学习技术（主要是深度神经网络）取得重大进展，由于深度神经网络可以有效地从大型数据集中总结和提取模式，因此适用于许多复杂的任务。深度神经网络的开发是为了从规则（欧式）域支持的数据中学习，如图像中的网格、文本序列和时间序列，此类模型包括卷积神经网络（CNNs）(LeCun et al. 1998; Krizhevsky, Sutskever, and Hinton 2012; Simonyan and Zisserman 2014)、递归神经网络（RNNs）(Bahdanau, Cho, and Bengio 2014; Sutskever, Vinyals, and Le 2014)、注意力Transformers(Vaswani et al. 2017)等，这些模型已被证明在处理欧式域上的数据时非常有效(Goodfellow et al. 2016)，在各类应用中表现出了前所未有的性能，最近的应用是聊天机器人，例如ChatGPT ((Adesso 2023)，以及和文本控制图像合成 (Rombach et al. 2022) 。 然而，各领域的科学数据往往结构不同，不被常规的欧式域支持。因此，用深度神经网络处理这类数据一直是个挑战。在此背景下，几何深度学习（geometric deep learning，GDL）(Zonghan Wu et al. 2020; Zhou et al. 2020; Bronstein et al. 2021)作为深度学习模型向非欧式域的扩展而出现。 为了做到这种扩展，GDL 将计算限制在对称性、不变性、等变性等几何规则上。当然，在处理任意数据域时，包括集合(Qi et al. 2017; Rempe et al. 2020; H. Deng, Birdal, and Ilic 2018; Y. Zhao et al. 2022; Jiahui Huang et al. 2022)、网格 (Boscaini et al. 2015, 2016; Masci et al. 2015; Kokkinos et al. 2012; Shuman, Ricaud, and Vandergheynst 2016; Zhirong Wu et al. 2015; Monti et al. 2017)、 流形(Boscaini et al. 2015, 2016; Masci et al. 2015; Kokkinos et al. 2012; Shuman, Ricaud, and Vandergheynst 2016; Zhirong Wu et al. 2015; Monti et al. 2017)、图 (Scarselli et al. 2008; Gallicchio and Micheli 2010; Zhou et al. 2020; Zonghan Wu et al. 2020; Boscaini et al. 2016; Monti et al. 2017; Bronstein et al. 2017; Kipf and Welling 2016)，也允许施加适当的归纳偏差。尤其是图，由于其在众多科学研究中的适用性及其对传统网格的泛化能力，引起了人们的广泛兴趣。因此，图神经网络（GNNs） (Bronstein et al. 2017; Kipf and Welling 2016)的发展极大地增强了人们对几类自然存在的图类数据进行建模和分析的能力。 尽管 GDL 和 GNN 取得了成功，但从纯粹的几何视角来看图形，只能产生局部抽象，无法捕捉数据中的非局部属性和依赖关系。拓扑数据（Topological data）包括边（在图中）、三角形（在网格中）或小团（cliques）等的相互作用，在复杂物理系统的一系列新应用中自然存在 (Battiston et al. 2021; Lambiotte, Rosvall, and Scholtes 2019)，诸如交通流预测(W. Jiang and Luo 2022)、社会影响力 (Zhu et al. 2018)、蛋白质相互作用 (Murgas, Saucan, and Sandhu 2022)、分子涉及 (Schiff et al. 2020)、视觉增强 (Efthymiou et al. 2021)、推荐系统 (La Gatta et al. 2022)，以及流行病学 (S. Deng et al. 2020)等。 为了对这些数据进行原生而有效的建模，我们必须超越图形，并且考虑在某些几何变换下保持不变的量化空间属性。换句话说，我们需要考虑数据的拓扑结构 (G. Carlsson 2009) 来制定能够从复杂数据中提取语义信息的神经网络架构。 从数据中抽取更多全局信息的方法是超越基于图的抽象，去考虑图的扩展，例如单纯复形（simplicial complexes）、胞腔复形（ cell complexes）、超图（hypergraphs），甚至推广到科学计算中会遇到的更多的数据域(Bick et al. 2021; Battiston et al. 2020; Benson, Gleich, and Higham 2021; Torres et al. 2021)。继续发展机器学习模型，以便从这些拓扑域的支持的数据中学习，这是正在快速发展的新领域，我们在下文中将其称为拓扑深度学习（topological deep learning，TDL）。TDL 将多个研究领域交织在一起，包括拓扑数据分析（topological data analysis，TDA)(Edelsbrunner and Harer 2010; G. Carlsson 2009; Dey and Wang 2022a; Love et al. 2023a; Ghrist 2014)、拓扑信号处理（topological signal processing） (Schaub and Segarra 2018; Yang et al. 2021; Schaub et al. 2022; T. Mitchell Roddenberry, Schaub, and Hajij 2022; Barbarossa and Sardellitti 2020a; Robinson 2014; Sardellitti and Barbarossa 2022)、网络科学（network science） (Skardal et al. 2021; Lambiotte, Rosvall, and Scholtes 2019; Barabási 2013; Battiston et al. 2020; Bick et al. 2021; Bianconi 2021; Benson, Gleich, and Leskovec 2016; De Domenico et al. 2016; Bao et al. 2022; Oballe et al. 2021)、几何深度学习（and geometric deep learning）(S.-X. Zhang et al. 2020; Cao et al. 2020; Fey and Lenssen 2019; Loukas 2019; P. W. Battaglia et al. 2018; Morris et al. 2019; P. Battaglia et al. 2016)。 尽管人们对 TDL 的兴趣与日俱增，但迄今为止还没有建立对这些思想的基本原理的广泛综合。我们认为，这是阻碍 TDL 取得进展的一个缺陷，因为它使得在不同概念之间建立联系变得具有挑战性，阻碍了比较，并使其他领域的研究人员难以找到 进入TDL领域 的切入点。因此，在本文中，我们旨在对 TDL 的基本原理进行基础性概述，不仅为近年来文献中出现的许多令人兴奋的观点提供一个统一的框架，而且作为一个概念起点，促进对新观点的探索。最终，我们希望这项工作能促进 TDL 的加速发展，我们相信这将是把深度学习的成功经验应用到更多应用场景的关键因素。 通过从代数拓扑学的传统拓扑概念(Ghrist 2014; Hatcher 2005)和高阶网络的最新进展 (Battiston et al. 2020, 2021; Torres et al. 2021; Bick et al. 2021)中汲取灵感，我们首先引入组合复形(combinatorial complexes ，CCs)作为我们TDL框架的主要构建模块。 CCs构建了一个新的拓扑域，将图、单纯复形、胞腔复形、超图等作为特例统一了起来，如图1.1所示1。与超图类似，CCs可以编码抽象实体集间的类集合关系。此外，CCs 还可以构建层次化的高阶关系，类似于单纯复形和胞腔复形中的关系。因此，CCs 概括并结合了超图和胞腔复形的理想特性。 图 1.1: 该图解直观展示了我们的主要贡献。(a): 不同的数学结构可用来表示抽象实体之间的不同关系。集合由无连接的实体组成，图编码了顶点间的二元关系，单纯复形和胞腔复形模型化了分层的高阶关系，超图则表示了无层次的任意集合型关系，我们采用组合复形（CCs）来覆盖图、单纯复形、胞腔复形和超图，CCs 不仅具有集合型关系，也能表示关系间的层次结构。(b): 通过利用 CCs 的层次和拓扑结构，我们引入了前推操作（push-forward），它是高阶消息传递协议和 CCs上非池化/池化操作的基本构件。前推操作可用来够构建组合复形神经网络（combinatorial complex neural networks，CCNN），为高阶域上的拓扑深度学习提供通用的概念框架。 此外，此外，我们还引入了构建深度神经网络所需的算子，用于学习锚定在 CCs上的输入特征和抽象摘要。这些算子提供了卷积、注意机制、消息传递，以及包含不变性、等变性或其他几何规律的方法。具体来说，前推操作（push-forward operation）允许在不同维度之间推送数据，从而形成了一个基本构件，用于在CCs上定义高阶消息传递协议（higher-order message-passing protocols）、非池化/池化（(un)pooling operations）等操作。由此产生的学习模型，我们称之为组合复杂神经网络（combinatorial complex neural networks，CCNNs），它能够学习抽象的高阶数据结构，这在我们的实验评估中得到了清晰的验证。 我们希望我们的贡献能成为一个平台，鼓励研究人员和从业人员扩展我们的 CCNNs，并邀请社区在我们工作的基础上扩展高阶领域的 TDL。图1.1直观地概括了我们的贡献，具体如下： 首先，我们为TDL引入了CCs域，我们描述了 CCs 的特征及其属性，并解释了它们如何用来推广到现有的数据域，如图、超图、单纯复形和胞腔复形。因此，CCs 可以作为一个统一的起点，帮助学习拓扑数据的表达式表示。 其次，使用CCs作为数据域，我们构造了CCNNs，一个高阶消息传递神经网络的抽象类，可为基于超图、胞腔复形的TDL模型提供统一的蓝图。 基于CCs上的push-forward运算，我们为CCNNs引入了卷积、注意力、非池化和池化操作。 我们形式化并研究了 CCNNs 的变换(permutation)和方向等变(orientation equivariance)，为今后 CCNNs 的几何化工作铺平了道路。 我们展示了如何通过图形符号直观地构建 CCNNs。 最后，我们在实践场景中评估了我们的想法 我们以python库的形式发布了我们框架的源代码：TopoNetX, TopoEmbedX，TopoModelX 我们的研究表明，在形状分析和图学习等各种应用中，CCNNs 的预测性能可与最先进的特定任务神经网络相媲美。 我们将我们的工作与 TDA 中的经典构造建立了联系，如 mapper (Singh et al. 2007)。尤其，我们用 TDL 框架实现了mapper，并演示了如何将其用于 CCs 的高阶非池化和池化操作。 我们也展示了任何 CC 都可以还原为一种叫做Hasse 图(Hasse graph)的特殊图。这使得我们能够用基于图的模型来描述 CCNNs 的某些方面，从而将高阶表示学习归约为图表示学习（使用放大的计算图）。 术语 在深入探讨更多细节之前，我们先介绍一下本文中使用的既定概念的基本术语。其中一些术语将在第3章中重新正式讨论。附录A为本文提出的新观点提供了符号和术语表。 胞腔复形(Cell complex)：一类拓扑空间，是拓扑圆盘（胞腔）的不相交并，其中每个胞腔都与欧式球的内部同构，这些胞腔通过附加映射以局部合理的方式连接在一起。 域(Domain)：通常指支持数据的底层空间。 实体（Entity）或顶点（vertex）：一个抽象的点，也可以认为是集合的元素 图（Grapg）或网络：一组由边集联系起来的实体或顶点集合，其中，边集表示了顶点间的二元关系。 拓扑域上的层次化结构(Hierarchical structure)：通过一个整数值函数为域中的每个关系都分配一个正整数（秩，rank），使得高阶关系可以被分配更高的秩值。例如，一个单纯复形具有由其单纯形的基数（cardinality）推导出的层次化结构。 高阶网络/拓扑域（Higher-order network/topological domain）：图的推广，反应实体间的二元或高阶关系，单纯复形、胞腔复形、超图式此类高阶网络的示例。 超图（Hypergraph）：一组由超边集联系的实体（顶点）集合，超边表示顶点间的二元或高阶关系。 消息传递（Message passing）：一种在域上领域实体间传递数据、`消息’的计算框架，根据从领域收到的消息来更新每个实体的表示 关系（Relation）或胞腔（cell）：实体（顶点）集的子集，如果关系的基数等于2，则说它是二元关系；如果基数大于2，则说它是高阶关系。 集合型关系（Set-type relation）：如果高阶网络中的某一关系存在且不蕴含于网络中的另一关系，则称该网络具有集合类型关系。例如，超图就包含集合类型关系。 单纯形（Simplex）：三角形或四面体在任意维度上的推广，例如，维度为0、1、2、3的单纯形分别是点、线段、三角形、四面体。 单纯复形 Simplicial complex:单纯形的集合，集合中每个单纯形的每个面也都在集合中，集合中任意两个单纯形的交集要么是空的，要么是两个单纯形的一个面。 拓扑数据（Topological data）：支持拓扑域中关系的特征向量 拓扑深度学习（Topological deep learning）：用深度学习技术研究拓扑域，用拓扑域在深度学习中表示数据 拓扑神经网络（Topological neural network）：在拓扑域上处理数据的深度学习模型 参考文献 Adesso, Gerardo. 2023. “Towards the Ultimate Brain: Exploring Scientific Discovery with ChatGPT AI.” AI Magazine 44 (3): 328–42. https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12113. Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv Preprint arXiv:1409.0473. https://arxiv.org/abs/1409.0473. Bao, Xiaoge, Qitong Hu, Peng Ji, Wei Lin, Jürgen Kurths, and Jan Nagler. 2022. “Impact of Basic Network Motifs on the Collective Response to Perturbations.” Nature Communications 13 (1): 5301. Barabási, Albert-László. 2013. “Network Science.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 371 (1987): 20120375. Barbarossa, Sergio, and Stefania Sardellitti. 2020a. “Topological Signal Processing over Simplicial Complexes.” IEEE Transactions on Signal Processing 68: 2992–3007. Battaglia, Peter W., Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, et al. 2018. “Relational Inductive Biases, Deep Learning, and Graph Networks.” arXiv Preprint arXiv:1806.01261. Battaglia, Peter, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu. 2016. “Interaction Networks for Learning about Objects, Relations and Physics.” In Proceedings of the 30th International Conference on Neural Information Processing Systems, 4509–17. NIPS’16. Red Hook, NY, USA: Curran Associates Inc. Battiston, Federico, Enrico Amico, Alain Barrat, Ginestra Bianconi, Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, et al. 2021. “The Physics of Higher-Order Interactions in Complex Systems.” Nature Physics 17 (10): 1093–98. Battiston, Federico, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. 2020. “Networks Beyond Pairwise Interactions: Structure and Dynamics.” Physics Reports 874: 1–92. Benson, Austin R., David F. Gleich, and Desmond J. Higham. 2021. “Higher-Order Network Analysis Takes Off, Fueled by Classical Ideas and New Data.” arXiv Preprint arXiv:2103.05031. Benson, Austin R., David F. Gleich, and Jure Leskovec. 2016. “Higher-Order Organization of Complex Networks.” Science 353 (6295): 163–66. Bianconi, Ginestra. 2021. Higher-Order Networks. Cambridge University Press. Bick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T Schaub. 2021. “What Are Higher-Order Networks?” arXiv Preprint arXiv:2104.11329. Boscaini, Davide, Jonathan Masci, Simone Melzi, Michael M Bronstein, Umberto Castellani, and Pierre Vandergheynst. 2015. “Learning Class-Specific Descriptors for Deformable Shapes Using Localized Spectral Convolutional Networks.” Computer Graphics Forum 34 (5): 13–23. Boscaini, Davide, Jonathan Masci, Emanuele Rodolà, and Michael Bronstein. 2016. “Learning Shape Correspondence with Anisotropic Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, 3189–97. Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” arXiv Preprint arXiv:2104.13478. Bronstein, Michael M., Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. “Geometric Deep Learning: Going Beyond Euclidean Data.” IEEE Signal Processing Magazine 34 (4): 18–42. Cao, Wenming, Zhiyue Yan, Zhiquan He, and Zhihai He. 2020. “A Comprehensive Survey on Geometric Deep Learning.” IEEE Access 8: 35929–49. Carlsson, Gunnar. 2009. “Topology and Data.” Bulletin of the American Mathematical Society 46 (2): 255–308. De Domenico, Manlio, Clara Granell, Mason A Porter, and Alex Arenas. 2016. “The Physics of Spreading Processes in Multilayer Networks.” Nature Physics 12 (10): 901–6. Deng, Haowen, Tolga Birdal, and Slobodan Ilic. 2018. “PPFNet: Global Context Aware Local Features for Robust 3D Point Matching.” In Cvpr, 195–205. Deng, Songgaojun, Shusen Wang, Huzefa Rangwala, Lijing Wang, and Yue Ning. 2020. “Cola-GNN: Cross-Location Attention Based Graph Neural Networks for Long-Term ILI Prediction.” In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, 245–54. Dey, Tamal K., and Yusu Wang. 2022a. Computational Topology for Data Analysis. Cambridge University Press. Edelsbrunner, Herbert, and John Harer. 2010. Computational Topology: An Introduction. American Mathematical Soc. Efthymiou, Athanasios, Stevan Rudinac, Monika Kackovic, Marcel Worring, and Nachoem Wijnberg. 2021. “Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings.” arXiv Preprint arXiv:2105.08190. Fey, Matthias, and Jan Eric Lenssen. 2019. “Fast Graph Representation Learning with PyTorch Geometric.” arXiv Preprint arXiv:1903.02428. Gallicchio, Claudio, and Alessio Micheli. 2010. “Graph Echo State Networks.” In The 2010 International Joint Conference on Neural Networks (IJCNN), 1–8. IEEE. Ghrist, Robert W. 2014. Elementary Applied Topology. Vol. 1. Createspace Seattle. Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep Learning. Vol. 1. MIT Press Cambridge. https://mitpress.mit.edu/9780262035613/deep-learning/. Hatcher, Allen. 2005. Algebraic Topology. Cambridge University Press. Huang, Jiahui, Tolga Birdal, Zan Gojcic, Leonidas J Guibas, and Shi-Min Hu. 2022. “Multiway Non-Rigid Point Cloud Registration via Learned Functional Map Synchronization.” IEEE Transactions on Pattern Analysis and Machine Intelligence. Jiang, Weiwei, and Jiayun Luo. 2022. “Graph Neural Network for Traffic Forecasting: A Survey.” Expert Systems with Applications, 117921. Kipf, Thomas N., and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907. Kokkinos, Iasonas, Michael M Bronstein, Roee Litman, and Alex M Bronstein. 2012. “Intrinsic Shape Context Descriptors for Deformable Shapes.” In Cvpr, 159–66. IEEE. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems. https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html. La Gatta, Valerio, Vincenzo Moscato, Mirko Pennone, Marco Postiglione, and Giancarlo Sperlı́. 2022. “Music Recommendation via Hypergraph Embedding.” IEEE Transactions on Neural Networks and Learning Systems. Lambiotte, Renaud, Martin Rosvall, and Ingo Scholtes. 2019. “From Networks to Optimal Higher-Order Models of Complex Systems.” Nature Physics 15 (4): 313–20. LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://ieeexplore.ieee.org/document/726791. Loukas, Andreas. 2019. “What Graph Neural Networks Cannot Learn: Depth Vs Width.” arXiv Preprint arXiv:1907.03199. Love, Ephy R., Benjamin Filippenko, Vasileios Maroulas, and Gunnar Carlsson. 2023a. “Topological Convolutional Layers for Deep Learning.” Jmlr 24 (59): 1–35. Masci, Jonathan, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. 2015. “Geodesic Convolutional Neural Networks on Riemannian Manifolds.” In Conference on Computer Vision and Pattern Recognition. Monti, Federico, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M. Bronstein. 2017. “Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.” In Cvpr, 5115–24. Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. Murgas, Kevin A., Emil Saucan, and Romeil Sandhu. 2022. “Hypergraph Geometry Reflects Higher-Order Dynamics in Protein Interaction Networks.” Scientific Reports 12 (1): 20879. Oballe, Christopher, Alan Cherne, Dave Boothe, Scott Kerick, Piotr J Franaszczuk, and Vasileios Maroulas. 2021. “Bayesian Topological Signal Processing.” Discrete &amp; Continuous Dynamical Systems-S. Qi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” In Cvpr, 652–60. Rempe, Davis, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar, and Leonidas J Guibas. 2020. “CASPR: Learning Canonical Spatiotemporal Point Cloud Representations.” Neurips 33: 13688–701. Robinson, Michael. 2014. Topological Signal Processing. Vol. 81. Springer. Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. “Signal Processing on Cell Complexes.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. “High-Resolution Image Synthesis with Latent Diffusion Models.” In Computer Vision and Pattern Recognition. https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600k0674/1H1iFsO7Zuw. Sardellitti, Stefania, and Sergio Barbarossa. 2022. “Topological Signal Representation and Processing over Cell Complexes.” arXiv Preprint arXiv:2201.08993. Scarselli, Franco, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 20 (1): 61–80. Schaub, Michael T., Jean-Baptiste Seby, Florian Frantzen, T. Mitchell Roddenberry, Yu Zhu, and Santiago Segarra. 2022. “Signal Processing on Simplicial Complexes.” In Higher-Order Systems, 301–28. Springer. Schaub, Michael T., and Santiago Segarra. 2018. “Flow Smoothing and Denoising: Graph Signal Processing in the Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39. Schiff, Yair, Vijil Chenthamarakshan, Karthikeyan Natesan Ramamurthy, and Payel Das. 2020. “Characterizing the Latent Space of Molecular Deep Generative Models with Persistent Homology Metrics.” arXiv Preprint arXiv:2010.08548. Shuman, David I., Benjamin Ricaud, and Pierre Vandergheynst. 2016. “Vertex-Frequency Analysis on Graphs.” Applied and Computational Harmonic Analysis 40 (2): 260–91. Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv Preprint arXiv:1409.1556. https://arxiv.org/abs/1409.1556. Singh, Gurjeet, Facundo Mémoli, Gunnar E Carlsson, et al. 2007. “Topological Methods for the Analysis of High Dimensional Data Sets and 3d Object Recognition.” PBG@ Eurographics 2: 091–100. Skardal, Per Sebastian, Lluı́s Arola-Fernández, Dane Taylor, and Alex Arenas. 2021. “Higher-Order Interactions Improve Optimal Collective Dynamics on Networks.” arXiv Preprint arXiv:2108.08190. Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In Advances in Neural Information Processing Systems. https://arxiv.org/abs/1409.3215. Torres, Leo, Ann S Blevins, Danielle Bassett, and Tina Eliassi-Rad. 2021. “The Why, How, and When of Representations for Complex Systems.” SIAM Review 63 (3): 435–85. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Wu, Zhirong, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 2015. “3D ShapeNets: A Deep Representation for Volumetric Shapes.” In Cvpr, 1912–20. Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. “A Comprehensive Survey on Graph Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems 32 (1): 4–24. Yang, Maosheng, Elvin Isufi, Michael T Schaub, and Geert Leus. 2021. “Finite Impulse Response Filters for Simplicial Complexes.” In 2021 29th European Signal Processing Conference (EUSIPCO), 2005–9. IEEE. Zhang, Shi-Xue, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, and Xu-Cheng Yin. 2020. “Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection.” In Cvpr, 9699–9708. Zhao, Yongheng, Guangchi Fang, Yulan Guo, Leonidas Guibas, Federico Tombari, and Tolga Birdal. 2022. “3DPointCaps++: Learning 3D Representations with Capsule Networks.” Ijcv 130 (9): 2321–36. Zhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. “Graph Neural Networks: A Review of Methods and Applications.” AI Open 1: 57–81. Zhu, Jianming, Junlei Zhu, Smita Ghosh, Weili Wu, and Jing Yuan. 2018. “Social Influence Maximization in Hypergraph in Social Networks.” IEEE Transactions on Network Science and Engineering 6 (4): 801–11. 本文中的所有图表都应使用彩色，因为不同的颜色传达不同的信息。↩︎ "],["motivation.html", "第 2 章 研究动机 2.1 从拓扑空间数据中建模和学习 2.2 拓扑的有用性 2.3 深度学习和结构化计算的统一视角", " 第 2 章 研究动机 本文介绍的组合复形神经网络（CCNNs）通过拓扑结构推广了图神经网络（GNNs）及其高阶类似物。在本节中，我们将从三个角度介绍在机器学习，特别是深度学习中使用拓扑结构的动机。第一，数据建模( data modeling)：拓扑抽象如何帮助我们推理和计算拓扑空间上支持的各种数据类型；第二，实用性(utility)：怎了利用拓扑结构来提高性能。第三，统一性（unification）：拓扑如何用于综合和抽象不同的概念。 2.1 从拓扑空间数据中建模和学习 在机器学习上下文中，域上支持的典型数据类型是线性或者说向量空间。这种底层向量空间为许多计算提供了便利，并且经常被隐含地假定。然而，正如几何深度学习所认识到的那样，考虑不同领域支持的数据往往至关重要。事实上，出于各种原因，明确考虑域所支持的数据并对其进行建模至关重要。 首先，不同的域可能具有不同的特征和属性，这就需要不同类型的深度学习模型来有效处理和分析数据。例如，图域可能需要图卷积网络 (Kipf and Welling 2016) 来考虑非欧结构，而点云域则可能需要类似 PointNet 的架构 (Qi et al. 2017; Zaheer et al. 2017)来处理无序的点集。 其次， 每个域中的具体数据在大小、复杂性和噪声方面都会有很大差异。通过了解特定域内的特定数据属性，研究人员可以开发出针对这些特定属性的模型。例如，针对高噪声点云设计的模型可能会采用离群点去除或局部特征聚合等技术。 第三，准确区分领域和数据对于开发能够很好地概括新的、未见过的数据的模型非常重要。通过识别域的底层结构，研究人员可以开发出既能适应数据变化，又能捕捉相关几何特征的模型。总之，通过同时考虑域和数据，研究人员可以开发出更适合处理不同几何学习任务的特定挑战和复杂性的模型。 数据的不同视角：域（domains）和关系（relations）。文献中提到拓扑数据或拓扑数据分析时，对于实际数据包括什么以及数据应该建模什么存在不同观点。在此，我们遵循(Bick et al. 2021)的观点，区分不同类型的数据和我们学习过程的目标，尽管这些不同类型数据之间的区别有时可能比这里介绍的更加模糊。 关系数据（Relational data），用于描述不同实体或对象间的关系，这一基本概念有多种表现形式，例如社交网络中用户之间的联系，朋友关系或追随者关系，就是这种关系的例子。传统上，人们通过图来理解这些关系，图中的顶点和边分别代表实体和实体之间的成对连接（pairwise connections）。换句话说，图中的边被视为度量单位，每条关系数据都涉及两个顶点。 然而，现实世界中的许多现象自然涉及复杂的多方互动，涉及不止两个实体以错综复杂的方式相互影响。例如，社会中的个人群体、合著者集合、相互影响的基因或蛋白质都是这种高阶关系的例子。这种依赖关系超越了成对的相互作用，可以用高阶关系（higher-order relations）来建模，并恰当地抽象为超图、胞腔复形或 CCs。显然，拓扑学思想可以在理解这些数据方面发挥重要作用，关键是能让我们从模拟成对关系的图转向更丰富的表征。 数据作为域的实例，与TDA 采用的相关概念类似，即我们的观测数据应该代表什么。也就是说，所有观测到的数据都应该对应于拓扑对象本身的噪声实例。例如，我们的目标是学习数据的 “拓扑形状”。请注意，在这种情况下，我们通常不会直接观察相关数据，而是从观察到的数据中构建关系，然后利用这些关系对观察到的数据集进行分类，对点云数据进行的持久同源性分析就是这种观点的一个主流例子。 拓扑域上支持的数据类型，当在拓扑域（例如图）上考察数据时，拓扑化思想起着重要的作用。它可能会是某种特定的动态变化，例如流行病的传播，也可能是顶点上支持的任何类型的其他数据或信号(图上的边缘信号（edage-signals）的情况不经常被考虑，尽管存在例外情况(Schaub and Segarra 2018; Schaub et al. 2021))。在图信号处理（graph signal processing）的语境下，在图域上可以定义多种函数或信号，并且称他们是被该域支持。重要的是，图本身可以任意复杂，典型情况下看作是固定的，观察到的数据不是关系的，而是图的顶点上支持所支持的。 在超越图结构方面更一般的拓扑结构，例如CCs，可支持的数据不仅在顶点和边上，也在其他的高阶实体上，正如图2.1(a-b)上所展示的。例如，计算机图形学中定义在网格上的向量场（vector fields）就是边和面上通常支持的数据形式，可以很方便地建模为高阶域上支持的数据形式 (Goes, Desbrun, and Tong 2016)。类似的，类别标签化的数据也可以在给定网格的边和面上提供支持，看图 2.1(c)中的例子。为了处理这样的数据，就需要再去考虑底层拓扑域上的结构。 图 2.1: 数据可以在高阶关系上很自然的得到支持. (a): 基于边的向量场（edge-based vector field）. (b): 基于面的向量场（face-based vector field）.（a）和（b）上的向量场都定义在胞腔复形torus结构上2。（c）：类别标签化的拓扑数据可以很自然的在高阶关系上被支持。例如，二维面（2-Face）上的网格分割标签（mesh segmentation labels）可以用不同的颜色来描述（蓝色，绿色，绿松石色，粉色，棕色），以表示马的不同部分（头部，颈部，身体，腿部，尾部） 建模和处理图之外的数据：示例。图非常适合建模表现出成对相互作用的系统，例如，如图2.2(a)所示，基于粒子的流体模拟可以用图有效地表示，消息传递用于更新流体分子的物理状态。采用这种方法，每个分子都表示为包含流体分子物理状态的顶点，边则将他们连接起来，以进行交互作用的计算(Sanchez-Gonzalez et al. 2020; Shlomi, Battaglia, and Vlimant 2020)。 图 2.2: 在图或高阶网络上处理数据的例子。 (a): 图可用于建模流体动力学中的分子相互作用，其中，顶点表示粒子， 粒子与粒子之间的相互作用通过顶点之间的信息传递来模拟 (Sanchez-Gonzalez et al. 2020; Shlomi, Battaglia, and Vlimant 2020). (b): 在对弹性（spring）和自碰撞(self-collision)建模时，自然要使用边而不是顶点，这是 这是因为布料的拉伸行为是由沿边缘作用的拉力和压力决定的，而不仅仅是由单个颗粒的位置决定的。 为了模拟多条边之间的相互作用，可以使用多边形面来表示布的局部几何形状,多边形面可提供计算边缘间高阶信息传递的方法 图不适用于布料仿真等更复杂的系统建模，因为状态变量是和边或三角面等关联的，而非顶点，在这种情况下，就需要高阶消息传递来计算和更新物理状态，如图2.2(b)所示。在自然语言处理中也有类似的挑战，语言表现出了多层次的语法、语义和语境。虽然，GNN可以捕获词汇间基本的语法和语义关系，但是否定、讽刺或挖苦等更复杂的关系可能难以表述 (Girault, Narayanan, and Ortega 2017)。将高阶关系和层次关系纳入其中，可以为这些关系提供更好的模型，并能更细致、更准确地理解语言。 2.2 拓扑的有用性 除了为复杂系统建模提供多功能框架外，TDL 模型还具有广泛的用途：它们可以为模型学习推导出有意义的归纳偏差，促进底层域中更大范围内的高效信息传播，增强现有基于图的模型的表达能力，并有可能揭示深度网络本身的关键特征，下文将对此进行介绍。 建立数据的分层表示， 虽然利用高阶信息对学习表征很重要，但保留实体间复杂的高阶关系对实现强大而多变的表征也很关键。例如，人们可以通过分层建立关系之间的关系来形成抽象和类比。然而，常用于在各种实体间建立关系推理的基于图的模型(Adam Santoro et al. 2017; S.-X. Zhang et al. 2020; Yunpeng Chen et al. 2019; Schlichtkrull et al. 2018)在建立实体间高阶关系的能力方面却是是有限的。 分层关系推理的需求(C. Xu et al. 2022)要求所用方法能够捕捉关系之间更深层次的抽象关系，而不仅仅是模拟原始实体之间的关系。高阶网络模型为应对高阶关系推理的挑战提供了一个很有前景的解决方案，如图所示2.3。 图 2.3: 用高阶网络表示分层数据的示例。黑色箭头表示通过高阶关系扩增图，橙色箭头表示粗略的图提取。(a): 表示抽象实体（黄色顶点）间二元关系（粉色边）的图编码。 (b): 蓝色胞腔表示的高阶关系可看作是原始图中顶点或边之间的关系。 (c): 提取原始图的粗略版本。在粗略图中，顶点代表原始图的高阶关系（蓝色胞腔），边代表这些蓝色胞腔的交集。 (d-e): 重复同样的过程，可以得到原始图的更粗略版本。整个过程对应于分层的高阶关系，即关系之间的关系，从而提取意义和内容 (包括数据形状),这是拓扑数据分析的常见任务(G. Carlsson 2009; Dey and Wang 2022a). 通过归纳偏置提高性能， 拓扑深度学习模型通过提供一个定义明确的过程，以将图形提升到指定的高阶网络，从而提高基于图形的学习任务的预测性能。通过这种方法纳入多节顶点关系可被视为一种归纳偏差，可用于提高预测性能。归纳偏置允许学习算法根据观察到的数据之外的因素，优先选择一种解决方案，而不是另一种解决方案(T. M. Mitchell 1980)。图 2.4 展示了使用高阶胞腔对图进行增强的两种形式。 图 2.4: 中间的图可以用高阶关系来增强，以改进学习任务.在(a)中,在缺失的面上添加了胞腔；(Bodnar et al. 2021)考虑了类似的归纳偏置，以改进分子数据的分类。在（b）中，图中的一些顶点添加了单跳邻域；(Feng et al. 2019)，并使用一种基于将图提升到其相应单跳邻域超图的归纳偏置，以提高基于顶点任务的性能 构建基于图的高效远程消息传递系统，TDL 网络的分层结构有助于高效地构建远距离交互。利用这种结构，定义在域顶点上的信号可以在通过长边路径连接的顶点之间有效传播远距离依赖关系。图 2.5说明了通过增加拓扑关系对图进行扩充，并将信号反池化（unpooling）回顶点，从而提升定义在图顶点上的信号的过程。(Hajij, Ramamurthy, et al. 2022) 利用这种池化和反池化操作来构建能够捕捉远距离依赖关系的模型，从而更有效、更自适应地学习底层域上的局部和全局结构。关于基于图的池化的相关工作，我们推荐读者参阅(Su, Hu, and Li 2021; Itoh, Kubo, and Ikeda 2022)。 图 2.5: 二元或高阶胞腔之间的消息传递示意图。(a): 使用基于图的消息传递方案时，一些从顶点\\(x\\) 开始的信息需要经过很长的距离，即长边路径，才能到达顶点 \\(y\\)。(b): 使用蓝色胞腔表示的高阶胞腔结构，消息可以从顶点提升到高阶胞腔，从而以更少的步骤在顶点 \\(x\\) 和 \\(y\\) 之间来回传播。这种快捷方式可以在属于蓝色胞腔的所有顶点之间，以更少的计算步骤高效地共享信息(Hajij, Ramamurthy, et al. 2022)。 提高表达能力，TDL 可以捕捉传统基于图的模型无法捕捉的复杂而微妙的依赖关系。通过允许更丰富的高阶交互，高阶网络可以提高表达能力，并在许多任务中带来卓越的性能(Morris et al. 2019; Bodnar et al. 2021)。 2.3 深度学习和结构化计算的统一视角 拓扑学提供了一个框架来泛化科学计算中通常会遇到的众多离散域。这些领域的例子包括图、单纯复形和胞腔复形；见图2.6。在这些域上支持的计算模型可以被视为各种深度学习架构的泛化；也就是说，可以为比较和设计 TDL 架构提供一个统一的框架。此外，通过统一理论了解不同类型 TDL 之间的联系，可以为复杂系统的底层结构和行为提供有价值的见解，因为在复杂系统中自然会出现高阶关系(Hansen and Ghrist 2019; Hajij, Istvan, and Zamzmi 2020; Bick et al. 2021; Majhi, Perc, and Ghosh 2022)。 图 2.6: 我们的工作引入了组合复形，它是一种高阶网络，可泛化科学计算中通常遇到的大多数离散域。 包括 (a) 序列和图像, (b) 图, (c) 3D形状和单纯复形, (d) 立方体（cubical）和胞腔复形（cellular complexes）, (e) 离散流形（discrete manifolds）, (f) 超图. 了解深度网络的工作原理，当代深度神经网络的一个显著特点是，即使拥有大量参数，它们也能很好地泛化未见数据。这与我们以往对统计学习理论的理解相矛盾，促使研究人员寻求新的方法来理解深度神经网络的底层工作原理(Neyshabur et al. 2019)。最近的研究发现，深度神经网络推导出的图拓扑或其相应的训练轨迹（建模为马尔科夫链）与泛化性能表现出很强的相关性 (Birdal et al. 2021; Dupuis, Deligiannidis, and Şimşekli 2023)。一个悬而未决的问题是，在支持高阶域的学习架构中，拓扑结构、计算函数和泛化特性之间能在多大程度上存在这种关联。我们相信，通过使用我们提出的拓扑结构（如 CCs 和 CCNNs）来研究这些问题，不仅能深入了解 TDL，还能更广泛地了解深度学习。 参考文献 Bick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T Schaub. 2021. “What Are Higher-Order Networks?” arXiv Preprint arXiv:2104.11329. Birdal, Tolga, Aaron Lou, Leonidas J Guibas, and Umut Simsekli. 2021. “Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks.” Advances in Neural Information Processing Systems. Bodnar, Cristian, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. 2021. “Weisfeiler and Lehman Go Cellular: CW Networks.” In Advances in Neural Information Processing Systems. Carlsson, Gunnar. 2009. “Topology and Data.” Bulletin of the American Mathematical Society 46 (2): 255–308. Chen, Yunpeng, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng, Jiashi Feng, and Yannis Kalantidis. 2019. “Graph-Based Global Reasoning Networks.” In Conference on Computer Vision and Pattern Recognition. Dey, Tamal K., and Yusu Wang. 2022a. Computational Topology for Data Analysis. Cambridge University Press. Dupuis, Benjamin, George Deligiannidis, and Umut Şimşekli. 2023. “Generalization Bounds with Data-Dependent Fractal Dimensions.” arXiv Preprint arXiv:2302.02766. Feng, Yifan, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. “Hypergraph Neural Networks.” Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 3558–65. Girault, Benjamin, Shrikanth S. Narayanan, and Antonio Ortega. 2017. “Towards a Definition of Local Stationarity for Graph Signals.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Goes, Fernando de, Mathieu Desbrun, and Yiying Tong. 2016. “Vector Field Processing on Triangle Meshes.” In ACM SIGGRAPH 2016 Courses, 1–49. Association for Computing Machinery. Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Hajij, Mustafa, Karthikeyan Natesan Ramamurthy, Aldo Saenz, and Ghada Zamzmi. 2022. “High Skip Networks: A Higher Order Generalization of Skip Connections.” In ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Hansen, Jakob, and Robert Ghrist. 2019. “Toward a Spectral Theory of Cellular Sheaves.” Journal of Applied and Computational Topology 3 (4): 315–58. Itoh, Takeshi D., Takatomi Kubo, and Kazushi Ikeda. 2022. “Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities.” Neural Networks 145: 356–73. Kipf, Thomas N., and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907. Majhi, Soumen, Matjaž Perc, and Dibakar Ghosh. 2022. “Dynamics on Higher-Order Networks: A Review.” Journal of the Royal Society Interface 19 (188): 20220043. Mitchell, Tom M. 1980. “The Need for Biases in Learning Generalizations.” Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. Neyshabur, Behnam, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. 2019. “The Role of over-Parametrization in Generalization of Neural Networks.” In International Conference on Learning Representations. Qi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” In Cvpr, 652–60. Sanchez-Gonzalez, Alvaro, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. 2020. “Learning to Simulate Complex Physics with Graph Networks.” In International Conference on Machine Learning. Santoro, Adam, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. 2017. “A Simple Neural Network Module for Relational Reasoning.” In Advances in Neural Information Processing Systems. Schaub, Michael T., and Santiago Segarra. 2018. “Flow Smoothing and Denoising: Graph Signal Processing in the Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39. Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. “Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.” Signal Processing 187: 108149. Schlichtkrull, Michael, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. “Modeling Relational Data with Graph Convolutional Networks.” In European Semantic Web Conference. Shlomi, Jonathan, Peter Battaglia, and Jean-Roch Vlimant. 2020. “Graph Neural Networks in Particle Physics.” Machine Learning: Science and Technology 2 (2): 021001. Su, Zidong, Zehui Hu, and Yangding Li. 2021. “Hierarchical Graph Representation Learning with Local Capsule Pooling.” In ACM International Conference on Multimedia in Asia. Xu, Chenxin, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. 2022. “GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning.” In Conference on Computer Vision and Pattern Recognition. Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. “Deep Sets.” In Advances in Neural Information Processing Systems. Zhang, Shi-Xue, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, and Xu-Cheng Yin. 2020. “Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection.” In Cvpr, 9699–9708. 此URL提供（a）和（b）的交互式可视化戳这里.↩︎ "],["preliminaries.html", "第 3 章 预备知识 3.1 邻域函数和拓扑空间 3.2 填补与高阶网络间的代沟 3.3 层次化结构与集合型关系", " 第 3 章 预备知识 在各种机器学习应用中，集合\\(S\\)中实体间的邻近性概念具有重要意义，因为它有助于理解\\(S\\)中实体间的关系。例如，聚类算法的目的是将相互接近的点进行分组。在推荐系统中，目标是推荐与用户已表示感兴趣的项具有相似性的项。然而，问题是我们如何精确量化近似性的概念？ 考虑一个由抽象实体集合组成的集合\\(S\\)，如图3.1(a)所示。考虑同一图中的红色实体（节点）\\(x\\)。我们希望找出\\(S\\)中与\\(x\\)“密切相关”或 “非常接近”的实体（节点）。然而，集合本身并没有实体之间的接近或关系的概念。 图 3.1: 集合\\(S\\)实体间邻近性概念的说明. (a): 抽象实体的有限集合\\(S\\). (b): \\(S\\)中实体\\(x\\)（红色）的邻域（黄色），邻域的定义是\\(S\\)中通过边与\\(x\\)相邻的实体集合。 (c): \\(x\\)的邻域，由\\(S\\)中与红色实体\\(x\\)距离最远为2的所有黄色实体组成. (d): 由\\(S\\)中所有与红色实体\\(x\\)形成三角形（蓝色）的黄色实体组成的\\(x\\)邻域. (e): 由 \\(S\\) 中所有与红色实体 \\(x\\) 形成梯形（蓝色）的黄色实体组成的 \\(x\\) 邻域。 在集合 \\(S\\) 的实体间定义二元关系（边）是引入邻近性概念的一种方法，其结果是一个顶点集为 \\(S\\) 的图，如图所示3.1(b)。 利用这种定义在集合 \\(S\\) 上的边的 “辅助结构”，我们就可以声明 \\(x\\) 的 “局部邻域”（用 \\(\\mathcal{N}(x)\\) 表示），它是 \\(S\\) 的子集，由通过一条边与 \\(x\\) 相邻的所有实体组成。在图3.1(b)中，\\(S\\)中顶点\\(x\\)（红色）的邻域由所有黄色顶点组成。 在图3.1(b) 中，邻域 \\(\\mathcal{N}(x)\\) 的选择可以是任意的。例如，通过定义\\(mathcal{N}(x)\\)包含所有与红色顶点\\(x\\)的距离最多为2的顶点，可以给出另一个可选的有效邻域概念，即图3.1(c)中所有黄色顶点。在图3.1(d)中，\\(mathcal{N}(x)\\)邻域是由所有与红色顶点\\(x\\)形成三角形的黄色顶点组成的。在图3.1(e)中，\\(mathcal{N}(x)\\)邻域是由所有与红色顶点\\(x\\)形成梯形的黄色顶点组成的。从图（d）和（e）中可以清楚地看出，三角形和梯形等其他辅助结构也可以用来定义邻域概念。实际上，邻域的选择通常取决于应用。最近的研究探索了使用图形几何来获得更丰富的邻域概念，如 (Morris et al. 2019; Hajij, Istvan, and Zamzmi 2020; L. Zhao et al. 2022) 所示。不过，基本概念仍然相同：首先引入定义在顶点集上的辅助结构，然后利用辅助结构推导出定义明确的邻近性概念。 将图推广到高阶网络后，自然也要将图的邻域概念推广到高阶网络。拓扑学(Munkres 1974)对集合 \\(S\\) 实体间邻域或邻近性的精确概念已经进行了研究，定义在 \\(S\\) 上的拓扑允许我们有意义地描述 \\(S\\) 中元素之间的邻近性。本节将介绍拓扑概念和定义，目的是将图推广到高阶网络。 3.1 邻域函数和拓扑空间 拓扑空间有几种等价的定义方法。例如，拓扑空间通常用 “开集”或 “闭集”来定义，参见 (Munkres 1974)。在本文中，我们选择用 “邻域”来定义拓扑空间。这一定义更符合通常定义在图上的消息传递范式(Gilmer et al. 2017)，而且可以推广到高阶网络。关于为什么以邻域为单位的定义等同于以开集为单位的定义，我们请读者参阅(Brown 2006)。 定义 3.1 (领域函数) 令\\(S\\) 是非空集。\\(S\\)上的邻域函数是函数\\(\\mathcal{N}\\colon S\\to\\mathcal{P}(\\mathcal{P}(S))\\)，该函数给\\(S\\)中的每个点\\(x\\)分配一个\\(S\\)非空子集\\(\\mathcal{N}(x)\\)。\\(\\mathcal{N}(x)\\)中的元素被称为\\(x\\)相对于\\(\\mathcal{N}\\)的邻域。 定义 3.2 (领域拓扑) 令\\(\\mathcal{N}\\)是集合\\(S\\)上的邻域函数，如果\\(\\mathcal{N}\\)满足如下规则，则称\\(\\mathcal{N}\\)为\\(S\\)上的领域拓扑: 如果\\(N\\)是\\(x\\)的邻域, 则\\(x\\in N\\). 如果\\(N\\)包含\\(x\\)邻域的\\(S\\)的子集,则\\(N\\)是\\(x\\)的邻域. \\(S\\)内顶点\\(x\\)的两个邻域的交集也是\\(x\\)的邻域. \\(S\\)内顶点\\(x\\)存在一个邻域\\(M\\)，使得\\(x\\)的任何邻域\\(N\\)也是\\(M\\)内每个顶点的邻域。 定义 3.3 (拓扑空间) 由非空集\\(S\\)和\\(S\\)上的邻域拓扑\\(\\mathcal{N}\\)组成的\\((S,\\mathcal{N})\\)称为拓扑空间。 因此，拓扑空间就是定义了邻域函数\\(\\mathcal{N}\\)，且满足定义3.2中属性的集合\\(S\\)。在章节4.4，我们将引入高阶网络环境下关于近邻的类似概念。进一步来讲，邻域函数\\(\\mathcal{N}\\)的选择是构建高阶域支持的深度学习模型的第一步，也是最基本的一步(参见5章节)。 3.2 填补与高阶网络间的代沟 给定抽象实体组成的有限集\\(S\\)，\\(S\\)上的邻域函数\\(\\mathcal{N}\\)可以通过给\\(S\\)配置辅助结构来推导，例如边，如图3.1(b)所示。边提供了一种定义\\(S\\)内实体间关系的方法3，特别说明的是，每条边都定义了\\(S\\)上的一种二元关系（例如，两个实体间的关系）。在许多应用中，我们希望允许包含两个以上实体间的关系，这种涉及两个以上实体间关系的思想就是高阶网络的核心。这种高阶关系使得可以在\\(S\\)上定义更广泛的邻域函数，以捕获\\(S\\)内实体间的多路交互作用。 为了描述更广泛的多路相互作用，就需要使用更复杂的邻域函数和拓扑。章节 4着眼于定义通用的高阶网络（正如章节2所谈到的研究动机），本节则综述通常研究的高阶网络的定义、优势和劣势，包括（抽象的）单纯复形、正则胞腔复形（regular cell complexes）、超图等。在章节4，我们将引入组合复形，该定义更广义且能填补通常研究的高阶网络间的代沟。 单纯复形是带有期望属性的最简单的高阶域，它将图的相应属性进行了扩展。例如，Hodge理论是在单纯复形上很自然的定义，它扩展了图的类似属性(Barbarossa and Sardellitti 2020a; Schaub et al. 2020, 2021)。 定义 3.4 (单纯复形，Simplicial complex) 非空集\\(S\\)上的抽象单纯复形（abstract simplicial complex）被表示为\\((S,\\mathcal{X})\\)，其中，\\(\\mathcal{X}\\) 是\\(\\mathcal{P}(S) \\setminus \\{\\emptyset\\}\\)的子集，且满足\\(x \\in \\mathcal{X} \\bigcup y \\subseteq x \\rightarrow y \\in \\mathcal{X}\\)， \\(\\mathcal{X}\\)中的元素被称为单纯形（simplices） 译者注：原文中上述定义中的\\(x \\in \\mathcal{X} \\bigcup y \\subseteq x \\rightarrow y \\in \\mathcal{X}\\)是这么写的\\(x \\in \\mathcal{X}\\) and \\(y \\subseteq x\\) imply \\(y \\in \\mathcal{X}\\) 图2.6(c)给出了三角形网格（triangular meshes）的示例，这是计算机图形学中许多应用中常见的单纯复形的特例。读者可参考(Schaub et al. 2021; Crane et al. 2013)获得关于单纯复形的相关介绍。从定义 3.4中可看出，\\(S\\)上的每个关系\\(x\\)必须包含所有具有\\(y\\subseteq x\\)的关系\\(y\\)。因此，一个单纯复形可能会编码了相当大量的数据，这将占用大量内存(T. Mitchell Roddenberry, Schaub, and Hajij 2022)。此外，现实世界中的高阶数据（如矩形街道网络上的交通流量）可能无法采用有意义的单纯复形结构，这是因为底层数据空间本身缺乏可用的单纯形。为了解决这一局限，可以采用胞腔复形，胞腔复形(Hatcher 2005; Hansen and Ghrist 2019)可以泛化单纯复形，并克服它们的许多缺点。 译者注： 图2.6(c)是三角剖分 定义 3.5 (正则胞腔复形，Regular cell complex) 正则胞腔复形是一个可划分为多个子空间（胞腔）的拓扑空间 \\(S\\)，其子空间为 \\(\\{x_\\alpha\\}_{\\alpha \\in P_{S} }\\)，其中 \\(P_{S}\\) 是索引集。正则胞腔复形满足以下条件： \\(S= \\cup_{\\alpha \\in P_{S}} \\mbox{int}(x_{\\alpha})\\), 其中，\\(\\mbox{int}(x)\\) 胞腔\\(x\\)内部。 对每个\\(\\alpha \\in P_S\\), 都存在\\(x_\\alpha\\)的一个同胚（homeomorphism）映射\\(\\psi_{\\alpha}\\)（称作粘合映射，attaching map）, 将\\(x_\\alpha\\)映射到\\(\\mathbb{R}^{n_\\alpha}\\)，\\(n_\\alpha\\in \\mathbb{N}\\)，\\(n_\\alpha\\)称作胞腔\\(x_\\alpha\\)的维度（dimension）。 对每个胞腔\\(x_\\alpha\\)，边界(boundary)\\(\\partial x_{\\alpha}\\)是有限多个胞腔的并集，且每个这样的胞腔的维度都小于 \\(x_\\alpha\\)的维度。 译者注： 粘合映射，attaching map，描述如何将一个更高维的胞腔粘合到现有的胞腔复形上的过程。在这个过程中，粘合映射定义了新胞腔的边界如何与现有复形的低维胞腔相连。在构造拓扑空间时，如何将两个或多个空间粘合在一起形成一个新的空间。粘合映射是定义在边界上的一个连续映射，它将一个空间的边界映射到另一个空间的边界上，从而实现空间的粘合。 同态, homomorphism，描述的是两个代数结构之间的映射关系，这种映射保持了两个结构之间的运算关系。 同胚，homeomorphism，描述的是两个空间之间可以通过连续的变形从一个变成另一个，双方连续的一一映射，从拓扑学的角度来说就认为两个图是没有区别的。 条件2说明\\(x_\\alpha\\)可被编码为实数空间中的向量编码？ 条件3说明\\(x_\\alpha\\)的边界是比\\(x_\\alpha\\)低阶的结构？ 为简洁起见，我们下文将 “正则胞腔复形”称为 “胞腔复形”。胞腔复形包含多种高阶网络，许多高阶网络可以看作胞腔复形的实例。例如，胞腔复形是图、单纯复形和立方复形的自然推广(Hajij, Istvan, and Zamzmi 2020)。图2.6(d) 给出了胞腔复形的一些示例。直观地说，一个胞腔复形是多个胞腔的不相交并，其中每个胞腔都与某个 \\(k\\) 的 \\(k\\)维欧式球的内部同构。这些胞腔通过粘合映射以局部适合的方式连接在一起。正则胞腔复形的粘合映射信息可以组合形式存储在矩阵序列中，该矩阵称作关联矩阵（incidence matrices）(Hatcher 2005)，在章节4.4.1对这些矩阵有详细的描述。 定义3.5中的条件3被称作正则胞腔复形的正则条件（regularity condition）。正则性条件意味着，当且仅当 \\(x_{\\alpha} \\subseteq \\overline{x_{\\beta}}\\) 时，可以通过在索引集 \\(P_{S}\\) 中配备一个由\\(\\alpha\\leq\\beta\\)给出的偏序结构(Poset structure)来组合实现胞腔复形的拓扑信息，其中 \\(\\overline{x}\\) 表示胞腔 \\(x\\) 的闭包（closure）。该偏序结构通常称为面偏序（face poset）(Hansen and Ghrist 2019)，它表明胞腔复形的拓扑信息编码完全由面偏序结构确定(Hansen and Ghrist 2019)，这使得组合复形事实上可以通过偏序结构来组合表示(Aschbacher 1996; Klette 2000; Basak 2010; Savoy 2021)。 译者注 poset，偏序，Partial order set的简写 closure，闭包 定义3.5意味着胞腔复形内每个胞腔的边界胞腔也是胞腔复形中的胞腔，因此，可以把胞腔复形看作是不同维度的胞腔的集合，这些胞腔通过它们的边界相互关联。就关系而言，这意味着胞腔复形中胞腔的边界也必须是胞腔复形中的胞腔。虽然胞腔复形构成了高阶网络的一般类别，但这一属性对胞腔复形的关系设置了约束。在某些应用中，如果数据不满足这种约束，那么这种约束可能并不可取。为了消除对集合中实体间关系的所有限制，通常会考虑超图。 定义 3.6 (超图) 非空集\\(S\\)上的超图是\\((S,\\mathcal{X})\\)，其中，\\(\\mathcal{X}\\)是\\(\\mathcal{P}(S)\\setminus\\{\\emptyset\\}\\)的子集，\\(\\mathcal{X}\\)的元素被称作超图。 基数为 2（cardinality two） 的超边称为边。超图可以看作是单纯复形和胞腔复形的一般化。然而，超图并不直接包含胞腔（或关系）维度的概念，而胞腔复形的定义中明确包含了维度的概念，并且在单纯复形中关系的基数来表明维度的概念。正如我们在第4.3节中所演示的，单纯复形和胞腔复形中胞腔和关系的维度可以用来赋予这些复形以层次结构，而层次结构可以用来在这些结构上进行（非）池化计算。 3.3 层次化结构与集合型关系 正如第 3.2节所概述的那样，单纯复形、胞腔复形和超图的性质产生了高阶域上关系的两个主要特征，即关系的层次和集合型关系。在本小节中，我们将形式化这两个特征。 定义 3.7 (秩函数，Rank function) 高阶域\\(\\mathcal{X}\\)上的秩函数是一个保序函数（order-preserving function）\\(\\mbox{rk}\\colon \\mathcal{X}\\to \\mathbb{Z}_{\\ge 0}\\)，例如，\\(\\forall x,y\\in\\mathcal{X}, x\\subseteq y\\rightarrow \\mbox{rk}(x) \\leq \\mbox{rk}(y)\\) 。 译者注 原书中写作：\\(x\\subseteq y\\) implies \\(\\mbox{rk}(x) \\leq \\mbox{rk}(y)\\) for all \\(x,y\\in\\mathcal{X}\\) 直观地说，在高阶域\\(\\mathcal{X}\\)上的秩函数\\(\\mbox{rk}\\)给\\(\\mathcal{X}\\)中的每一个关系附加一个用非负整数值表示的秩，使得\\(\\mathcal{X}\\)中的集合胞腔通过\\(\\mbox{rk}\\)得以保全。实际上，秩函数在\\(\\mathcal{X}\\)上诱导了一个层次结构。胞腔和单纯复形是具有秩函数的高阶域的常见例子，因此具有层次化关系。。 定义 3.8 (集合型关系，Set-type relations) 如果高阶域中的一个关系的存在并不隐含于域中的另一个关系，那么这种关系就被称为集合型关系。 超图可算作具有集合类型关系的高阶域的例子。鉴于单纯复形、胞腔复形和超图在建模上的局限性，我们在章节 4 中提出了组合复形，这是一个同时具有层次关系和集合类型关系的高阶域。 参考文献 Aschbacher, Michael. 1996. “Combinatorial Cell Complexes.” In Progress in Algebraic Combinatorics, 1–80. Mathematical Society of Japan. Barbarossa, Sergio, and Stefania Sardellitti. 2020a. “Topological Signal Processing over Simplicial Complexes.” IEEE Transactions on Signal Processing 68: 2992–3007. Basak, Tathagata. 2010. “Combinatorial Cell Complexes and Poincaré Duality.” Geometriae Dedicata 147 (1): 357–87. Brown, Ronald. 2006. Topology and Groupoids. BookSurge Publishing. Crane, Keenan, Fernando De Goes, Mathieu Desbrun, and Peter Schröder. 2013. “Digital Geometry Processing with Discrete Exterior Calculus.” In ACM SIGGRAPH 2013 Courses, 1–126. Association for Computing Machinery. Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. “Neural Message Passing for Quantum Chemistry.” In International Conference on Machine Learning. Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Hansen, Jakob, and Robert Ghrist. 2019. “Toward a Spectral Theory of Cellular Sheaves.” Journal of Applied and Computational Topology 3 (4): 315–58. Hatcher, Allen. 2005. Algebraic Topology. Cambridge University Press. Klette, Reinhard. 2000. “Cell Complexes Through Time.” In Vision Geometry IX, 4117:134–45. SPIE. Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. Munkres, James R. 1974. Topology; a First Course. Prentice-Hall. Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. “Signal Processing on Cell Complexes.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Savoy, Maxime. 2021. “Combinatorial Cell Complexes: Duality, Reconstruction and Causal Cobordisms.” PhD thesis, École Polytechnique Fédérale de Lausanne. Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. 2020. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62 (2): 353–91. Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. “Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.” Signal Processing 187: 108149. Zhao, Lingxiao, Wei Jin, Leman Akoglu, and Neil Shah. 2022. “From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness.” In International Conference on Learning Representations. \\(S\\)上的关系是\\(S\\)的非空子集↩︎ "],["combinatorial-complexes.html", "第 4 章 组合复形 4.1 组合复形定义 4.2 CC同态和子CCs 4.3 引入CCs的动机 4.4 CCs上的邻域函数 4.5 CCs上的数据", " 第 4 章 组合复形 在这一节，我们引入了组合复形（combinatorial complexes ，CCs），一类新的高阶域，它一般化了图、单纯复形、胞腔复形、超图等结构。图4.1展示了CCs在这些域上一般化的第一个例子。表4.1列举了高阶域和图相关的关系特征,并概括了CCs对这种关系实现一般化的能力。 图 4.1: 演示CCs怎样一般化不同的域. (a):集合\\(S\\)由实体（顶点）抽象组成，但不包含关系(b): 图建模了顶点间的二元关系(例如，\\(S\\)的元素). (c): 单纯复形建模了高阶关系的层次化结构 (例如，关系间的关系) ，但对其关系的“形状”有严格的约束。 (d): 与单纯复形类似，胞腔复形建模了高阶关系的层次化结构，但是在关系的形状的表现上更灵活(例如，胞腔)。 (f):超图建模了\\(S\\)中元素间的任意集合型关系,但是这些关系不能反应层次化结构。(e): CC组合了胞腔复形（关系间的层次化结构）和超图（任何集合型关系）的特征,将两种域进行了一般化。 表 4.1: 以表格形式总结拓扑域和图的关系的相关特征. 回想一下，关系是域的一个元素，域通过它的关系和这些关系相互关联的方式来指定的。与关系相关的理想特征在第一列中标出，关系的层次结构意味着高阶域的关系可以有不同的等级，集合型关系不受关系之间或关系长度的限制，多关系耦合意味着每个关系都可以通过定义在高阶域上的多个邻域函数来拥有其他邻近关系，等级而非基数表示在高阶域的给定层次中具有相同等级的关系不需要具有相同的基数。 关系相关的特征 CC 超图 胞腔复形 单纯复形 层次化关系(Hierarcy of relations) √ √ √ 集合型关系(Set-type relations) √ √ 多关系耦合(Multi-relation coupling) √ √ √ 秩而非基数(Rank not cardinality) √ √ √ 章节4.1 引入了组合复形的定义并提供了若干CCs的示例，章节4.2定义了组合复形同态（CC-homomorphism）的概念并给出了了相关示例，章节4.3从实用角度介绍 CC 结构背后的动，章节4.4展示了邻域矩阵上邻域函数的计算方法，章节4.5引入了组合复形共链（CC-cochain）的概念。 4.1 组合复形定义 我们希望定义一种结构，该结构能够填补单纯/胞腔复形和超图间的代沟，正如章节3.3所描述的那样。为此，本章引入了组合复形（combinatorial complex，CC），一种高阶域，可以从三个角度来看，作为一个单纯复形，其胞腔和单纯子可以缺失；作为一个具有松弛结构的广义胞腔复形；或者作为一个通过加入秩函数而丰富的超图。 定义 4.1 (组合复形，Combinatorial complex) 组合复形（combinatorial complex，CC)是一个三元组\\((S,\\mathcal{X},\\mbox{rk})\\), \\(S\\)是集合，\\(\\mathcal{X}\\)是\\(\\mathcal{P}(S)\\setminus\\{\\emptyset\\}\\)的子集，\\(\\mbox{rk} \\colon \\mathcal{X}\\to \\mathbb{Z}_{\\ge 0}\\)是秩函数且满足如下属性: \\(\\forall s\\in S, \\{s\\}\\in\\mathcal{X}\\); 函数\\(\\mbox{rk}\\)是保序的（order-preserving），即：如果\\(x,y\\in \\mathcal{X}\\)满足\\(x\\subseteq y\\)，那么\\(\\mbox{rk}(x) \\leq \\mbox{rk}(y)\\). \\(S\\)的元素称作实体（entities）或顶点（vertices）, \\(\\mathcal{X}\\)的元素称作关系（relations）或胞腔（cells）, \\(\\mbox{rk}\\)称作CC的秩函数（rank function）。 简单起见, \\(\\mathcal{X}\\)被用作CC\\((S,\\mathcal{X},\\mbox{rk})\\)的速记符号。定义4.1建立了构建高阶网络的框架，在此基础上，我们可以定义通用的高阶深度学习架构。请注意，CC既表现出了层次关系，也表现出了集合类型关系。特别是，CC 的秩函数 \\(\\mbox{rk}\\)给CC引入了关系的层次结构。此外，由于定义 4.1中没有关系约束，CCs也可以包含集合型关系。因此，CCs包含了胞腔复形和超图，因为它们结合了两者与关系相关的特征。表 4.1提供了 CCs 与常见的高阶网络和图之间关系相关特征的比较总结。 备注. 我们通常要求CC中的每个单一胞腔\\(\\{s\\}\\)的\\(\\mbox{rk}(\\{s\\})=0\\)，这种约定使 CCs 自然地与单纯复形和胞腔复形（cellular complexes）相一致。 胞腔\\(x\\in\\mathcal{X}\\)的秩是\\(x\\)上秩函数\\(\\mbox{rk}\\)的值\\(\\mbox{rk}(x)\\)，CC\\(\\mathcal{X}\\)的维度\\(\\mbox{dim}(\\mathcal{X})\\)是 \\(\\mathcal{X}\\) 中胞腔的最大秩。秩为\\(k\\)的胞腔称作\\(k\\)-cell，记作\\(x^k\\)；CC \\(\\mathcal{X}\\)的\\(k\\)-骨架（skeleton）记作\\(\\mathcal{X}^{(k)}\\)，表示\\(\\mathcal{X}\\)内秩最大为\\(k\\)的胞腔的集合；秩为\\(k\\)的胞腔集合记作\\(\\mathcal{X}^k\\)，该集合对应于\\(\\mathcal{X}^k=\\mbox{rk}^{-1}(\\{k\\})\\)。\\(1\\)-cells称作\\(\\mathcal{X}\\)的边界（edges）。一般来讲，CC的边界可以包含不止两个节点，而边仅有两个节点的CCs称作基于图（graph-based）的CCs。本文中，主要关注基于图的CCs。 示例 4.1 (维度为2和3的CCs) 图4.2给出了四个CCs示例。例如，图4.2(a)是顶点集\\(S=\\{s_0,s_1,s_2\\}\\)上的2维CC，由\\(0\\)-cells \\(\\{s_0\\}\\), \\(\\{s_1\\}\\)，\\(\\{s_2\\}\\)组成 (用橙色表示)，\\(1\\)-cell \\(\\{s_0, s_1\\}\\) (紫色)，\\(2\\)-cell \\(\\{s_0, s_1, s_2\\} = S\\) (蓝色). 图 4.2: CCs示例，橙色圆圈代表顶点，粉色、蓝色和绿色分别代表秩为1、2和3的胞腔，(a)、(b) 和 (d) 中的每个 CC 的维度都等于 2，而 (c) 中的 CC 的维度等于 3。 4.2 CC同态和子CCs CC 同态是将 CC 互相联系起来的映射，CC 同态在描述将图或其他高阶域提升为 CC 的过程中发挥着重要作用。直观地说，提升映射是一个定义明确的过程，它将特定类型的域（如图）转换为另一种类型的域（如 CC）。提升映射的用处在于，它能将定义在 CC 上的深度学习模型的应用实现在更常见的域上，如图、胞腔复形或单纯复形。我们将在附录 B中详细研究提升映射，并提供相关示例。定义 4.2正式提出了CC同态的概念。 定义 4.2 (CC-同态，CC-homomorphism) 从CC \\((S_1, \\mathcal{X}_1, \\mbox{rk}_1)\\)到另一个CC\\((S_2, \\mathcal{X}_2, \\mbox{rk}_2)\\)的同态，也称作CC-同态，是一个映射函数 \\(f \\colon \\mathcal{X}_1 \\to \\mathcal{X}_2\\)，该函数需满足如下属性： 如果 \\(x,y\\in\\mathcal{X}_1\\) 满足 \\(x\\subseteq y\\), 那么 \\(f(x) \\subseteq f(y)\\). 如果 \\(x\\in\\mathcal{X}_1\\)，那么\\(\\mbox{rk}_1(x)\\geq \\mbox{rk}_2(f(x))\\). 定义4.2的条件2确保CC同态仅映射\\(\\mathcal{X}_1\\)中的\\(k\\)-cell到\\(\\mathcal{X}_2\\)中秩不大于\\(k\\)的胞腔。如果\\(\\forall x \\in \\mathcal{X}_1, \\mbox{rk}_1(x) = \\mbox{rk}_2(f(x))\\) 并且 \\(f\\)是单射（injective），那么称同态映射\\(f\\)是CC-嵌入（CC-embedding）。CC 嵌入在实践中非常有用，因为它们可以通过用高阶胞腔来增强域（例如图结构），从而将该域 “提升（lifting）”为 CC。示例 4.2 给出了三种CC嵌入，而示例4.3给出的CC同态不是CC嵌入。 示例 4.2 (CC嵌入，CC-embeddings) 图4.3(a) 和 (b)展示了两种CC嵌入。图4.3(a)中，左侧图形中的每个胞腔都会被发送到右侧 CC 中的相应胞腔。同样，在图4.3(b)中，左边胞腔复形中的每个胞腔都被传送到右边 CC 中的对应胞腔。很容易验证这两个映射都是 CC 嵌入。 图 4.3: CC同态示例. 粉色、蓝色和绿色分别代表秩为1、2、3胞腔。(a): 将 1 维 CC 嵌入 2 维 CC。(b)：一个 2 维的 CC 嵌入一个 3 维的 CC。(c) CC 同态的一个例子。左边CC \\(\\mathcal{X}_1\\)，右边是\\(\\mathcal{X}_2\\)，同态映射\\(f\\)用黑箭头表示。直观地说，CC同态映射 \\(f\\) 可以类似看作是 \\(S_1 =\\{1, 2, 3、 4\\}\\) 和 \\(S_2 =\\{a, b, c\\}\\) 之间连续函数的组合（combinatorial analogue），该连续函数塌缩（collapses）胞腔\\(\\{1,2,3\\}\\)到胞腔\\(\\{a,b\\}\\)，胞腔\\(\\{1,2,3,4\\}\\)到胞腔\\(\\{a,b,c\\}\\)。从这个角度观察，CC 同态推广了单纯映射(Munkres 2018)。 译者注 单纯映射(simplicial map)是联系复形的多面体之间的一类重要映射,它是从复形K的多面体|K|到复形L的多面体|L|的连续映射. 示例 4.3 (CC同态，CC-homomorphism) 这里给出一个非CC嵌入的CC同态例子。考虑集合\\(S_1 = \\{1, 2, 3,4\\}\\)和\\(S_2 = \\{a, b, c\\}\\)，令\\(\\mathcal{X}_1\\)表示由与\\(S_1\\)中元素对应的1个3-cell\\(\\{1,2,3,4\\}\\)、1个2-cell\\(\\{1,2,3\\}\\)和4个0-cells组成的\\(S_1\\)上的CC。同样，令\\(\\mathcal{X}_2\\)表示由与\\(S_2\\)中元素对应的1个3-cell\\(\\{a,b,c\\}\\)、1个2-cell \\(\\{a,b\\}\\)和3个0-cells。图4.3(c)可视化了CCs\\(\\mathcal{X}_1\\) 和 \\(\\mathcal{X}_2\\)。设有函数\\(f \\colon S_1 \\to S_2\\)定义为\\(f(1) = f(2) = a,~f(3) = b\\)和\\(f(4) = c\\)，很容易验证\\(f\\)诱导了从\\(\\mathcal{X}_1\\) 到 \\(\\mathcal{X}_2\\)的CC同态。 定义 4.3 (子CC，Sub-CC) 令\\((S,\\mathcal{X}, \\mbox{rk})\\)是一个CC，CC \\((S,\\mathcal{X}, \\mbox{rk})\\)的子组合复形（sub-combinatorial complex，sub-CC） 是 CC \\((A,\\mathcal{Y},\\mbox{rk}^{\\prime})\\)，使得 \\(A\\subseteq S\\), \\(\\mathcal{Y}\\subseteq\\mathcal{X}\\) 且 \\(\\mbox{rk}^{\\prime} = \\mbox{rk}|_{\\mathcal{Y}}\\)是\\(\\mbox{rk}\\)在\\(\\mathcal{Y}\\)上的约束（restriction）. 简单起见，本文把子CC \\((A,\\mathcal{Y},\\mbox{rk}^{\\prime})\\)记作\\(\\mathcal{Y}\\)，任何\\(A \\subseteq S\\)的子集都能用于按下述方式诱导子CC。考虑带有约束 \\(\\mbox{rk}|_{\\mathcal{X}_A}\\)的集合\\(\\mathcal{X}_A = \\{x \\in \\mathcal{X} \\mid x \\subseteq A\\}\\) ，很容易看到三角\\((A,\\mathcal{X}_A,\\mbox{rk}|_{\\mathcal{X}_A})\\) 构成了一个CC，称之为由\\(A\\)诱导的\\(\\mathcal{X}\\)的子CC。注意， 集合 \\(\\mathcal{X}\\) 中的任何一个胞腔都会诱导出一个子 CC，这个子 CC 是通过考虑包含在其中的所有胞腔而得到的。最后，对于任意 \\(k\\)，我们不难发现，CC \\(\\mathcal{X}^{k}\\) 的骨架 \\(\\mathcal{X}\\) 是一个子 CC。 示例 4.4 (子CC，Sub-CC) 回忆一下图 4.2(a)中给出的CC \\(\\mathcal{X}= \\{\\{s_0\\}, \\{s_1\\}, \\{s_2\\}, \\{s_0, s_1\\}, \\{s_0, s_1, s_2\\}\\}\\)，集合\\(A = \\{s_0, s_1\\}\\)诱导了\\(\\mathcal{X}\\)的子CC\\(\\mathcal{X}_A = \\{\\{s_0\\}, \\{s_1\\}, \\{s_0, s_1\\}\\}\\) 4.3 引入CCs的动机 CCs的定义4.1旨在实现高阶建模的所有目标，正如章节2所概述的那样。为了进一步说明定义 4.1的动机，我们考虑了 CCs 的池化操作以及 CCs在结构上的一些优势。 4.3.1 CCs上的池化操作 我们首先考虑图上池化的一般特征，然后演示如何通过 CCs 以统一的方式实现基于图的池化。图 \\(\\mathcal{G}\\) 上的通用池化函数是 \\(\\mathcal{POOL} \\colon \\mathcal{G} \\to \\mathcal{G}^{\\prime }\\)，其中 \\(\\mathcal{G}^{\\prime}\\) 是代表 \\(\\mathcal{G}\\)的粗粒度版本的池化图。 \\(\\mathcal{G}^{\\prime}\\)的顶点对应于原图\\(\\mathcal{G}\\)中的顶点簇（超级顶点），而\\(\\mathcal{G}^{\\prime}\\)中的边表示这些顶点簇之间是否存在连接。参见图 4.4。 图 4.4: 通过 CCs 实现的基于图的池化操作。(a): 在这里，图的池化操作用池化函数来表示，该函数将图及其上定义的数据映射到该图的粗粒度版本上。右侧池化图中的超级顶点（super-vertices）与左侧原图中的顶点簇相对应，而右侧图中的边则表示这些簇之间是否存在连接关系。 (b-d): 右侧池化图中的超级顶点可以通过从原图中得到的 CC 中的高阶增强胞腔（蓝色胞腔）来实现，右侧池化图中的边可以通过顶点与高阶增强胞腔之间的 CC的 关联矩阵来实现。 基于CCs提出的一些形式化描述将把基于图的池化操作作为特例来描述。尤其，在\\(\\mathcal{G}^{\\prime}\\)的顶点上定义的超级顶点（\\(\\mathcal{G}\\)上的顶点簇），通过将超级顶点表示为胞腔来增强\\(\\mathcal{G}\\)，从而可以实现为CC上的高秩胞腔（higher-order ranked cells）。这种层次化结构由原图\\(\\mathcal{G}\\)，还有带高秩胞腔增强的CC一起组成。这种以基于CC的高秩胞腔为基础实现的图上的池化操作是层次化的，因为新的胞腔可以以递归的方式组合在一起，从而获得底层空间的粗粒度版本。有了这种对 CCs 进行高阶池化操作的概念，我们在 7.2.1和 7.2.2两节中分别演示了如何表达图/网格上的池化操作和基于图像的池化操作。 从这个角度来看，CCs提供了在图、图像等高阶网络上定义池化操作的通用框架。 此外，还通过高阶池化操作探讨了 CCs 的两个主要特点。首先，CCs模型化集合型胞腔的能力为在顶点簇的形状上定义池化操作提供了灵活性，这种灵活性是实现自定义池化操作所必需的，在这种操作中，簇的形状可能是特定于任务的。其次，CC上的高秩胞腔（higher-order ranked， cells）对应于底层空间的粗粒度版本，这里提出的高阶池化可用于构建更粗粒度的表征，参见图4.4。需要注意的是，超图和胞腔复形等通用性较低的结构无法同时灵活地进行簇整形（ cluster shaping）和生成更粗粒度的底层空间表示。 4.3.2 CCs的结构化优势 除了实现基于图的池化操作，CCs还提供了多种结构优势。尤其，具体来说，CCs统一了众多常用的高阶网络，使得可以对拓扑特性进行细粒度分析，促进深度学习中拓扑特征的消息传递，并能灵活建模关系之间的关系。 灵活的高阶结构和细粒度消息传递。消息传递图模型在图的顶点之间传递消息，以学习图的表示，消息是根据顶点和边的特征计算出来的。这些消息更新顶点和边的特征，并从图的局部邻域收集消息。与其他高阶网络和图相比，CCs 的秩函数在两个方面更具通用性。首先，秩函数使得CCs在高阶结构表示方面更具灵活性；其次，在深度学习场景下，秩函数为CC提供了更细粒度的信息传递能力。例如，超图中的每个超边都被视为一个集合，没有秩的概念，因此所有超边都被统一处理，没有任何区别。更多详情请参见章节5。 在关系间灵活的建模关系，在用拓扑数据填充拓扑域的过程中，由于域中所有胞腔都缺乏可自然支持的数据，因此构建有意义的关系可能具有挑战性，在处理单纯复形或胞腔复形时尤其如此。例如，一个单纯复形的 \\(k\\) 个实体之间的任何关系都必须从 \\(k-1\\) 个实体的所有相应子集的关系中建立。现实世界的数据可能包含这些关系的一个子集，而不是全部。虽然胞腔复形在关系建模方面提供了更大的灵活性，但胞腔复形必须满足的边界条件限制了允许的关系类型。为了消除关系之间的所有限制，超图可以提供帮助，因为超图允许任意的集合类型关系。不过，超图不提供层次特征，这在需要同时考虑局部和全局特征的应用中可能会处于劣势。 4.4 CCs上的邻域函数 我们在 CC 上引入了 CC 邻域函数的概念作为一种利用CC中拓扑信息的机制。在实践中，精心设计邻域函数通常是学习任务的一部分。出于我们的目的，我们只讨论两类广义邻域函数，即指定邻接和关联的函数(adjacency and incidence)。从深度学习的角度来看， 从深度学习的角度看，CC-邻域函数奠定了扩展深度学习模型的一般性消息传递方案的基础，从而涵盖了几种最先进的 GNNs (P. Battaglia et al. 2016; Kipf and Welling 2016; P. W. Battaglia et al. 2018; Fey and Lenssen 2019; Loukas 2019; Morris et al. 2019). 给定一个 CC，我们的目标是描述该 CC 的子 CC 的局部邻近的胞腔。为此，我们定义了 CC 邻近函数，该函数与CC s上下文环境中的定义 3.1类似。 定义 4.4 (CC领域函数，CC-neighborhood function) CC \\((S,\\mathcal{X}, \\mbox{rk})\\)上的CC邻域函数记作\\(\\mathcal{N}\\)，该函数可为CC的每个子CC \\((A,\\mathcal{Y},\\mbox{rk}^{\\prime})\\) 分配一个\\(S\\)子集的非空集合\\(\\mathcal{N}(\\mathcal{Y})\\)。 不失一般性，我们假定邻域\\(\\mathcal{N}(\\mathcal{Y})\\)的元素是\\(\\mathcal{X}\\)的胞腔或子CCs。直观上来讲，sub-CC \\(\\mathcal{Y}\\)的邻域\\(\\mathcal{N}(\\mathcal{Y})\\)是\\(S\\)子集的集合，他们位于\\(\\mathcal{Y}\\)的局部邻近地区（local vicinity）。术语“局部邻近地区”是一般性描述，通常由具体情况而定。 定义 4.4是定义3.1的离散化版本，两个定义的相对应可参见图4.5。在本文的其余部分，CC 邻域函数被简地称为邻域函数。实际上，CC 邻域函数中编码的信息是用矩阵来表示的，如下所述。 图 4.5: 连续域邻域函数与 CC 邻域函数的可视化比较. (a): 具有连续域 \\(S\\) 的邻域函数为 \\(x\\in S\\) 分配了一组\\(S\\) 的子集 \\(\\mathcal{N}(x)\\) ，这些子集位于 \\(x\\) 的局部邻近地区。 (b): 类似的，CC \\((S,\\mathcal{X}, \\mbox{rk})\\)上的CC邻域函数分配给\\(x\\in S\\)一组\\(\\mathcal{X}\\)的子集\\(\\mathcal{N}(x)\\)，它们也在 \\(x\\)的局部邻近地区。 邻域函数诱导的邻域矩阵。 为了便于计算，可以用矩阵来表示邻域函数。关联矩阵、邻接矩阵和共邻接矩阵是编码各类邻域函数的常见矩阵形式。在定义 4.5中，我们引入了这些矩阵的广义化，称为 “邻域矩阵”。在这个定义中以及以后，我们用 \\(|S|\\) 表示集合 \\(S\\) 的基数（cardinality）。 定义 4.5 (邻域矩阵，Neighborhood matrix) 令\\(\\mathcal{N}\\)是定义在 CC \\(\\mathcal{X}\\)上的邻域函数，令\\(\\mathcal{Y}=\\{y_1,\\ldots,y_n\\}\\) 和 \\(\\mathcal{Z}=\\{z_1,\\ldots,z_m\\}\\) 是 \\(\\mathcal{X}\\)上两个胞腔的集合，使得\\(\\forall 1\\leq i \\leq n，\\mathcal{N}(y_{i}) \\subseteq \\mathcal{Z}\\)。则，\\(\\mathcal{N}\\)上关于\\(\\mathcal{Y}\\)和\\(\\mathcal{Z}\\)的邻域矩阵是\\(|\\mathcal{Z}| \\times|\\mathcal{Y}|\\)二值矩阵(binary matrix)\\(G\\)，如果\\(z_i\\in \\mathcal{N}(y_j)\\)，则其第\\((i,j)\\)个项\\([G]_{ij}\\)的值为\\(1\\)，否则为\\(0\\)。 备注. 在定义4.5中，\\(\\mathcal{N}(y_j)\\)存储在邻域矩阵\\(G\\)的第\\(j-th\\)列。因此，当我们使用邻域矩阵 \\(G\\) 时，我们用 \\(\\mathcal{N}_{G}(j)\\) 表示胞腔 \\(y_j\\) 的邻域函数。 在 CC 上定义有用的邻接函数有很多种方法。在这项工作中，我们仅限于最直接的邻域函数：关联邻域函数和邻接邻域函数。 4.4.1 CC中的关联关系（Incidence） 我们定义了关联的三种术语，以捕捉 CC 中胞腔关联结构的不同方面。首先，在定义4.6种，我们引入了下关联（down-incidence）和上关联（up-incidence）邻域函数来描述，通过任意秩的胞腔来描述胞腔的关联结构。其次，在定义4.7，引入了\\(k-down\\)下关联和\\(k-up\\)上关联邻域函数，通过特定秩\\(k\\)的胞腔来描述胞腔的关联结构。然后，在定义4.8，引入\\((r, k)\\)-incidence关联矩阵，以描述特定秩\\(r\\)和\\(k\\)的胞腔的关联结构。在下文中，我们假定 CC\\((S,\\mathcal{X}, \\mbox{rk})\\)的\\(\\mathcal{X}\\) 集合中的胞腔有固定的顺序。 定义 4.6 (下/上关联邻域函数，Down/up-incidence neighborhood functions) 令\\((S,\\mathcal{X}, \\mbox{rk})\\)是一个CC，CC的两个胞腔\\(x, y\\in\\mathcal{X}\\)被称作关联的（ incident），如果\\(x \\subsetneq y\\)或\\(y \\subsetneq x\\)。尤其，胞腔\\(x\\in\\mathcal{X}\\)的下关联邻域函数（down-incidence neighborhood function） \\(\\mathcal{N}_{\\searrow}(x)\\) 被定义为集合\\(\\{ y\\in \\mathcal{X} \\mid y \\subsetneq x\\}\\)，而\\(x\\)的上关联邻域函数（up-incidence neighborhood function） \\(\\mathcal{N}_{\\nearrow}(x)\\) 被定义为集合 \\(\\{ y\\in \\mathcal{X} \\mid x \\subsetneq y\\}\\). 定义4.7 与定义 4.6相比，定义 4.7提供了更精细的关联结构规定。具体来说，定义 4.7 和 4.6 分别描述了一个胞腔相对于特定秩或任意秩胞腔的关联结构。 定义 4.7 (k-down/up incidence neighborhood functions) 令\\((S,\\mathcal{X}, \\mbox{rk})\\) 是一个CC，对于任何\\(k\\in\\mathbb{N}\\), 胞腔\\(x\\)的\\(k-down\\)下关联邻域函数 \\(\\mathcal{N}_{\\searrow,k}(x)\\)被定义为集合\\(\\{ y\\in \\mathcal{X} \\mid y \\subsetneq x, \\mbox{rk}(y)=\\mbox{rk}(x)-k \\}\\)。胞腔\\(x\\)的\\(k-up\\)上关联邻域函数 \\(\\mathcal{N}_{\\nearrow,k}(x)\\) 被定义为集合\\(\\{ y\\in \\mathcal{X} \\mid y \\subsetneq x,\\mbox{rk}(y)=\\mbox{rk}(x)+k \\}\\)。 显然，\\(\\mathcal{N}_{\\searrow}(x)= \\bigcup_{k\\in \\mathbb{N}} \\mathcal{N}_{\\searrow,k}(x)\\)，并且\\(\\mathcal{N}_{\\nearrow}(x)= \\bigcup_{k\\in\\mathbb{N}} \\mathcal{N}_{\\nearrow,k}(x)\\)。直接关联（immediate incidence）尤其重要，为此，胞腔\\(x \\in \\mathcal{X}\\)的面（faces）集合定义为\\(\\mathcal{N}_{\\searrow,1} (x)\\)，并且共面（cofaces）集合定义为\\(\\mathcal{N}_{\\nearrow,1} (x)\\)。参见图4.6，给出了\\(k\\)-down和\\(k\\)-up关联邻域函数的描述。 定义 4.8 (邻域矩阵，Neighborhood matrix) 令\\((S,\\mathcal{X}, \\mbox{rk})\\)是CC，对于任何\\(0\\leq r&lt;k \\leq \\dim(\\mathcal{X})\\)下的\\(r,k \\in \\mathbb{Z}{\\ge 0}\\)， \\(\\mathcal{X}^{r}\\) 和 \\(\\mathcal{X}^{k}\\)之间的\\((r,k)\\)关联矩阵 \\(B_{r,k}\\)被定义为 \\(|\\mathcal{X}^r| \\times |\\mathcal{X}^k|\\)二值矩阵，其中，如果\\(x^r_i\\)关联于\\(x^k_j\\)，那么第\\((i, j)\\)-th项\\([B_{r,k}]_{ij}\\)等于1，否则等于0。 图 4.6: 维度为3的CC上的\\(k\\)-down 和 \\(k\\)-up 关联邻域函数。 (a): \\(k\\)-down 关联邻域函数， 目标橙色胞腔 \\(x\\) 的秩为3。从左到右，红色胞腔标记了\\(\\mathcal{N}_{\\searrow,1}(x)\\), \\(\\mathcal{N}_{\\searrow,2}(x)\\) 和 \\(\\mathcal{N}_{\\searrow,3}(x)\\). (b): \\(k\\)-up 关联邻域函数，目标橙色胞腔\\(x\\)秩为0，从左到右，红色胞腔标记了\\(\\mathcal{N}_{\\nearrow,1}(x)\\), \\(\\mathcal{N}_{\\nearrow,2}(x)\\) and \\(\\mathcal{N}_{\\nearrow,3}(x)\\). 定义4.8种的关联矩阵\\(B_{r,k}\\)规定了CC的邻域函数，关联矩阵诱导的邻域函数可用于构造CCs上的高阶消息传递实现方案，正如章节5所描述的。 4.4.2 CC内的邻接关系（Adjacency） CCs 既有关联函数，也有其他邻域函数。例如，对于简化为图的 CC，更自然的邻域函数是基于邻接关系的概念。关联关系定义了不同秩的胞腔之间的关系，而邻接关系定义了相同秩的胞腔之间的关系。定义 4.9、 4.10和 4.11/4.12引入了各自与关联关系相关的定义 4.6、 4.7和 4.8类似的（共）邻接关系。 定义 4.9 (共邻接邻域函数，(Co)adjacency neighborhood functions) 令\\((S,\\mathcal{X}, \\mbox{rk})\\)是CC，胞腔\\(x\\in \\mathcal{X}\\)的邻接邻域函数（adjacency neighborhood function） \\(\\mathcal{N}_{a}(x)\\)定义为集合 \\[\\begin{equation*} \\{ y \\in \\mathcal{X} \\mid \\mbox{rk}(y)=\\mbox{rk}(x), \\exists z \\in \\mathcal{X} \\text{ with } \\mbox{rk}(z)&gt;\\mbox{rk}(x) \\text{ such that } x,y\\subsetneq z\\}. \\end{equation*}\\] \\(x\\)的共邻接邻域函数（coadjacency neighborhood function） \\(\\mathcal{N}_{co}(x)\\) 定义为集合 \\[\\begin{equation*} \\{ y \\in \\mathcal{X} \\mid \\mbox{rk}(y)=\\mbox{rk}(x), \\exists z \\in \\mathcal{X} \\text{ with } \\mbox{rk}(z)&lt;\\mbox{rk}(x) \\text{ such that } z\\subsetneq y\\text{ and }z\\subsetneq x \\}. \\end{equation*}\\] 满足条件\\(\\mathcal{N}_{a}(x)\\) 或 \\(\\mathcal{N}_{co}(x)\\)的胞腔\\(z\\)称为桥接胞腔（bridge cell）. 定义 4.10 (k-(共)邻接领域函数，k-(co)adjacency neighborhood functions) 令\\((S, \\mathcal{X}, \\mbox{rk})\\)是CC，对于任何\\(k\\in\\mathbb{N}\\), 胞腔\\(x \\in \\mathcal{X}\\)的\\(k\\)-邻接邻域函数 \\(\\mathcal{N}_{a,k}(x)\\)定义为集合 \\[\\begin{equation*} \\{ y \\in \\mathcal{X} \\mid \\mbox{rk}(y)=\\mbox{rk}(x), \\exists z \\in \\mathcal{X} \\text{ with } \\mbox{rk}(z)=\\mbox{rk}(x)+k \\text{ such that } x,y\\subsetneq z \\}. \\end{equation*}\\] \\(x\\)的\\(k\\)-共邻邻域函数 \\(\\mathcal{N}_{co,k}(x)\\)定义为集合 \\[\\begin{equation*} \\{ y \\in \\mathcal{X} \\mid \\mbox{rk}(y)=\\mbox{rk}(x), \\exists z \\in \\mathcal{X} \\text{ with } \\mbox{rk}(z)=\\mbox{rk}(x)-k \\text{ such that } z\\subsetneq y\\text{ and }z\\subsetneq x \\}. \\end{equation*}\\] 定义 4.11 (邻接矩阵，Adjacency matrix) 对于满足\\(0\\leq r&lt;r+k \\leq \\dim(\\mathcal{X})\\)的任何\\(r\\in\\mathbb{Z}_{\\ge 0}\\) and \\(k\\in \\mathbb{Z}_{&gt;0}\\) ， \\(\\mathcal{X}^{r}\\)关于 \\(\\mathcal{X}^{k}\\)的胞腔之间的 \\((r,k)\\)-邻接矩阵 \\(A_{r,k}\\)被定义为\\(|\\mathcal{X}^r| \\times |\\mathcal{X}^r|\\) 二值矩阵（binary matrix），该矩阵的第\\((i, j)\\)-th项，如果\\(x^r_i\\) \\(k\\)-邻接于 \\(x^r_j\\)，那么\\([A_{r,k}]_{ij}\\)等于1，否则等于0。 定义 4.12 (共邻接矩阵，Coadjacency matrix) 对于满足\\(0\\leq r-k&lt;r \\leq \\dim(\\mathcal{X})\\)的任何\\(r\\in \\mathbb{Z}_{\\ge 0}\\) and \\(k\\in\\mathbb{N}\\) ， \\(\\mathcal{X}^{r}\\)关于\\(\\mathcal{X}^{k}\\)的胞腔之间的\\((r,k)\\)-共邻接矩阵 \\(coA_{r,k}\\) among the cells of \\(\\mathcal{X}^{r}\\) 定义为 \\(|\\mathcal{X}^r| \\times |\\mathcal{X}^r|\\)二值矩阵，如果\\(x^r_i\\) \\(k\\)-共邻于\\(x^r_j\\) ，那么该矩阵的\\((i, j)\\)-th项\\([coA_{r,k}]_{ij}\\)等于1，否则等于0。 显然, \\(\\mathcal{N}_{a}(x)= \\cup_{k\\in\\mathbb{N}} \\mathcal{N}_{a,k}(x)\\)，并且 \\(\\mathcal{N}_{co}(x)= \\cup_{k\\in\\mathbb{N}} \\mathcal{N}_{co,k}(x)\\)。图4.7给出了\\(k\\)-邻接和\\(k\\)-共邻邻域函数的描述。 图 4.7: 维度为3的域CC的\\(k\\)-(共)邻接邻域函数。(a): \\(k\\)-邻接邻域函数。目标橙色胞腔\\(x\\) 秩为0，从左到右，红色胞腔标记了\\(\\mathcal{N}_{a,1}(x)\\), \\(\\mathcal{N}_{a,2}(x)\\) 和 \\(\\mathcal{N}_{a,3}(x)\\)。 (b): \\(k\\)-共邻邻域函数，目标橙色胞腔\\(x\\)秩为2，从左到右，红色胞腔标记了\\(\\mathcal{N}_{co,1}(x)\\), \\(\\mathcal{N}_{co,2}(x)\\)和\\(\\mathcal{N}_{co,3}(x)\\). 4.5 CCs上的数据 由于我们对处理定义在 CC \\((S,\\mathcal{X},\\mbox{rk})\\) 上的数据感兴趣，因此我们引入了 \\(k\\)-共链空间（\\(k\\)-cochain spaces）、\\(k\\)-共链（\\(k\\)-cochains）和共链映射(cochain maps)等概念。 定义 4.13 (k-共链空间，k-cochain spaces) 令\\(\\mathcal{C}^k(\\mathcal{X},\\mathbb{R}^d )\\)是秩为\\(k \\in \\mathbb{Z}_{\\ge 0}\\)且维度为\\(d\\)的函数\\(\\mathbf{H}_k\\colon\\mathcal{X}^k\\to \\mathbb{R}^d\\)的\\(\\mathbb{R}\\)-向量空间（vector space）。\\(d\\) 称作数据维度（data dimension），\\(\\mathcal{C}^k(\\mathcal{X},\\mathbb{R}^d)\\)称作\\(k\\)-共链空间（cochain space），\\(\\mathcal{C}^k(\\mathcal{X},\\mathbb{R}^d)\\)中的元素\\(\\mathbf{H}_k\\)称作\\(k\\)-共链（cochains） 或 \\(k\\)-信号（signals）。 当底层CC很明确的时候，可以直接使用\\(\\mathcal{C}^k(\\mathcal{X})\\) 或 \\(\\mathcal{C}^k\\)来简略表示。此外，还可以说\\(k\\)-共链空间\\(\\mathcal{C}^k(\\mathcal{X})\\)被定义在\\(\\mathcal{X}\\)上。直观来讲，\\(k\\)-cochain可被解释为定义在\\(\\mathcal{X}\\)的\\(k\\)-cells上的信号(Grady and Polimeni 2010)。图4.8(a)给出了单纯复形的0、1、2秩上支持的共链。 图 4.8: CC在维度4上支持的\\(k\\)-cochains (左图) 和共链映射（ cochain maps，右图) 。左图： \\(k\\)-cochain可以被解释为定义在\\(k\\)-cells上的信号或特征向量（feature vector）。图中，维度3的共链附在顶点上，维度2的共链附在1-cells上，维度4的共链附在2-cells上。右图：\\(coA_{r,k}\\) 和 \\(A_{r,k}\\) 定义了等维共链空间之间的共链映射（cochain map），\\(B_{r,k}\\)定义了不同维度空间之间的共链映射。 当\\(\\mathcal{X}\\)是图时，\\(0\\)-cochains对应于图上的信号(Ortega et al. 2018)。通过对\\(\\mathcal{X}^k\\)中的胞腔排序，我们可以用欧式向量空间\\(\\mathbb{R}^{ |\\mathcal{X}^k| \\times d}\\) 规范地识别出\\(\\mathcal{C}^k(\\mathcal{X},\\mathbb{R}^d )\\) ，并且直接将\\(\\mathbf{H}_k\\)写作向量\\([ \\mathbf{h}_{x^k_1},\\ldots,\\mathbf{h}_{x^k_{|\\mathcal{X}^k|} }]\\)。其中，\\(\\mathbf{h}_{x^k_j} \\in \\mathbb{R}^d\\)是与胞腔\\(x^k_j\\)相关的特征向量。符号\\(\\mathbf{H}_{k,j}\\)指特征向量\\(\\mathbf{h}_{x^k_j}\\) ，以避免明确引用胞腔\\(x^k_j\\)。我们也可以处理共链空间之间的映射，他们被称为共链映射。 定义 4.14 (共链映射，Cochain maps) 对于\\(r&lt; k\\), 关联矩阵\\(B_{r,k}\\) 产生映射 \\[\\begin{align*} B_{r,k}\\colon \\mathcal{C}^k(\\mathcal{X}) &amp;\\to \\mathcal{C}^r(\\mathcal{X}),\\\\ \\mathbf{H}_k &amp;\\to B_{r,k}(\\mathbf{H}_k), \\end{align*}\\] 其中，\\(B_{r,k}(\\mathbf{H}_k)\\) 表示矩阵\\(B_{r,k}\\)和向量\\(\\mathbf{H}_k\\)的通常意义上的乘积\\(B_{r,k}\\mathbf{H}_k\\)。类似的，\\((r,k)\\)-邻接矩阵\\(A_{r,k}\\)产生映射 \\[\\begin{align*} A_{r,k}\\colon \\mathcal{C}^r(\\mathcal{X}) &amp;\\to \\mathcal{C}^r(\\mathcal{X}),\\\\ \\mathbf{H}_r &amp;\\to A_{r,k}(\\mathbf{H}_r). \\end{align*}\\] 共链空间之间的这两类映射都称为 共链映射（cochain maps）。 共链映射可以作为算子来用，例如在底层CC上的“变换（shuffle）”、“再分布（redistribute）”数据等操作。在TDL场景下，共链映射是定义高阶消息传递(Section 6.1)和反池化/池化操作(Section 7.1)的主要工具。每个邻接矩阵\\(A_{r,k}\\)或共邻接矩阵\\(coA_{r,k}\\)都定义了等维共链空间之间的共链映射，而每个关联矩阵\\(B_{r,k}\\)都定义了不同维度空间之间的共链映射。参见图4.8(b) 给出的维度4下的CC的共链映射示例。 参考文献 Battaglia, Peter W., Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, et al. 2018. “Relational Inductive Biases, Deep Learning, and Graph Networks.” arXiv Preprint arXiv:1806.01261. Battaglia, Peter, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu. 2016. “Interaction Networks for Learning about Objects, Relations and Physics.” In Proceedings of the 30th International Conference on Neural Information Processing Systems, 4509–17. NIPS’16. Red Hook, NY, USA: Curran Associates Inc. Fey, Matthias, and Jan Eric Lenssen. 2019. “Fast Graph Representation Learning with PyTorch Geometric.” arXiv Preprint arXiv:1903.02428. Grady, Leo J., and Jonathan R. Polimeni. 2010. Discrete Calculus: Applied Analysis on Graphs for Computational Science. Vol. 3. Springer. Kipf, Thomas N., and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907. Loukas, Andreas. 2019. “What Graph Neural Networks Cannot Learn: Depth Vs Width.” arXiv Preprint arXiv:1907.03199. Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. ———. 2018. Elements of Algebraic Topology. CRC press. Ortega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. “Graph Signal Processing: Overview, Challenges, and Applications.” Proceedings of the IEEE 106 (5): 808–28. "],["combinatorial-complex-neural-networks.html", "第 5 章 组合复形神经网络（Combinatorial complex neural networks） 5.1 构建 CCNN：张量图 5.2 前推操作（Push-forward operator）和聚合节点 5.3 三种主要的张量操作 5.4 组合复形卷积网络的定义（combinatorial complex convolutional networks） 5.5 组合复形注意力神经网络", " 第 5 章 组合复形神经网络（Combinatorial complex neural networks） CCs建模的灵活性使得可以研究和分析多种基于CC构建的神经网络架构，基于 CC 的神经网络可以利用所有邻域矩阵或其中的一个子集，从而考察 CC 中各个胞腔之间的多向交互作用，以解决学习任务。在本节中，我们将通过制定基于 CC 的 TDL 模型的一般原则来介绍 TDL 的蓝图。我们将利用我们的 TDL 蓝图框架来研究当前的方法，并为设计新型模型提供指导。 TDL中的学习任务可以粗略分成三类：胞腔分类、复形分类、胞腔预测，参见图5.1。我们在章节9的实验提供了关于胞腔和复分类的示例，三类学习任务的具体细节如下： 胞腔分类（Cell classification）：预测胞腔复形中每个胞腔。为了做到这一点，我们可以利用 TDL分类器，将目标胞腔的拓扑邻域及其相关特征考虑在内。胞腔分类的一个例子是三角形网格分割（triangular mesh segmentation），其中的任务是预测给定网格中每个面或边的类别。 复形分类（Complex classification）: 预测整个复形。为了实现这一目标，我们可以使用高阶胞腔将复形的拓扑结构还原成一个常见的表示形式，例如池化方法，然后在得到的平面向量（flat vector）上学习 TDL 分类器。复形分类的一个例子是对每个输入网格进行类别预测。 胞腔预测（Cell prediction）: 预测胞腔复形中胞腔-胞腔相互作用的特征，在某些情况下，预测胞腔复形中是否存在某个胞腔。这可以通过利用胞腔的拓扑结构和相关特征来实现。一个相关的例子是预测超图的超边中实体之间的联系。 图 5.1: 拓扑空间上的学习可以粗略地分为三类任务 (1) 胞腔分类（Cell classification）: 预测胞腔复形中单个胞腔。 这种任务的一个例子是网格分割，拓扑神经网络会为输入网格中的每个面输出一个分割标签。 (2) 复形分类（Complex classification）: 预测整个复形，涉及到将拓扑结构还原为一种通用的表示方法。输入网格的类别预测就是这一任务的一个例子。 (3) 胞腔预测（Cell prediction）: 通过利用底层胞腔复形的拓扑结构和相关特征，预测胞腔相互作用的属性，有时包括预测胞腔的存在。这种任务的一个例子是预测超图超桥中的联系。 图5.2概述了TDL的一般设置。首先，在集合 \\(S\\) 上构建一个由 CC 表示的高阶域。然后选择一组定义在该域上的邻域函数。邻域函数通常根据当前的学习问题来选择，并用于构建拓扑神经网络。为了开发我们的通用 TDL 框架，我们引入了组合复形神经网络（combinatorial complex neural networks，CCNN），这是一类由 CC 支持的抽象神经网络，它有效地捕捉了图5.2中的流水线。CCNN 可被视为一种模板，它概括了许多流行的架构，如卷积神经网络和注意力神经网络,CCNN 的抽象化具有很多优势。首先，任何适用于 CCNN 的结果都立即适用于 CCNN 架构的任何特定实例。事实上，只要符合 CCNN 定义，本文的理论分析和结果都适用于任何基于 CC 的神经网络。其次，如果神经网络的架构复杂，使用特定参数可能会很麻烦。在第 5.1节中，我们将详细介绍参数化 TDL 模型的复杂架构。CCNN 更抽象的高层表示简化了学习过程的符号和一般目的，从而使 TDL 建模更直观。 图 5.2: TDL蓝图 (a): 一组抽象实体. (b): 定义在\\(S\\)上的CC \\((S, \\mathcal{X}, \\mbox{rk})\\) . (c): 对于元素 \\(x \\in \\mathcal{X}\\), 选择定义在CC上的邻域函数集合 (d): 用(c)中选择的领域函数建立神经网络. 神经网络利用在 (c) 中选择的邻域函数来更新 \\(x\\) 上支持的数据. 定义 5.1 (组合复形神经网络，Combinatorial complex neural networks) 令 \\(\\mathcal{X}\\) 是 CC， 令\\(\\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\ldots \\times \\mathcal{C}^{i_m}\\) 和 \\(\\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\ldots \\times \\mathcal{C}^{j_n}\\) 是\\(m\\) 和 \\(\\mathcal{X}\\)上共链空间（cochain spaces）\\(n\\) 的笛卡尔积。 那么， 组合复形神经网络（combinatorial complex neural network，CCNN) 就是如下形式的函数 \\[\\begin{equation*} \\mbox{CCNN}: \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\ldots \\times \\mathcal{C}^{i_m} \\longrightarrow \\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\ldots \\times \\mathcal{C}^{j_n}. \\end{equation*}\\] 直观上，CCNN以共链向量 \\((\\mathbf{H}_{i_1},\\ldots, \\mathbf{H}_{i_m})\\)作为输入，返回共链向量\\((\\mathbf{K}_{j_1},\\ldots, \\mathbf{K}_{j_n})\\)作为输出。在章节5.1，我们将展示邻域函数如何在构建一般 CCNN 时发挥核心作用。定义 5.1并没有说明 CCNN 在一般情况下是如何计算的，章节 6 和 7将 形式化将 CCNN 的计算工作流程。 5.1 构建 CCNN：张量图 与涉及顶点或边缘信号的图不同，高阶网络需要更多的信号（见图 4.8）。因此，构建 CCNN 需要构建大量相互作用的子神经网络。由于通过 CCNN 处理的共链数量可能很大，我们引入了张量图（tensor diagrams），这是一种图解符号，用于描述拓扑域上支持的通用计算模型，并描述该域上支持和处理的各种信号的流向。 备注. 图解符号在几何拓扑文献中很常见Turaev (2016)，通常用于构建由更简单的构件构成的函数。进一步讨论见附录 C。关于单纯形神经网络的相关构造，另请参阅(T. Mitchell Roddenberry, Glaze, and Segarra 2021)。 定义 5.2 (张量图，Tensor diagram) 张量图可把CCNN表示成有相同图，张量图上的信号从源节点流向目标节点。源节点和目标节点分别对应 CCNN 的域和共域. 图5.3张量图的示例。左边显示的是三维的 CC，考虑0-cochain \\(\\mathcal{C}^0\\)，1-cochain \\(\\mathcal{C}^1\\)和2-cochain \\(\\mathcal{C}^2\\)；中间的图给出了CCNN，它把\\(\\mathcal{C}^0 \\times \\mathcal{C}^1\\times \\mathcal{C}^2\\)的共链向量映射到\\(\\mathcal{C}^0\\times\\mathcal{C}^1 \\times \\mathcal{C}^2\\)中的共链向量；右图给出的是CCNN的张量图。用共链映射或其矩阵表示来标注张量图上的每条边，张量图5.3上的边标签是\\(A_{0,1}, B_{0,1}^{T}, A_{1,1}, B_{1,2}\\) 和 \\(coA_{2,1}\\)。因此，该张量图指明了 CC 上的共链流向。 图 5.3: 张量图是 CCNN 的图解表示法，可捕捉 CCNN 上的信号流。 张量图箭头上的标签构成了一个\\(\\mathbf{G}= (G_i)_{i=1}^l\\) 的共链映射序列，该序列定义在底层 CC 上，例如在5.3中的\\(\\mathbf{G}=(G_i)_{i=1}^5 = (A_{0,1}, B_{0,1}^{T}, A_{1,1}, B_{1,2}, coA_{2,1})\\)。当使用张量图表示 CCNN 时，就可使用符号 \\(\\mbox{CCNN}_{\\mathbf{G}}\\) 表示张量图及其对应的 CCNN。共链映射\\((G_i)_{i=1}^l\\)反应了CC的结构，也确定了CC上的信号流。在第 4.4节中提到的任何邻域矩阵都可以用作共链图，共链映射的选择取决于学习任务。 图5.4可视化了张量图的其它示例。张量图的高度（heigh）是指从源节点到目标节点的最长路径上的边的数量。例如，图 5.4 (a) 和 5.4 (d) 中张量图的高度分别为 1 和 2。两个张量图的垂直连接表示其对应的 CCNN 的组成。例如，图 5.4(d)中的张量图就是图 5.4(c)和(b)中张量图的垂直连接。 图 5.4: 张量图示例 (a): 张量图 \\(\\mbox{CCNN}_{coA_{1,1}}\\colon \\mathcal{C}^1 \\to \\mathcal{C}^1\\). (b): 张量图 \\(\\mbox{CCNN}_{ \\{B_{1,2}, B_{1,2}^T\\}} \\colon \\mathcal{C}^1 \\times \\mathcal{C}^2 \\to \\mathcal{C}^1 \\times \\mathcal{C}^2\\). (c):合并节点，可合并三条共链. (d): 由 (c) 和 (b) 中的张量图垂直连接生成的张量图，边标签 \\(Id\\) 表示同一矩阵。 如果张量图上的节点可以收到一个或多个信号，就称为聚合节点（merge node）。在数学上来解释，聚合节点就是函数\\(\\mathcal{M}_{G_1,\\ldots ,G_m}\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\ldots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j}\\)，该函数由下式给出： \\[\\begin{equation} (\\mathbf{H}_{i_1},\\ldots,\\mathbf{H}_{i_m}) \\xrightarrow[]{\\mathcal{M}} \\mathbf{K}_{j}= \\mathcal{M}_{G_1,\\ldots,G_m}(\\mathbf{H}_{i_1},\\ldots,\\mathbf{H}_{i_m}), \\tag{5.1} \\end{equation}\\] 其中，\\(G_k \\colon C^{i_k}(\\mathcal{X})\\to C^{j}(\\mathcal{X}), k=1,\\ldots,m\\)，是共链映射。我们将 \\(\\mathcal{M}\\) 视为一个消息传递函数，它考虑了由映射 \\(G_1,\\ldots,G_m\\)输出的消息，这些映射共同作用于一个共链向量 \\((\\mathbf{H}_{i_1},\\ldots,\\mathbf{H}_{i_m})\\)，从而得到一个更新的共链 \\(\\mathbf{K}_{j}\\)。更多细节参看5.2 和 6.2，图5.4(c)就给出了这样的一个聚合节点示例。 5.2 前推操作（Push-forward operator）和聚合节点 本节引入了前推操作，它是一种计算方案，可以将 \\(i-cells\\)胞腔支持的共链发送到 \\(j-cells\\) 胞腔。前推操作是一个计算构件，用于形式化定义方程 (5.1)中给出的聚合节点、第 6章中介绍的高阶消息传递，以及第 7节中介绍的(un)pooling操作。 定义 5.3 (共链前推，Cochain push-forward) 在 CC \\(\\mathcal{X}\\)上, 有共链映射 \\(G\\colon\\mathcal{C}^i(\\mathcal{X})\\to \\mathcal{C}^j(\\mathcal{X})\\) 和 \\(\\mathbf{H}_i\\) in \\(\\mathcal{C}^i(\\mathcal{X})\\)。 \\(G\\)诱导的(cochain) push-forward 就是算子 \\(\\mathcal{F}_G \\colon \\mathcal{C}^i(\\mathcal{X})\\to \\mathcal{C}^j(\\mathcal{X})\\)，该算子定义如下 \\[\\begin{equation} \\mathbf{H}_i \\to \\mathbf{K}_j=[ \\mathbf{k}_{y^j_1},\\ldots,\\mathbf{k}_{y^j_{|\\mathcal{X}^j|} }] = \\mathcal{F}_G(\\mathbf{H}_i), \\end{equation}\\] 使得 \\(k=1,\\ldots,|\\mathcal{X}^j|\\), \\[\\begin{equation} \\mathbf{k}_{y_k^j}= \\bigoplus_{x_l^i \\in \\mathcal{N}_{G^T(y_k^j)}} \\alpha_{G} ( \\mathbf{ \\mathbf{h}_{x_l^i}}), \\tag{5.2} \\end{equation}\\] 其中，\\(\\bigoplus\\) 是具有置换不变性（permutation-invariant）的聚合函数，\\(\\alpha_G\\) 是可微函数（differentiable function）。 译者注：置换不变性（permutation-invariant），元素的次序对函数的响应没有影响。假设函数\\(f\\)将\\(\\mathcal{X}\\in \\mathbb{R}^{d}\\)域的向量空间映射到\\(\\mathcal{Y}\\)域的离散空间，作用于集合上的函数\\(f:\\mathcal{X}\\rightarrow \\mathcal{Y}\\)对集合中元素的作用在任何置换\\(\\pi\\)下都不变，即：\\(f(\\{x_{1},\\ldots,x_{M}\\})=f(\\{x_{\\pi(1)},\\ldots,x_{\\pi(M)})\\}\\)，则说\\(f\\)具有置换不变性。 算子\\(\\mathcal{F}_{G}\\)前推\\(\\mathcal{X}^i\\)支持的\\(i-cochain\\) \\(\\mathbf{H}_i\\)到\\(\\mathcal{X}^j\\)支持的 \\(j-cochain\\) \\(\\mathcal{F}_{G}(\\mathbf{H}_i)\\)。对于每个胞腔\\(y \\in \\mathcal{X}^j\\)，公式(5.2)通过聚合所有隶属于\\(y\\)由邻域函\\(\\mathcal{N}_{G^T}\\)确定的邻域\\(x \\in \\mathcal{X}^i\\)上的向量\\(\\mathbf{h}_x\\)来构造向量\\(\\mathbf{k}_y\\)，而且还对聚合向量集合\\(\\{ \\mathbf{h}_x| x\\in \\mathcal{N}_{G^T}(y)\\}\\)施加了微分函数\\(\\alpha_G\\)。 图5.5可视化了两个前推算子的示例。示例5.1提供了由关联矩阵（indicence matrix）诱导的前推函数，而示例5.1中的前推函数不包含任何参数，因此无法训练。在章节5.4，将给出参数化的前推操作，其参数是可训练的。 图 5.5: 前推操作示例 (a): 令 \\(G_1\\colon \\mathcal{C}^1\\to \\mathcal{C}^2\\) 是共链映射。由\\(G_1\\)诱导的前推 \\(\\mathcal{F}_{G_1}\\) 操作输入一个定义在底层CC \\(\\mathcal{X}\\)的边上的1-cochain \\(\\textbf{H}_{1}\\)，然后前推该共链到定义在\\(\\mathcal{X}^2\\)上的2-cochain \\(\\mathbf{K}_2\\)。通过用邻域函数 \\(\\mathcal{N}_{G_1^T}\\)聚合\\(\\mathbf{H}_1\\)上的消息就形成了共链\\(\\mathbf{K}_2\\)。在这种情况下，相对于 \\(G_1\\)，2-rank（蓝色）胞腔的邻居是该胞腔边界上的四条（粉色）边。（b）类似的，\\(G_2\\colon \\mathcal{C}^0\\to \\mathcal{C}^2\\)诱导前推映射\\(\\mathcal{F}_{G_2}\\colon \\mathcal{C}^0\\to \\mathcal{C}^2\\)，发送0-cochain \\(\\mathbf{H}_0\\)到2-cochain \\(\\mathbf{K}_2\\)。使用邻域函数\\(\\mathcal{N}_{G_2^T}\\)聚合\\(\\mathbf{H}_0\\)上的消息就形成了共链\\(\\mathbf{K}_2\\)。 示例 5.1 (维度2 CC上的前推操作) 在维度2 CC \\(\\mathcal{X}\\)上，令 \\(B_{0,2}\\colon \\mathcal{C}^2 (\\mathcal{X})\\to \\mathcal{C}^0 (\\mathcal{X})\\) 是关联矩阵。由\\(\\mathcal{F}^{m}_{B_{0,2}}(\\mathbf{H_{2}})= B_{0,2} (\\mathbf{H}_{2})\\)定义的函数\\(\\mathcal{F}^{m}_{B_{0,2}}\\colon\\mathcal{C}^2 (\\mathcal{X})\\to \\mathcal{C}^0 (\\mathcal{X})\\)是由\\(B_{0,2}\\)诱导的前推操作。\\(\\mathcal{F}^{m}_{B_{0,2}}\\) 前推共链 \\(\\mathbf{H}_{2}\\in \\mathcal{C}^2\\) 到共链 \\(B_{0,2} (\\mathbf{H}_{2}) \\in \\mathcal{C}^0\\). 在定义 5.4中, 我们使用前推算子来表述聚合节点的概念. 图5.6用张量图可视化了聚合节点的定义5.4 。 定义 5.4 (聚合节点，Merge node) 令 \\(\\mathcal{X}\\)是CC，令 \\(G_1\\colon\\mathcal{C}^{i_1}(\\mathcal{X})\\to\\mathcal{C}^j(\\mathcal{X})\\)和 \\(G_2\\colon\\mathcal{C}^{i_2}(\\mathcal{X})\\to\\mathcal{C}^j(\\mathcal{X})\\)是两个共链映射。给定共链向量 \\((\\mathbf{H}_{i_1},\\mathbf{H}_{i_2}) \\in \\mathcal{C}^{i_1}\\times \\mathcal{C}^{i_2}\\), 聚合节点 \\(\\mathcal{M}_{G_1,G_2}\\colon\\mathcal{C}^{i_1} \\times \\mathcal{C}^{i_2} \\to \\mathcal{C}^j\\) 被定义为 \\[\\begin{equation} \\mathcal{M}_{G_1,G_2}(\\mathbf{H}_{i_1},\\mathbf{H}_{i_2})= \\beta\\left( \\mathcal{F}_{G_1}(\\mathbf{H}_{i_1}) \\bigotimes \\mathcal{F}_{G_2}(\\mathbf{H}_{i_2}) \\right), \\end{equation}\\] 其中， \\(\\bigotimes \\colon \\mathcal{C}^j \\times \\mathcal{C}^j \\to \\mathcal{C}^j\\) 是聚合函数, \\(\\mathcal{F}_{G_1}\\) 和 \\(\\mathcal{F}_{G_2}\\)是\\(G_1\\) 和 \\(G_2\\)诱导的前推操作，并且\\(\\beta\\)是激活函数。 图 5.6: 聚合节点定义的可视化. 5.3 三种主要的张量操作 CCNN 的任何张量图都可以通过两个基本操作构建：前推算子和聚合节点。在实践中，引入其他操作可以更有效地构建相关的神经网络架构。例如，一个有用的操作是聚合节点的双重操作，称之为分裂节点。 定义 5.5 (分裂节点，Split node) 令 \\(\\mathcal{X}\\)是CC，令\\(G_1\\colon\\mathcal{C}^{j}(\\mathcal{X})\\to\\mathcal{C}^{i_1}(\\mathcal{X})\\) 和 \\(G_2\\colon\\mathcal{C}^{j}(\\mathcal{X})\\to\\mathcal{C}^{i_2}(\\mathcal{X})\\)是两个共链映射。给定共链\\(\\mathbf{H}_{j} \\in \\mathcal{C}^{j}\\)， 分裂节点操作 \\(\\mathcal{S}_{G_1,G_2}\\colon\\mathcal{C}^j \\to \\mathcal{C}^{i_1} \\times \\mathcal{C}^{i_2}\\) 定义为: \\[\\begin{equation} \\mathcal{S}_{G_1,G_2}(\\mathbf{H}_{j})= \\left( \\beta_1(\\mathcal{F}_{G_1}(\\mathbf{H}_{j})) , \\beta_2(\\mathcal{F}_{G_2}(\\mathbf{H}_{j})) \\right), \\end{equation}\\] 其中， \\(\\mathcal{F}_{G_i}\\) 是\\(G_i\\)诱导的前推操作，并且\\(\\beta_i\\)是激活函数，\\(i=1, 2\\)。 虽然从定义 5.5中可以看出，分裂节点只是前推操作的元组，但使用分裂节点可以让我们更有效、更直观地构建神经网络。定义 5.6提出了一组基本的张量运算，包括分裂节点，以方便用张量图来表述 CCNN。 定义 5.6 (基本张量运算) 我们将前推操作、聚合节点和分裂节点统称为基本张量操作。 图 5.7 给出了基本张量运算的张量图，图 5.8 举例说明现有拓扑神经网络如何通过基于基本张量运算的张量图来表达。 例如，文献(T. Mitchell Roddenberry, Glaze, and Segarra 2021)提出的基于霍奇分解（Hodge decomposition）的神经网络–单纯复形网（SCoNe），就可以有效地通分裂分和聚合节点来实现，如图所示。 5.8(a). 霍奇分解定理,曲面上任意一个光滑切向量场，可以被唯一地分解为三个向量场：梯度场、散度场和调和场 图 5.7: 基本张量运算（即前推运算、聚合节点和分裂节点）的张量图。这三种基本映射张量运算是构建一般 CCNN 张量图的基石. 利用三种基本张量运算的合成和横向连接，可以形成一般的张量图。 (a): 由共链 \\(G\\colon\\mathcal{C}^i \\to \\mathcal{C}^j\\) 定义的操作为。）（b）两个共链映射 \\(G_1\\colon\\mathcal{C}^{i_1} \\to \\mathcal{C}^j\\)和\\(G_2\\colon\\mathcal{C}^{i_2} \\to \\mathcal{C}^j\\) 诱导生成的聚合节点。（c）两个共链映射公 \\(G_1\\colon\\mathcal{C}^{j}\\to\\mathcal{C}^{i_1}\\) 和 \\(G_2\\colon\\mathcal{C}^{j}\\to\\mathcal{C}^{i_2}\\) 诱导生成的分裂节点. 在图示中，函数 \\(\\Delta \\colon \\mathcal{C}^{j}\\to \\mathcal{C}^{j}\\times \\mathcal{C}^{j}\\) 被定义为 \\(\\Delta(\\mathbf{H}_j)= (\\mathbf{H}_j,\\mathbf{H}_j)\\). 图 5.8: 可通过三种基本张量运算实现现有神经网络的示例。为简化说明，去掉了边标签. (a):文献(T. Mitchell Roddenberry, Glaze, and Segarra 2021)提出的单纯复形网络（simplicial complex net，SCoNe）可以实现为分裂节点的组合，把输入的1-cochain分裂为三个0维、1维、2维共链，然后通过一个聚合节点再把这些共链聚合为 1-cochain。 (b): 文献(Ebli, Defferrard, and Spreemann 2020)提出的单纯形神经网络（simplicial neural network，SCN）可以实现为前推操作。 (c)–(e): 胞腔复形神经网络的示例（cell complex neural networks，CXNs)，参见文献 (Hajij, Istvan, and Zamzmi 2020)。 注意： (e) 通过将0-cochains和2-cochains聚合为1-cochains的聚合节点，以及将1-cochain分裂为0-cochain和2-cochains的分裂节点来实现。 备注. 基本张量运算构成了定义任何参数化拓扑神经网络所需的唯一框架。事实上，既然可以通过三个基本张量运算建立张量图，那么只需定义前推运算和聚合运算，就可以完全定义一类参数化的 CCNN（回顾一下，分裂节点完全由前推运算决定）。 在章节5.4 和 5.5，我们建立了两类参数化的CCNN：卷积类和注意力类。在这两种情况下，我们只定义了相应的参数化基本张量运算。除了卷积和注意力版本的CCNNs，三种基本张量运算允许我们构建任意参数化的张量图，因此为发现新的拓扑神经网络架构提供了空间，而我们的理论仍适用于这些架构。 构建 CCNN 的另一种方法是借鉴拓扑量子场论（topological quantum field theory，TQFT）的思想。在附录 C中，我们简要地讨论了他们之间的深层关系。 5.4 组合复形卷积网络的定义（combinatorial complex convolutional networks） 在高阶域上的深度学习的基本计算需求之一是具备定义和计算卷积运算的能力。在本节，我们将介绍配备卷积算子的CCNNs，称之为组合复形卷积神经网络（combinatorial complex convolutional neural networks，CCCNN）。特别地，为 CCCNNs 提出了两种卷积算子：CC-卷积（CC-convolutional）前推算子和CC-卷积聚合节点。 我们将演示如何从两个基本模块引入 CCCNN：在章节5.2抽象定义的前推和聚合操作。正如在定义 5.7所设想的那样，CC-convolutional前推操作用最简单的形式将文献(Kipf and Welling 2016)中引入的卷积图神经网络进行了泛化。 定义 5.7 (CC卷积前推，CC-convolutional push-forward) 假设在一个CC \\(\\mathcal{X}\\) 上, 有共链映射 \\(G\\colon \\mathcal{C}^i (\\mathcal{X}) \\to \\mathcal{C}^j(\\mathcal{X})\\) 和 \\(\\mathbf{H}_i \\in C^i(\\mathcal{X}, \\mathbb{R}^{{s}_{in}})\\)。 CC-convolutional push-forward 操作就是共链映射 \\(\\mathcal{F}^{conv}_{G;W} \\colon C^i(\\mathcal{X}，\\mathbb{R}^{{s}_{in}}) \\to C^j(\\mathcal{X}, \\mathbb{R}^{{t}_{out}})\\) 定义如下 \\[\\begin{equation} \\mathbf{H}_i \\to \\mathbf{K}_j= G \\mathbf{H}_i W , \\tag{5.3} \\end{equation}\\] 其中， \\(W \\in \\mathbb{R}^{d_{s_{in}}\\times d_{s_{out}}}\\) 是可训练参数。 一旦有了CC-convolutional前推操作的定义，CC-convolutional聚合节点的定义（参见5.8）就是定义5.4的直接应用。在最近的文献 (Bunch et al. 2020; Ebli, Defferrard, and Spreemann 2020; Hajij, Istvan, and Zamzmi 2020; Schaub et al. 2020, 2021; T. Mitchell Roddenberry, Glaze, and Segarra 2021; Calmon, Schaub, and Bianconi 2022; Hajij, Zamzmi, et al. 2022; T. Mitchell Roddenberry, Schaub, and Hajij 2022; Yang and Isufi 2023)中也出现了一些定义5.8的变种。 定义 5.8 (CC卷积聚合节点，CC-convolutional merge node) 令 \\(\\mathcal{X}\\)是CC,令\\(G_1\\colon\\mathcal{C}^{i_1}(\\mathcal{X}) \\to\\mathcal{C}^j(\\mathcal{X})\\) 和 \\(G_2\\colon\\mathcal{C}^{i_2}(\\mathcal{X})\\to\\mathcal{C}^j(\\mathcal{X})\\)是两个共链映射。给定共链向量 \\((\\mathbf{H}_{i_1},\\mathbf{H}_{i_2}) \\in \\mathcal{C}^{i_1}\\times \\mathcal{C}^{i_2}\\)， CC-convolutional聚合节点 \\(\\mathcal{M}^{conv}_{\\mathbf{G};\\mathbf{W}} \\colon\\mathcal{C}^{i_1} \\times \\mathcal{C}^{i_2} \\to \\mathcal{C}^j\\) 可以定义如下： \\[\\begin{equation} \\begin{aligned} \\mathcal{M}^{conv}_{\\mathbf{G};\\mathbf{W}} (\\mathbf{H}_{i_1},\\mathbf{H}_{i_2}) &amp;= \\beta\\left( \\mathcal{F}^{conv}_{G_1;W_1}(\\mathbf{H}_{i_1}) + \\mathcal{F}^{conv}_{G_2;W_2}(\\mathbf{H}_{i_2}) \\right)\\\\ &amp;= \\beta ( G_1 \\mathbf{H}_{i_1} W_1 + G_2 \\mathbf{H}_{i_2} W_2 ),\\\\ \\end{aligned} \\end{equation}\\] 其中， \\(\\mathbf{G}=(G_1, G_2)\\), \\(\\mathbf{W}=(W_1, W_2)\\)是一组可训练参数，\\(\\beta\\) 是激活函数。 事实上，定义5.7中共链映射\\(G\\)的矩阵表示可能在训练过程中需要针对具体问题进行归一化（normalization）。关于高阶卷积算子背景下的各种归一化，我们推荐读者参阅(Kipf and Welling 2016; Bunch et al. 2020; Schaub et al. 2020) (Kipf and Welling 2016; Bunch et al. 2020; Schaub et al. 2020)。 我们用 \\(\\mbox{CCNN}_{\\mathbf{G}}\\) 概念表示张量图及其对应的 CCNN。 我们所用的负号表明，CCNN 是由基于定义在底层 CC 上的共链映射序列 \\(\\mathbf{G}= (G_i)_{i=1}^l\\) 的基本张量运算组成的。 当组成 CCNN 的基本张量运算由可训练参数序列 \\(\\mathbf{W}= (W_i)_{i=1}^k\\) 参数化时，就可以用 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\) 表示 CCNN 及其张量图表示。 5.5 组合复形注意力神经网络 大多数高阶深度学习模型都侧重于使用各向同性聚合(isotropic aggregation)的层，这意味着元素附近的邻近元素对元素表示的更新贡献相同。由于信息是以扩散的方式聚合的，这种各向同性的聚合会限制这些学习模型的表现力，从而导致过平滑（oversmoothing）等现象(Beaini et al. 2021)。相比之下，基于注意力的学习（attention-based learning） (Choi et al. 2017)允许深度学习模型为底层领域中元素局部附近的邻域分配概率分布，从而突出具有最相关任务信息的组件(Veličković et al. 2018)。 基于注意力的模型在实践中是成功的，因为它们忽略了域中的噪声，从而提高了信噪比（signal-to-noise ratio）(Mnih et al. 2014; J. B. Lee et al. 2019)。因此，基于注意力的模型在图的传统机器学习任务中取得了显著的成功，包括节点分类和链接预测 (Z. Li et al. 2021)、节点排名 (Sun et al. 2009) 和基于注意力的嵌入 (Choi et al. 2017; J. B. Lee, Rossi, and Kong 2018) 等。 章节5.4给CC-convolutional引入了前推操作，本节将给出前推操作的第二个示例：CC注意力前推操作（CC-attention push-forward）。 那么，就可以使用配备了CC-attention前推算子的CCNNs，称之为组合复形注意力神经网络（combinatorial complex attention neural networks，CCANNs）。我们首先提供一个 CC 的sub-CC \\(\\mathcal{Y}_0\\) 相对于 CC 中其他sub-CC 的注意力的一般概念。 定义 5.9 (高阶注意力，Higher-order attention) 令 \\(\\mathcal{X}\\) 是 CC, \\(\\mathcal{N}\\)是\\(\\mathcal{X}\\)上的邻域函数，\\(\\mathcal{Y}_0\\)是 \\(\\mathcal{X}\\)的sub-CC。 令 \\(\\mathcal{N}(\\mathcal{Y}_0)=\\{ \\mathcal{Y}_1,\\ldots, \\mathcal{Y}_{|\\mathcal{N}(\\mathcal{Y}_0)|} \\}\\) 是sub-CCs 集合，该集合的元素是邻域函数\\(\\mathcal{N}\\)下\\(\\mathcal{Y}_0\\)的邻居。那么，在\\(\\mathcal{N}\\)下，\\(\\mathcal{Y}_0\\)的higher-order attention就是函数 \\(a\\colon {\\mathcal{Y}_0}\\times \\mathcal{N}(\\mathcal{Y}_0)\\to [0,1]\\)，该函数给每个元素\\(\\mathcal{Y}_i\\in\\mathcal{N}(\\mathcal{Y}_0)\\)赋予权重 \\(a(\\mathcal{Y}_0, \\mathcal{Y}_i)\\)，使得 \\(\\sum_{i=1}^{| \\mathcal{N}(\\mathcal{Y}_0)|} a(\\mathcal{Y}_0,\\mathcal{Y}_i)=1\\). 译者注：原文中，上述定义写的是Cigher-order attention，我觉得是写错了，应该是Highr 正如定义5.9所反应的，邻域函数\\(\\mathcal{N}\\)下sub-CC \\(\\mathcal{Y}_0\\)的高阶注意力给\\(\\mathcal{Y}_0\\)的每个邻居赋予了一个离散分布值。 基于注意力的学习通常旨在学习函数 \\(a\\)。请注意，函数 \\(a\\) 依赖于邻域函数 \\(\\mathcal{N}\\)。在我们的语境中，我们的目标是学习邻域函数\\(a\\)，它是一个关联或（共）邻接函数 ，正如第4.4节所介绍的那样。 回忆一下定义5.9，权重\\(a(\\mathcal{Y}_0,\\mathcal{Y}_i)\\)需要一个源sub-CC \\(\\mathcal{Y}_0\\)和一个目标sub-CC \\(\\mathcal{Y}_i\\)作为输入。因此，一个CC-attention push-forward操作需要两个共链空间。定义5.10 引入了概念CC-attention push-forward，其中两个底层共链空间包含等秩胞腔上支持的共链。 定义 5.10 (等秩（equal rank）胞腔上的CC注意力前推CC-attention push-forward) 令 \\(G\\colon C^{s}(\\mathcal{X})\\to C^{s}(\\mathcal{X})\\)是邻域矩阵。 \\(G\\)诱导的CC-attention push-forward 就是一个共链映射 \\(\\mathcal{F}^{att}_{G}\\colon C^{s}(\\mathcal{X}, \\mathbb{R}^{d_{s_{in}}}) \\to C^{s}(\\mathcal{X},\\mathbb{R}^{d_{s_{out}}})\\)，定义为： \\[\\begin{equation} \\mathbf{H}_s \\to \\mathbf{K}_{s} = (G \\odot att) \\mathbf{H}_{s} W_{s} , \\tag{5.4} \\end{equation}\\] 其中，\\(\\odot\\) 是Hadamard积， \\(W_{s} \\in \\mathbb{R}^{d_{s_{in}}\\times d_{s_{out}}}\\)可训练参数， \\(att\\colon C^{s}(\\mathcal{X})\\to C^{s}(\\mathcal{X})\\) 是 高阶注意力矩阵（higher-order attention matrix），该矩阵和矩阵\\(G\\)有同样的维度。 矩阵\\(att\\)的第\\((i,j)\\)-th项定义为： \\[\\begin{equation} att(i,j) = \\frac{e_{ij}}{ \\sum_{k \\in \\mathcal{N}_{G}(i) e_{ik} } }, \\end{equation}\\] 其中， \\(e_{ij}= \\phi(a^T [W_{s} \\mathbf{H}_{s,i}||W_{s} \\mathbf{H}_{s,j} ] )\\), \\(a \\in \\mathbb{R}^{2 \\times s_{out}}\\)是可训练向量， \\([a ||b ]\\) 表示\\(a\\) 和 \\(b\\)的连接（concatenation）， \\(\\phi\\) 是激活函数，\\(\\mathcal{N}_{G}(i)\\)是矩阵\\(G\\)下胞腔\\(i\\)的邻居。 译者注：哈达玛积(Hadamard product)，若\\(A=(a_{i,j})\\)和\\(B=(b_{i,j})\\)是两个同阶矩阵，若\\(c_{i,j}=a_{i,j}\\times b_{i,j}\\)，则称矩阵\\(C=(c_{i,j})\\)为A和B的哈达玛积，或称基本积 定义 5.11处理的情况比定义 5.10更普遍。具体来说，定义 5.11引入了一个 CC-attention push-forward 的概念，在这个概念中，两个底层共链空间可以包含不同秩的胞腔上所支持的共链。 定义 5.11 (不等秩胞腔（unequal ranks）上的CC注意力前推CC-attention push-forward) 对于 \\(s\\neq t\\), 令 \\(G\\colon C^{s}(\\mathcal{X})\\to C^{t}(\\mathcal{X})\\) 是邻域矩阵，\\(G\\)诱导的CC-attention block 是共链映射 \\(\\mathcal{F}_{G}^{att} {\\mathcal{A}}\\colon C^{s}(\\mathcal{X},\\mathbb{R}^{d_{s_{in}}}) \\times C^{t}(\\mathcal{X},\\mathbb{R}^{d_{t_{in}}}) \\to C^{t}(\\mathcal{X},\\mathbb{R}^{d_{t_{out}}}) \\times C^{s}(\\mathcal{X},\\mathbb{R}^{d_{s_{out}}})\\)，定义为 \\[\\begin{equation} (\\mathbf{H}_{s},\\mathbf{H}_{t}) \\to (\\mathbf{K}_{t}, \\mathbf{K}_{s} ), \\end{equation}\\] 且 \\[\\begin{equation} \\mathbf{K}_{t} = ( G \\odot att_{s\\to t}) \\mathbf{H}_{s} W_{s} ,\\; \\mathbf{K}_{s} = (G^T \\odot att_{t\\to s}) \\mathbf{H}_{t} W_{t} , \\tag{5.5} \\end{equation}\\] 其中， \\(W_s \\in \\mathbb{R}^{d_{s_{in}}\\times d_{t_{out}}} , W_t \\in \\mathbb{R}^{d_{t_{in}}\\times d_{s_{out}}}\\)是可训练参数，\\(att_{s\\to t}^{k}\\colon C^{s}(\\mathcal{X})\\to C^{t}(\\mathcal{X}) , att_{t\\to s}^{k}\\colon C^{t}(\\mathcal{X})\\to C^{s}(\\mathcal{X})\\) 是 高阶注意力矩阵（higher-order attention matrices），这些矩阵分别和矩阵\\(G\\) 、 \\(G^T\\)有同样的维度。 矩阵\\(att_{s\\to t}\\)的第 \\((i,j)\\)-th项和\\(att_{t\\to s}\\)可定义为 \\[\\begin{equation} (att_{s\\to t})_{ij} = \\frac{e_{ij}}{ \\sum_{k \\in \\mathcal{N}_{G} (i) e_{ik} } },\\; (att_{t\\to s})_{ij} = \\frac{f_{ij}}{ \\sum_{k \\in \\mathcal{N}_{G^T} (i) f_{ik} } }, \\tag{5.6} \\end{equation}\\] 且 \\[\\begin{equation} e_{ij} = \\phi((a)^T [W_s \\mathbf{H}_{s,i}||W_t \\mathbf{H}_{t,j}] ),\\; f_{ij} = \\phi(rev(a)^T [W_t \\mathbf{H}_{t,i}||W_s \\mathbf{H}_{s,j}]), \\tag{5.7} \\end{equation}\\] 其中， \\(a \\in \\mathbb{R}^{t_{out} + s_{out}}\\) 是可训练向量，并且 \\(rev(a)= [ a^l[:t_{out}]||a^l[t_{out}:]]\\). 定义4.8中的关联矩阵可用作定义5.11中的邻居矩阵。在图5.9中，我们说明了关联邻域矩阵的 CC-attention 概念。图5.9(c)给出了与图5.9(a) 中显示的 CC 相关的非方正关联矩阵（non-squared incidence matrix） \\(B_{1,2}\\)。注意力模块\\(\\mbox{HB}_{B_{1,2}}\\)负责学习两个关联矩阵\\(att_{s\\to t}\\) 和 \\(att_{t\\to s}\\)，矩阵\\(att_{s\\to t}\\)和\\(B_{1,2}\\)有同样的形状，并且非零元素恰好位于 \\(B_{1,2}\\) 中元素等于 1 的地方。 在 \\(att_{s\\to t}\\) 中，每列 \\(i\\)都代表一个概率分布，它定义了第 \\(i\\)-th个 2-cell相对其关联1-cells的注意力。矩阵 \\(att_{t\\to s}\\) 的形状与 \\(B_{1,2}^T\\)相同，并且1-cells到2-cells的注意力也有同样的表示形式。 图 5.9: 图示不等秩胞腔上的CC-attention (a): 一个CC。 CC的每个 \\(2\\)-cell (蓝色面)都连接着它自己的关联\\(1\\)-cells(粉色边). (b): 图上给出了由胞腔及其关联关系确定的注意力权重。 (c): (a)中CC的关联矩阵。列\\([1,2,3]\\)中的非零元对应于\\([1,2,3]\\)关于\\(B_{1,2}\\)的邻域\\(\\mathcal{N}_{B_{1,2}}([1,2,3])\\) 备注. 前推共链\\(\\mathbf{K}_t\\)的计算需要两个共链，称作\\(\\mathbf{H}_s\\) and \\(\\mathbf{H}_t\\)。在公式(5.5)中，\\(\\mathbf{K}_t\\)直接依赖\\(\\mathbf{H}_s\\)，但是从(5.5)–(5.7)可见，它也通过\\(att_{s\\to t}\\)非直接依赖于\\(\\mathbf{H}_t\\)。此外，共链\\(\\mathbf{H}_t\\)在训练期间仅需要有\\(att_{s\\to t}\\)就行，但是在推断期间则不行。也就是说，CC-attention前推模块的计算在推断期间仅需要单个共链\\(\\mathbf{H}_s\\)即可，这和定义5.3中定义的通用共链前推操作的计算一样。 公式(5.4)和(5.5)中的算子\\(G \\odot att\\) 和 \\(G \\odot att_{s\\to t}\\) 可被看作\\(G\\)的注意力学习版本，这使得可以使用CCANNs来学习任意类型的离散外积分算子（discrete exterior calculus operators），正如附录D中所描述的那样。 参考文献 Beaini, Dominique, Saro Passaro, Vincent Létourneau, Will Hamilton, Gabriele Corso, and Pietro Liò. 2021. “Directional Graph Networks.” In International Conference on Machine Learning. Bunch, Eric, Qian You, Glenn Fung, and Vikas Singh. 2020. “Simplicial 2-Complex Convolutional Neural Nets.” NeurIPS Workshop on Topological Data Analysis and Beyond. Calmon, Lucille, Michael T. Schaub, and Ginestra Bianconi. 2022. “Higher-Order Signal Processing with the Dirac Operator.” In Asilomar Conference on Signals, Systems, and Computers. Choi, Edward, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng Sun. 2017. “GRAM: Graph-Based Attention Model for Healthcare Representation Learning.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Ebli, Stefania, Michaël Defferrard, and Gard Spreemann. 2020. “Simplicial Neural Networks.” NeurIPS Workshop on Topological Data Analysis and Beyond. Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Hajij, Mustafa, Ghada Zamzmi, Theodore Papamarkou, Vasileios Maroulas, and Xuanting Cai. 2022. “Simplicial Complex Representation Learning.” In Machine Learning on Graphs (MLoG) Workshop at ACM International WSD Conference. Kipf, Thomas N., and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907. Lee, John Boaz, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. 2019. “Attention Models in Graphs: A Survey.” ACM Transactions on Knowledge Discovery from Data 13 (6): 1–25. Lee, John Boaz, Ryan Rossi, and Xiangnan Kong. 2018. “Graph Classification Using Structural Attention.” In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Li, Zhifei, Hai Liu, Zhaoli Zhang, Tingting Liu, and Neal N Xiong. 2021. “Learning Knowledge Graph Embedding with Heterogeneous Relation Attention Networks.” IEEE Transactions on Neural Networks and Learning Systems. Mnih, Volodymyr, Nicolas Heess, Alex Graves, et al. 2014. “Recurrent Models of Visual Attention.” In Advances in Neural Information Processing Systems. Vol. 27. Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021. “Principled Simplicial Neural Networks for Trajectory Prediction.” In International Conference on Machine Learning. Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. “Signal Processing on Cell Complexes.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. 2020. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62 (2): 353–91. Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. “Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.” Signal Processing 187: 108149. Sun, Yizhou, Jiawei Han, Peixiang Zhao, Zhijun Yin, Hong Cheng, and Tianyi Wu. 2009. “RankClus: Integrating Clustering with Ranking for Heterogeneous Information Network Analysis.” In Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology. Turaev, Vladimir G. 2016. Quantum Invariants of Knots and 3-Manifolds. Vol. 18. Walter de Gruyter GmbH &amp; Co KG. Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. “Graph Attention Networks.” In International Conference on Learning Representations. Yang, Maosheng, and Elvin Isufi. 2023. “Convolutional Learning on Simplicial Complexes.” arXiv Preprint arXiv:2301.11163. "],["message-passing.html", "第 6 章 消息传递 6.1 高阶消息传递的定义 6.2 高阶消息传递神经网络就是CCNNs 6.3 聚合节点和高阶消息传递：量化比较 6.4 注意力高阶消息传递和CCANNs", " 第 6 章 消息传递 在本节中，我们将解释在第5.2节中引入的聚合节点与高阶消息传递之间的关系。 特别是，我们证明了 CC 上的高阶消息传递可以通过第 5.3节中介绍的基本张量运算来实现。 此外，我们还证明了 CCANNs（见章节5.5）与高阶消息传递之间的联系，并介绍了高阶消息传递的注意力版本。我们首先定义了CC上的高阶消息传递，泛化了(Hajij, Istvan, and Zamzmi 2020)中引入的概念。 我们要指出的是，这里讨论的许多构造都是最基本的形式，但还可以进一步扩展。这个方向的一个重要方面是构建与特定群的作用相关的不变量或等变量的信息传递协议。 6.1 高阶消息传递的定义 高阶消息传递指的是一种计算框架，它涉及使用一组邻域函数在高阶域中的实体和胞腔之间交换消息。 在定义@ref(def: homp-definition)中，我们形式化了CCs的高阶消息传递概念,图6.1说明了定义6.1。 定义 6.1 (CC上的高阶消息传递(Higher-order message passing)) 令 \\(\\mathcal{X}\\)是CC， \\(\\mathcal{N}=\\{ \\mathcal{N}_1,\\ldots,\\mathcal{N}_n\\}\\)是定义在 \\(\\mathcal{X}\\)上的邻域函数几何，\\(x\\)是胞腔， 并且对于某个\\(\\mathcal{N}_k \\in \\mathcal{N}\\) 有\\(y\\in \\mathcal{N}_k(x)\\)。 胞腔\\(x\\) 和 \\(y\\)之间的message \\(m_{x,y}\\)依赖于两个胞腔或胞腔上支持的数据。用\\(\\mathcal{N}(x)\\)表示多集合\\(\\{\\!\\!\\{ \\mathcal{N}_1(x) , \\ldots , \\mathcal{N}_n (x) \\}\\!\\!\\}\\)，用\\(\\mathbf{h}_x^{(l)}\\)表示在层\\(l\\)的胞腔\\(x\\)上支持的某些数据。由\\(\\mathcal{N}\\)诱导的\\(\\mathcal{X}\\)上的高阶消息传递按如下四点规则来定义。 \\[\\begin{align} m_{x,y} &amp;= \\alpha_{\\mathcal{N}_k}(\\mathbf{h}_x^{(l)},\\mathbf{h}_y^{(l)}), \\tag{6.1} \\\\ m_{x}^k &amp;= \\bigoplus_{y \\in \\mathcal{N}_k(x)} m_{x,y}, \\; 1\\leq k \\leq n, \\tag{6.2} \\\\ m_{x} &amp;= \\bigotimes_{ \\mathcal{N}_k \\in \\mathcal{N} } m_x^k, \\tag{6.3} \\\\ \\mathbf{h}_x^{(l+1)} &amp;= \\beta (\\mathbf{h}_x^{(l)}, m_x). \\tag{6.4} \\end{align}\\] 其中, \\(\\bigoplus\\)是置换不变性聚合函数（permutation-invariant aggregation function），称作 \\(x\\)的内领域（intra-neighborhood）； \\(\\bigotimes\\)是\\(x\\)的外邻域（inter-neighborhood） ，称作聚合函；\\(\\alpha_{\\mathcal{N}_k},\\beta\\) 等是可微函数。 图 6.1: 高阶信息传递示例. 左侧：选择一系列邻域函数 \\(\\mathcal{N}_1,\\ldots,\\mathcal{N}_k\\)， 选择通常取决于学习任务。 右侧：对于每个 \\(\\mathcal{N}_k\\) ，使用内部邻域函数 \\(\\bigoplus\\) 聚合消息。 然后，用邻域间函数 \\(\\bigotimes\\) 汇集从所有邻域获得的最终信息。 关于定义 6.1的一些讨论如下。首先，方程 (6.1)中的信息\\(m_{x,y}\\)并不仅仅取决于胞腔 \\(x、y\\) 上支持的数据\\(\\mathbf{h}_x^{(l)}\\)、\\(\\mathbf{h}_y^{(l)}\\)，还取决于胞腔本身。例如，如果\\(\\mathcal{X}\\)是一个胞腔复形，那么在计算信息\\(m_{x,y}\\)时，\\(x\\)和\\(y\\)的方向都会被考虑在内。 另外，如果\\(x\\cup y\\) 或 \\(x\\cap y\\) 是 \\(\\mathcal{X}\\) 中的胞腔，那么在计算信息\\(m_{x,y}\\)时包含它们的数据也可能是有用的。这种独特的特性只体现在高阶域中，而不会出现在基于图的消息传递框架中(Gilmer et al. 2017; Bronstein et al. 2021)4。其次，高阶消息传递依赖于邻域函数集\\(\\mathcal{N}\\)的选择。这也是只有在高阶领域中才会出现的独特特征，在高阶领域中，邻域函数必然是由一组邻域关系来描述的，而不是像基于图的消息传递那样由图邻接关系来描述。 第三，在公式 (6.1)中，由于\\(y\\)是根据邻域关系\\(\\mathcal{N}_k \\in\\mathcal{N}\\)来隐含定义的，所以函数\\(\\alpha_{\\mathcal{N}_k}\\)和消息\\(m_{x,y}\\)都取决于\\(\\mathcal{N}_k\\)。第四，邻域间 \\(\\bigotimes\\) 并不一定非得是一个置换不变的聚合函数。 例如，可以在多重集合 \\(\\mathcal{N}(x)\\) 上设置一个阶（order），并根据这个阶计算\\(m_x\\)。最后，高阶消息传递依赖于两个聚合函数，即邻内聚合函数和邻间聚合函数，而基于图的消息传递依赖于一个聚合函数。正如第4章节中所说明的，集合 \\(\\mathcal{N}\\)的选择，使得可以在高阶消息传递中使用各种邻域函数。 备注. 定义 5.3中给出的前推算子与公式 (6.1)的更新规则有关。 一方面，公式(6.1)需要两个共链\\(\\mathbf{X}_i= [\\mathbf{h}_{x^i_1}^{(l)},\\ldots,\\mathbf{h}_{x^i_{|\\mathcal{X}^i|}}^{(l)}]\\) 和 \\(\\mathbf{Y}_{j}^{(l)}=[\\mathbf{h}_{y^{j}_1}^{(l)},\\ldots,\\mathbf{h}_{y^{j}_{|\\mathcal{X}^{j}|}}^{(l)}]\\) 来计算 \\(\\mathbf{X}^{(l+1)}_i = [\\mathbf{h}_{x^i_1}^{(l+1)},\\ldots,\\mathbf{h}_{x^i_{|\\mathcal{X}^i|}}^{(l+1)}]\\)，因此\\(\\mathcal{C}^j\\) 和\\(\\mathcal{C}^i\\)上的信号必须存在，才能计算公式(6.1)。 从这个角度来看，把这个操作看作更新规则是很自然和习惯的。另一方面，定义 5.3中的前推算子计算的是在给定共链 \\(\\mathbf{H}_i\\in \\mathcal{C}^i\\)时，\\(\\mathcal{C}^j\\)中的共链\\(\\mathbf{K}_{j}\\)。 由于只需要 \\(\\mathbf{H}_i\\) 一条共链就可以完成这个计算，所以我们可以很自然地把方程 (5.2)看作一个函数。更多详情，请参见第 6.3节。 定义6.1给出的高阶消息传递框架可用于构造CC上的新的神经网络架构，正如在图5.2中隐含反应的那样。首先，对于给定的的CC \\(\\mathcal{X}\\)，给出\\(\\mathcal{X}\\)上支持的共链 \\(\\mathbf{H}_{i_1}\\ldots, \\mathbf{H}_{i_m}\\)。 其次，根据期望的学习任务选择合适的邻域函数集合。然后,用选定的邻域函数在输入共链\\(\\mathbf{H}_{i_1}\\ldots, \\mathbf{H}_{i_m}\\)上执行定义6.1中的更新规则。 重复第2、3步，直到获得最终结果。 定义 6.2 (高阶消息传递神经网络，Higher-order message-passing neural network) 凡是基于定义6.1构造的任何神经网络都可称作高阶消息传递神经网络. 6.2 高阶消息传递神经网络就是CCNNs 在本节，将表明高阶消息传递计算可以用聚合节点计算来实现，因此，高阶消息传递神经网络就可认为是CCNNs。结果，高阶消息传递就可以通过一致性更新规则集合来统一单纯复形、胞腔复形、超图上的消息传递，作为可选项，也可以通过张量图表示语言来实现统一。 定理 6.1 (聚合节点计算，Merge node computation) 定义6.1中的高阶消息传递计算可以用聚合节点计算来实现。 证明. 令 \\(\\mathcal{X}\\)是CC，\\(\\mathcal{N}=\\{ \\mathcal{N}_1,\\ldots,\\mathcal{N}_n\\}\\)是定义6.1中规定的一组邻域函数，\\(G_k\\)是邻域函数\\(\\mathcal{N}_k\\)诱导的矩阵。假定定义6.1指定的胞腔 \\(x\\)是 \\(j\\)-cell，\\(y \\in \\mathcal{N}_k(x)\\)的邻居是 \\(i_k\\)-cells。 这里将指出公式 (6.1)–(6.4)可以看作是聚合节点的应用实现。接下来，我们将定义的邻域函数是\\(\\mathcal{N}_{Id}(x)=\\{x\\}\\) for \\(x\\in \\mathcal{X}\\)。 此外，我们将用\\(Id\\colon\\mathcal{C}^j\\to \\mathcal{C}^j\\)来标记与\\(\\mathcal{N}_{Id}\\)关联的的邻居矩阵，因为它就是一个单位阵（identity matrix）。 译者注：最后一句不知道翻的对不对，感觉句子不通顺，意思不明确。 计算公式(6.1)中的消息\\(m_{x,y}\\)涉及到两个共链: \\[\\begin{equation*} \\mathbf{X}_j^{(l)}= [\\mathbf{h}_{x^j_1}^{(l)},\\ldots,\\mathbf{h}_{x^j_{|\\mathcal{X}^j|}}^{(l)}],~ \\mathbf{Y}_{i_k}^{(l)}= [\\mathbf{h}_{y^{i_k}_1}^{(l)},\\ldots,\\mathbf{h}_{y^{i_k}_{|\\mathcal{X}^{i_k}|}}^{(l)}]. \\end{equation*}\\] 每个消息\\(m_{x^{^j}_t, y^{i_k}_s }\\)都与矩阵\\(G_k\\)中的一个项\\([G_k]_{st}\\)相对应。换句话说，矩阵\\(G_k\\)的每个非零项和消息\\(m_{x^{^j}_t, y^{i_k}_s }\\)都是一一对应的。 从第5.2节可以看出，计算 \\(\\{m_x^k\\}_{k=1}^n\\) 相当于聚合节点 \\(\\mathcal{M}_{Id_j、 G_k}\\colon \\mathcal{C}^j\\times \\mathcal{C}^{i_k}\\to \\mathcal{C}^j\\) 执行通过 \\(\\alpha_k\\) 和 \\(\\bigoplus\\) 确定的计算，并且得出 \\[\\begin{equation*} \\mathbf{m}_j^k=[m_{x^j_1}^k,\\ldots,m_{x^j_{|\\mathcal{X}^j|}}^k]= \\mathcal{M}_{Id_j,G_k}(\\mathbf{X}_j^{(l)},\\mathbf{Y}_{i_k}^{(l)}) \\in \\mathcal{C}^{j}. \\end{equation*}\\] 在这个阶段， 有 \\(n\\) \\(j\\)-cochains \\(\\{\\mathbf{m}_j^k\\}_{k=1}^n\\)。公式(6.3) 和 (6.4) ，将这些共链与输入的\\(j\\)-cochain \\(\\mathbf{X}_j^{(l)}\\)聚合。 尤其，计算公式 (6.3)中的 \\(m_x\\) 相当于在共链 \\(\\{\\mathbf{m}_j^k\\}_{k=1}^n\\)上应用\\(n-1\\)次 $_{Id_k,Id_k}^j \\(的聚合节点操作。 很明显，首先通过聚合\\)_j^1$和 \\(\\mathbf{m}_j^2\\) 来获得 \\(\\mathbf{n}_j^1=\\mathcal{M}_{Id_j,Id_j}(\\mathbf{m}_j^1,\\mathbf{m}_j^2)\\)。然后，聚合\\(j\\)-cochain \\(\\mathbf{n}_j^1\\) 和 \\(j\\)-cochain \\(\\mathbf{m}_j^3\\)，等等。最后的聚合节点是执行聚合操作 \\(\\mathbf{n}_j^{n-1}=\\mathcal{M}_{Id_j,Id_j}(\\mathbf{n}_j^{n-2},\\mathbf{m}_j^n)\\)， 也即 \\(\\mathbf{m}_j = [ m_{x_1^j},\\ldots, m_{x_{|\\mathcal{X}^j|}^j }]\\)5. 最后，计算由聚合节点\\(\\mathcal{M}_{(Id_j,Id_j)}(\\mathbf{m}_j, \\mathbf{X}_j^{(l)})\\)（该计算由公式(6.4)中的函数\\(\\beta\\)确定）实现的 \\(\\mathbf{X}_j^{(l+1)}\\) 。 定理 6.1表明CCs上定义的高阶消息传递网络可以从基本张量操作来构造， 因此它们是 CCNN 的特例。我们在定理6.2中正式阐述了这一结果。 定理 6.2 (高阶消息传递和CCNs,Higher-order message passing and CCNNs) 高阶信息传递神经网络就是 CCNN。 证明. 从定义 6.2和定理 6.1可以立即得出该结论。 由定理 6.2可知，定义在通用性不如 CCs 的高阶域（如单纯复形、胞腔复形和超图）上的高阶消息传递神经网络也是 CCNNs 的特例。因此，定义5.2中引入的张量图形成了一种通用的图解方法，用于表达定义在常用高阶域上的神经网络。 定理 6.3 (消息传递神经网络和张量图，Message-passing neural networks and tensor diagrams) Message-passing neural networks defined on simplicial complexes, cell complexes or hypergraphs can be expressed in terms of tensor diagrams and their computations can be realized in terms of the three elementary tensor operators. 证明. 从两点可得出这一结论，即：定理@ref(thm:thm-unifying)，以及单纯复形、胞腔复形和超图可看作是 CCs 的特例实现。 定理 6.2 和 6.3 基于张量图提出了一个统一的TDL框架，从而为今后的发展提供了空间。 例如, 文献(Papillon et al. 2023) 已经利基于该框架，用张量图来表达了单纯复形、胞腔复形和超图等现有的 TDL 架构。 6.3 聚合节点和高阶消息传递：量化比较 定义 6.1中给出的高阶消息传递提供了一种更新规则，可以使用由 \\(\\mathcal{N}(x)\\) 确定的一组邻域向量 \\(\\mathbf{h}_y^{l}\\) 从向量 \\(\\mathbf{h}_x^{l}\\) 获得向量 \\(\\mathbf{h}_x^{l+1}\\)。显然，这个计算框架假定向量 \\(\\mathbf{h}_x^{(l)}\\) 和 \\(\\mathbf{h}_{y}^{(l)}\\) 是作为输入提供的。 换句话说，根据定义 6.1需要目标域中的 共链\\(\\mathbf{X}_j^{(l)} \\in \\mathcal{C}^{j}\\)，以及共链 \\(\\mathbf{Y}_{i_k}^{(l)} \\in \\mathcal{C}^{i_k}\\)，以便计算更新后的 \\(j-\\)cochain \\(\\mathbf{X}_j^{(l+1)}\\) 。 另一方面，进行聚合节点计算需要一个共链向量 \\((\\mathbf{H}_{i_1},\\mathbf{H}_{i_2})\\)，这可以从公式 (5.1)和定义 5.4中看出。 这两种计算框架之间的差异可能看起来是符号性的，并且消息传递的视角可能看起来更直观，尤其是在处理基于图的模型时。然而，我们认为，在存在自定义高阶网络架构的情况下，聚合节点框架在计算上更自然、更灵活。为了说明这一点，我们考虑一下图 6.2中可视化的示例。 在图6.2中显示的神经网络有一个共链输入向量 \\((\\mathbf{H}_0,\\mathbf{H}_2) \\in \\mathcal{C}^0 \\times \\mathcal{C}^2\\)，第一层的神经网络负责计算共链\\(\\mathbf{H}_1 \\in \\mathcal{C}^1\\)，第二层的神经网络负责计算共链 \\(\\mathbf{H}_3\\in \\mathcal{C}^3\\)。为了在第一层获得共链\\(\\mathbf{H}_1\\)，需要考察由\\(B_{0,1}^T\\) 和 \\(B_{1,2}\\)诱导的邻域函数。 然而，如果使用公式 (6.1) 和 (6.2)执行在图6.2中张量图第一层确定的计算时，那么应该注意到在\\(\\mathcal{C}^1\\)上并没有提供共链来作为输入的一部分。因此，当使用公式(6.1) 和(6.2)时，需要特殊处理，因为向量 \\(\\mathbf{h}_{x^1_j}\\) 还没有计算出来。 请注意，GNN 中不存在这种人工制品，因为它们经常更新节点特征，而节点特征通常是作为输入的一部分提供的。具体来说，在 GNN 中，公式 (6.1)更新规则中的前两个参数是在底层图的 0-cells上支持的共链。 图 6.2: 图中描述的神经网络可以实现为两个聚合节点的组合。更具体的说，输入是共链向量 \\((\\mathbf{H}_0,\\mathbf{H}_2)\\)，第一个聚合节点负责计算1-chain \\(\\mathbf{H}_1 = \\mathcal{M}_{B_{0,1}^T,B_{1,2}} (\\mathbf{H}_0,\\mathbf{H}_2)\\)；第二个聚合节点 \\(\\mathcal{M}_{B_{1,3}^T, B_{2,3}} \\colon\\mathcal{C}^1 \\times \\mathcal{C}^2 \\to \\mathcal{C}^3\\)负责计算3-cochain \\(\\mathbf{H}_3=\\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\\mathbf{H}_1,\\mathbf{H}_2)\\)。从聚合节点的角度来看，可以计算出 1-cells 胞腔和 3-cells 胞腔支持的共链，而无需这些胞腔的初始共链。另一方面，高阶消息传递框架有两个主要限制：一是假定输入域所有维度所有胞腔上都有支持的初始共链，二是假定每次迭代都能更新输入域所有维度所有胞腔上所支持的所有共链。 类似的，为了计算图6.2中第二层的共链\\(\\mathbf{H}_3 \\in \\mathcal{C}^3\\)，就必须考虑由\\(B_{1,3}^T\\) and \\(B_{2,3}\\)诱导的邻域函数，而且必须使用共链向量\\((\\mathbf{H}_1,\\mathbf{H}_2)\\)。 这意味着图6.2中给出的神经网络计算得出的 共链\\(\\mathbf{H}_1\\) 和 \\(\\mathbf{H}_3\\) 不能通过迭代过程得到。 此外，输入向量 \\(\\mathbf{H}_0\\) 和\\(\\mathbf{H}_2\\) 在迭代过程的任何一步都不会被更新，结果就导致共链 \\(\\mathbf{H}_1\\) 和 \\(\\mathbf{H}_3\\) 也从不会被更新。从更新规则（如高阶消息传递框架中出现的更新规则）的角度来看（定义6.1），这种设置是不自然的，因为它假定所有维度的所有胞腔上的初始共链都可作为输入，并在每次迭代时都要对输入域的的复形上所有胞腔所支持的所有共链进行更新。 事实上，如果在使用高阶消息传递框架时遇到这样的困难，可以使用一些临时的解决办法，比如通过开启或关闭某些共链的迭代，或通过引入辅助共链来解决，聚合节点就是为了克服这些限制而设计的。 尤其, 从聚合节点的角度来看，我们可以把图6.2的第一层看作是函数\\(\\mathcal{M}_{B_{0,1}^T,B_{1,2}}\\colon \\mathcal{C}^0 \\times \\mathcal{C}^1 \\to \\mathcal{C}^1\\)，参见公式(5.2)。函数 \\(\\mathcal{M}_{B_{0,1}^T,B_{1,2}}\\) 把共链向量 \\((\\mathbf{H}_0,\\mathbf{H}_2)\\)作为输入，然后来计算1-chain \\(\\mathbf{H}_1 = \\mathcal{M}_{B_{0,1}^T,B_{1,2}} (\\mathbf{H}_0,\\mathbf{H}_2)\\)。 类似的，可以用聚合节点\\(\\mathcal{M}_{B_{1,3}^T, B_{2,3}} \\colon \\mathcal{C}^1 \\times \\mathcal{C}^2 \\to \\mathcal{C}^3\\)来计算3-cochain \\(\\mathbf{H}_3=\\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\\mathbf{H}_1,\\mathbf{H}_2)\\)。 6.4 注意力高阶消息传递和CCANNs 在这里，我们展示了高阶消息传递（定义 6.1）与 CCANNs（章节5.5）之间的联系。我们将首先介绍一个注意力版本的定义 6.1。 定义 6.3 (CC上的注意力高阶消息传递(Attention higher-order message passing on a CC)) 令 \\(\\mathcal{X}\\)是CC，\\(\\mathcal{N}=\\{ \\mathcal{N}_1,\\ldots,\\mathcal{N}_n\\}\\)是定义在\\(\\mathcal{X}\\)上的邻域函数\\(\\mathcal{X}\\)，\\(x\\)是胞腔，并且对于某个\\(\\mathcal{N}_k \\in \\mathcal{N}\\)，存在\\(y\\in \\mathcal{N}_k(x)\\)。所谓胞腔\\(x\\)和\\(y\\)之间的消息\\(m_{x,y}\\)就是要完成某种计算，它依赖于两个胞腔或他们所支持的数据。用\\(\\mathcal{N}(x)\\)表示多集 \\(\\{\\!\\!\\{ \\mathcal{N}_1(x) , \\ldots , \\mathcal{N}_n (x) \\}\\!\\!\\}\\)，\\(\\mathbf{h}_x^{(l)}\\)表示层\\(l\\)的胞腔\\(x\\)支持的数据。\\(\\mathcal{N}\\)诱导的\\(\\mathcal{X}\\)上的注意力高阶消息传递可沟通过如下四条规则来定义： \\[\\begin{align} m_{x,y} &amp;= \\alpha_{\\mathcal{N}_k}(\\mathbf{h}_x^{(l)},\\mathbf{h}_y^{(l)}), \\tag{6.5} \\\\ m_{x}^k &amp;= \\bigoplus_{y \\in \\mathcal{N}_k(x)} a^k(x,y) m_{x,y}, \\; 1\\leq k \\leq n , \\tag{6.6} \\\\ m_{x} &amp;= \\bigotimes_{ \\mathcal{N}_k \\in \\mathcal{N} } b^k m_x^k , \\tag{6.7} \\\\ \\mathbf{h}_x^{(l+1)} &amp;= \\beta (\\mathbf{h}_x^{(l)}, m_x) . \\tag{6.8} \\end{align}\\] 其中, \\(a^k \\colon \\{x\\} \\times \\mathcal{N}_k(x)\\to [0,1]\\) 是高阶注意力函数(参见定义5.9), \\(b^k\\) 是满足\\(\\sum_{k=1}^n b^k=1\\)的可训练的注意力权重, \\(\\bigoplus\\) 是置换不变性聚合函数，\\(\\bigotimes\\) 是一般的聚合函数， \\(\\alpha_{\\mathcal{N}_k}\\) 和 \\(\\beta\\) 是可微函数。 定义6.3 区别了两类，第一种由函数 \\(a^k\\)确定，公式(6.6)的权重 \\(a^k(x,y)\\)依赖于邻域函数 \\(\\mathcal{N}_k\\)和胞腔 \\(x\\) and \\(y\\)。更进一步，\\(a^k(x,y)\\)确定了 确定了由邻域函数 \\(\\mathcal{N}_k\\) 决定的胞腔 \\(x\\) 对其周围邻域 \\(y\\in\\mathcal{N}_k\\) 的关注度。在章节5.5提到的CC-attention前推操作由这些权重来特定的参数化实现。另一方面，公式(6.7)的权重 \\(b^k\\) 仅是邻域\\(\\mathcal{N}_k\\)的函数， 并由此决定了胞腔 \\(x\\) 对从每个邻域函数 \\(\\mathcal{N}_k\\) 中获得的消息的关注程度。在章节5.5给出的CC-attention前推操作中，我们设置\\(b^k\\)等于1。然而，聚合节点的概念（参见定义 5.4）可以很容易地扩展为到引入相应的注意力聚合节点的概念，这反过来又可以用来在实践中实现公式 (6.7)。 请注意，由权重 \\(b^k\\) 决定的注意力是高阶领域所独有的，在基于图的注意力模型中不会出现。 参考文献 Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” arXiv Preprint arXiv:2104.13478. Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. “Neural Message Passing for Quantum Chemistry.” In International Conference on Machine Learning. Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Papillon, Mathilde, Sophia Sanborn, Mustafa Hajij, and Nina Miolane. 2023. “Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.” arXiv Preprint arXiv:2304.10031. \\(m_{x,y}\\)中的消息 “方向”是从\\(y\\)到\\(x\\)。一般来说，\\(m_{x,y}\\) 和 \\(m_{y,x}\\)是不相等的↩︎ 请注意，虽然我们对所有聚合节点都使用了相同的符号 \\(\\mathcal{M}_{Id_k,Id_k}\\)，但这些节点一般都有不同的参数。↩︎ "],["push-forward-pooling-and-unpooling.html", "第 7 章 前推、池化和反池化 7.1 CC池化和反池化 7.2 将常见的池化操作表述为 CC-pooling 7.3 池化与反池化CCNNs 7.4 映射器和CC池化操作", " 第 7 章 前推、池化和反池化 本节将展示如何利用定义5.3的前推操作来实现对CCc的（反）池化操作，随后接着介绍CCNNs的（反）池化操作。此外，本节还展示了基于 CC 的池化如何为基于图像和图的池化提供统一框架，以及 CC s上的保形池化(shape-preserving pooling)与图上的映射器是如何相关的。 特别是，我们建立了另一个统一的数学原理：作为消息传递的池化，可以从根本上由前推操作（push-forward）构建而成。因此，前推操作构成了主要的基本构件，所有高阶计算都可以从中实现。这种实现方式非常重要，因为它为在复形上建立统一的深度学习应用编程接口（API）奠定了数学基础，该接口将池化和基于消息传递的计算结合为单一操作。事实上，在我们贡献的 Python 软件包之一 TopoModelX中，高阶消息传递和池化/反池化操作就是作为一个单一函数在各种拓扑域中实现的。 7.1 CC池化和反池化 我们定义了一种基于 CC 的池化操作，它扩展了基于图像和基于图的池化操作的主要特点。 具体来说，我们建立了一种池化操作，可以 “下采样（downscales）”CC \\(\\mbox{X}\\) 上支持的信号大小。 To 为此，我们利用CCs自然的层次性，将池化操作定义为由共链映射 \\(G\\colon\\mathcal{C}^{i}(\\mathcal{X})\\to \\mathcal{C}^{j}(\\mathcal{X})\\) 引导的前推操作，该共链映射将\\(i\\)-cochain推向\\(j\\)-cochain。为了得到一个有用的池化操作，这需要缩减其输入 \\(i\\)-cochain 的大小，我们施加了 \\(j&gt;i\\) 的约束。定义 7.1实现了我们对 CC 池化的设想。图 7.1形象地展示了定义7.1背后的直觉。尤其是，图7.1展示了在三维 CC 上支持的共链上连续应用池操化作的例子。 定义 7.1 (CC池化操作，CC-pooling operation) 令 \\(\\mbox{X}\\)是一个CC，\\(G\\colon\\mathcal{C}^{i}( \\mbox{X})\\to \\mathcal{C}^{j}( \\mbox{X})\\)是共链映射，则如果 \\(j&gt;i\\)，那么\\(G\\)引导的前推操作被称为CC池化操作。 图 7.1: 连续CC（反）池化操作的示例. CC 池化操作利用了底层 CC 的分层结构，通过将低阶共链推向高阶胞腔来粗化低阶共链，从而提高对某些结构扭曲的不变性能力。图中粉色、蓝色和绿色胞腔分别有秩1、2、3，图（a）是维度为3的CC \\(\\mbox{X}\\)。在\\(\\mbox{X}\\)上，我们将考察三个共链操作: \\(B_{0,1} \\colon\\mathcal{C}^1\\to\\mathcal{C}^0\\), \\(B_{1,2} \\colon\\mathcal{C}^2\\to \\mathcal{C}^1\\) 和 \\(B_{2,3}\\colon\\mathcal{C}^3\\to\\mathcal{C}^2\\)。 对于图（b）、（c）、（d）中的顶行，假定首先给定0-cochain \\(\\mathbf{H}_0\\)；而对于这三幅图的底部行，则假定首先给定3-cochain\\(\\mathbf{H}_3\\)。例如，在图（b）的顶部行，输入0-cochain \\(\\mathbf{H}_0\\) 通过函数\\(\\mathcal{F}_{B_{0,1}^T}\\colon\\mathcal{C}^0\\to \\mathcal{C}^1\\)前推到1-cochain \\(\\mathbf{H}_1\\)。\\(B_{0,1}^T\\)诱导的前推操作就是CC-pooling操作，因为该操作发送0-cochain到了高阶共链。在图（b）的底部行，\\(B_{0,1}\\)诱导的前推操作 \\(\\mathcal{F}_{B_{0,1}}\\colon \\mathcal{C}^1\\to \\mathcal{C}^0\\)则是反池化操作，因为该操作发送 \\(1\\)-cochain 到了 \\(0\\)-cochain。图 (c) 和 (d)和图（b）类似，这些图就说明了\\(B_{1,2}^T\\)和\\(B_{2,3}^T\\) (顶部)诱导的池化操作, 以及\\(B_{1,2}\\) 和 \\(B_{2,3}\\)诱导的反池化操作。 在定义7.2中，我们引入了CCs上的反池化操作来将共链前推到低秩共链，图7.1给出了CCs上的反池化操作示例。 定义 7.2 (CC-unpooling操作，CC-unpooling operation) 令\\(\\mbox{X}\\)是CC，\\(G\\colon\\mathcal{C}^i( \\mbox{X})\\to \\mathcal{C}^j( \\mbox{X})\\)是共链映射。如果\\(j&lt;i\\)，那么称\\(G\\) 诱导的前推操为CC反池化操作（CC-unpooling operation）。 7.2 将常见的池化操作表述为 CC-pooling 在本节中，我们将用 CC 池化来表述常见的池化操作。特别是，我们将证明图形和图像池可以被视为 CC 池化。 7.2.1 用CC-pooling表示图池化操作 在此，我们将简要说明CC-pooing操作（参见定义7.1）和基于图的池化算法是一致的。令\\(\\mathbf{H}_0\\)定义在图\\(\\mathcal{G}\\)的顶点和边上的共链，\\(\\mathbf{H}^{\\prime}_0\\)是定义在\\(\\mathcal{G}\\)的粗粒度版本\\(\\mathcal{G}^{\\prime }\\)上的共链。在这种情况下，\\(\\mathbf{H}^{\\prime}_0\\)就是\\(\\mathbf{H}_0\\)的粗粒度版，元组 \\((\\mathcal{G},\\mathbf{H}_0)\\)上支持的图池化函数是\\(\\mathcal{POOL} \\colon (\\mathcal{G},\\mathbf{H}_0) \\to (\\mathcal{G}^{\\prime},\\mathbf{H}^{\\prime}_0)\\)，该函数发送\\(\\mathcal{G}\\)中的每个顶点到\\(\\mathcal{G}^{\\prime}\\)中的每一个顶点，这对应于\\(\\mathcal{G}\\)上顶点的聚类操作。接下来，将正式阐述怎么用CC-pooling来实现\\(\\mathcal{POOL}\\)函数。 命题 7.1 (CC-pooling的角色) 函数\\(\\mathcal{POOL}\\)可用CC-pooling操作实现。 证明. 图\\(\\mathcal{G}^{\\prime}\\)中的每个顶点都可表示原图\\(\\mathcal{G}\\)中的一个顶点聚类。 利用这些聚类的成员元素，我们能够通过一个 2-cells 胞腔集合来扩展 \\(\\mathcal{G}\\) 以构建 CC，这样得到的每个胞腔都对应于 \\(\\mathcal{G}^{\\prime}\\) 的一个超节点。我们用 \\(\\mathcal{X}_{\\mathcal{G}}\\) 来表示由此产生的 CC 结构，它由 2-cells 胞腔增强的 \\(\\mathcal{G}\\) 组成。因此，任何定义在\\(\\mathcal{G}^{\\prime}\\)上的0-cochain \\(\\mathbf{H}^{\\prime}_0\\)都能写作2-cochain \\(\\mathbf{H}_2 \\in \\mathcal{C}^2(\\mathcal{X}_{\\mathcal{G}})\\)。在原图 \\(\\mathcal{G}\\)的顶点和池化图\\(\\mathcal{G}^{\\prime}\\)的顶点，或其等价描述形式CC \\(\\mathcal{X}_{\\mathcal{G}}\\)的顶点间的关系，都可以通过关联矩阵\\(B_{0,2}^T\\)来描述。因此，为了学习信号\\(\\mathbf{H}_2\\)可以用映射\\(B_{0,2}^T \\colon \\mathcal{C}^{2} (\\mathcal{X}_{\\mathcal{G}}) \\to \\mathcal{C}^{0}(\\mathcal{X}_{\\mathcal{G}})\\)来实现，该操作把共链\\(\\mathbf{H}_0\\) 推向 \\(\\mathbf{H}_2\\)。 定义\\(\\mathcal{X}_{\\mathcal{G}}\\)中的2-cells可以用 图映射（mapper on graphs）工具 (Hajij, Wang, and Rosen 2018)来实现, 它是一个TDA分类工，更详细的构造方法参见章节7.4。 7.2.2 图像池化作为CC-pooing 由于图像可以实现为网格图(lattice graphs)，因此存储在图像网格(image grid)上的信号可以实现为与图像相对应的网格图的 0-cochain，参见图7.2(a–b)。在此，我们将阐明 CC-pooling（定义7.1）与已知的图像池化的定义是一致的。事实上，可以用2-cells来增强7.2(b)中的网格图，正如图 7.2(c)所示，以便执行图像池化操作。通常，这些胞腔都有规则的窗口大小。在图7.2(c)中，我们选择的池化窗口大小，或者说是 2-cell的大小，是 \\(2\\times 2\\)，池化步长为 1。 这种情况下的图像池化操作可以通过共链映射\\(B_{0,2}^T \\colon\\mathcal{C}^0 \\to \\mathcal{C}^2\\)诱导的CC-pooling来实现，如图 7.2(d).所示。我们在下面的命题中正式正式阐述这一点。 图 7.2: 用CC-pooling实现图像池化。 (a): 尺寸为\\(3\\times3\\)的图像。 (b): 与图（a）对应的网格图。(c): 2-cells增强的网格图。选择 (c) 中的那些特定胞腔，相当于选择图像池化窗口大小为\\(2\\times 2\\) ，池化步长为 1。 (d): 执行图像池化计算等价于执行由共链映射\\(B_{0,2}^T \\colon\\mathcal{C}^0 \\to \\mathcal{C}^2\\)诱导的CC-pooling操作，该共链映射前推图像信号（\\(\\mathcal{X}^2\\)支持的\\(0\\)-cochain）到\\(\\mathcal{X}^2\\)支持的信号。 命题 7.2 (图像池化的实现) 图像池化操作可以通过从底层图像域到二维 CC 的前推操作来实现，该二维 CC是通过适当的二维胞腔对图像进行增强而得到的，图像池化计算就在该二维胞腔中进行。 证明. 从图像池化的定义中可以直接得出结论。 7.3 池化与反池化CCNNs 定义7.1中的池化操作仅考虑了CCNN的张量图仅有单一边的特殊情况，在下文中，我们将通过确定 CCNN 作为池化 CCNN 的定义属性来概括池化概念。为此，我们从张量图高度为 1 的 CCNN 开始讲解。 定义 7.3 (高度为1的CCNN池化，Pooling CCNN of height one) 对于高度为1的张量图\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)表示的CCNN，令\\(\\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m}\\)是CCNN的域，令\\(\\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n}\\)是CCNN的共域（codomain），令\\(i_{min}=min(i_1,\\ldots,i_m)\\)和\\(j_{min}=min(j_1,\\ldots,j_n)\\)。如果满足如下条件，则说CCNN是高度为1的池化CCNN: \\(i_{min}&lt; j_{min}\\), 且 张量图\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\) 对于某个\\(k\\geq j_{min}\\)，有共链操作\\(G\\colon \\mathcal{C}^{i_{min}} \\to \\mathcal{C}^{k}\\)标记的边。 直观地说，如果一个高度为1的张量图所代表的 CCNN 将其最低秩的信号推送给较高等级的胞腔，那么它就是高度为1的池化 CCNN。请注意，读出操作可以作为高度为1的池化 CCNN 来实现，见图 7.3中的说明。 图 7.3: 池化操作示例。在这个例子中，展示了一个维度为3的CC。秩为 3 的胞腔（绿色表示）在 CC中的所有胞腔中秩最高。(a): 高度为 1 的池化 CCNN 会将一个共链向量\\((\\mathbf{H}_0,\\mathbf{H}_1)\\) 池化为一个 2-cochain \\(\\mathbf{H}_2\\) 。 (b):读出操作可以作为高度为 1 的池化 CCNN来实现，方法是由一个秩数高于CC中所有其他胞腔的单个（绿色）胞腔封装整个CC，并将低秩胞腔的所有信号池化（读出）到封装（绿色）胞腔。 CCNN 可能不会在每一层都执行池化操作，它可能会保持最低秩信号的维度。在给出池化 CCNN 的一般性定义之前，我们首先定义高度为1的最低秩保持 （rank-preserving ）CCNN。 定义 7.4 (高度为1的最低秩保持CCNN，Lowest rank-preserving CCNN of height one) 对于由高度为1的张量图表示的CCNN，令 \\(\\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m}\\)是CCNN的域，令 \\(\\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n}\\) 是CCNN的共域，令 \\(i_{min}=min(i_1,\\ldots,i_m)\\) 且\\(j_{min}=min(j_1,\\ldots,i_n)\\)。如果 \\(i_{min}= j_{min}\\)，那么就说CCNN高度为1的最低秩保持CCNN。 每个 CCNN 都是由高度为1的张量图表示的 CCNN 组成的。因此，正如定义 7.5所阐述的，池化 CCNN 可以用高度为1的张量图来表征。 定义 7.5 (池化CCNN，Pooling CCNN) 令 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)是CCNN的张量图表示， 则可将CCNN分解作 \\[\\begin{equation*} \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}= \\mbox{CCNN}_{\\mathbf{G}_N;\\mathbf{W}_N} \\circ \\cdots \\circ \\mbox{CCNN}_{\\mathbf{G}_1;\\mathbf{W}_1}, \\end{equation*}\\] 其中， \\(\\mbox{CCNN}_{\\mathbf{G}_i;\\mathbf{W}_i},i=1,\\ldots,N\\), 是高度为1的张量图，表示了CCNN的第\\(i\\)-th层，并且 \\(\\mathbf{G}_i \\subseteq \\mathbf{G}\\)。称 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)表示的CCNN是池化 CCNN，如果： 每个 \\(\\mbox{CCNN}_{\\mathbf{G}_i;\\mathbf{W}_i}\\) 都是高度为1的池化CCNN，或者是高度为1的最低秩保持CCNN，且 在CCNN的层 \\(\\mbox{CCNN}_{\\mathbf{G}_i;\\mathbf{W}_i}\\)中至少有一个是高度为1的池化CCNN。 \\end{enumerate} 直观地说，池化CCNN 是一种 CCNN，其张量图形成了一个 “阶梯”，将信号推送到每一层的高秩胞腔。图 5.4(d) 给出了一个高度为二的池化 CCNN 的例子。 高度为1的反池化CCNN的定义类似高度为1的池化CCNN(参见定义7.3)，唯一的不同是不等式\\(i_{min}&lt;j_{min}\\) 变成了 \\(i_{min}&gt;j_{min}\\)。除此之外，反池化 CCNN (参见定义7.6) 的定义都类似于池化CCNN (定义7.5)。 定义 7.6 (反池化CCNN，Unpooling CCNN) 令 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)是CCNN的张量图表示，那么可解耦CCNN为 \\[\\begin{equation*} \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}= \\mbox{CCNN}_{\\mathbf{G}_N;\\mathbf{W}_N} \\circ \\cdots \\circ \\mbox{CCNN}_{\\mathbf{G}_1;\\mathbf{W}_1}, \\end{equation*}\\] 其中，\\(\\mbox{CCNN}_{\\mathbf{G}_i;\\mathbf{W}_i},i=1,\\ldots,N\\), 是表征CCNN第\\(i\\)-th层的高度为1的张量图，并且 \\(\\mathbf{G}_i \\subseteq \\mathbf{G}\\)，则称\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)表征的CCNN是反池化CCNN，如果： 每个\\(\\mbox{CCNN}_{\\mathbf{G}_i;\\mathbf{W}_i}\\) 都要么是高度为1的反池化CCNN，要么是高度为1的最低秩保持CCNN，且 在CCNN的层 \\(\\mbox{CCNN}_{\\mathbf{G}_i;\\mathbf{W}_i}\\)中至少有一个是高度为1的反池化CCNN。 7.4 映射器和CC池化操作 在实际使用时，在高阶域上构建有用的 CC 池化操作取决于输入 CC 中的高秩胞腔。 与基于图像的模型类似，CC-池化操作可在高阶网络的末端依次应用，以提供输入域的汇总表示，参见图7.1。 输入的 CC 中可能并没有这样的分层摘要，例如, 如果 \\(\\mathcal{X}\\)是图，那么一个CC-pooling操作，正如定义7.1中给出的那样, 可能仅仅是前推输入的节点信号到边信号，而并不总是能提供输入信号的简明摘要。 在这种情况下，我们可以用一组维度为 \\(\\dim(\\mathcal{X})+1\\) 的新胞腔来增强输入 CC \\(\\mathcal{X}\\) ，以使得新胞腔近似于输入 CC \\(\\mathcal{X}\\) 的形状。图7.4 给出了一个增强图 \\(\\mathcal{X}\\)的示例，它是一个维度为1的CC，其新胞腔的维度为2，新胞腔根据文献(Dey, Mémoli, and Wang 2016; Singh et al. 2007; Hajij, Wang, and Rosen 2018)6中的方法使用图mapper(mapper on graph, MOG)来构造。通过 MOG 构造方法得到的增强高秩胞腔概要了底层图的形状特征，这是一种理想的池化特征（例如在形状分析中）。关于 MOG 构建的拓扑保持（ topology-preserving） CC 池化操作的详情，请参阅附录 E 。 图 7.4: 构造CC\\(\\mathcal{X}\\)上形状保持（shape-preserving）的池化操作示例。在此，我们将演示 \\(\\mathcal{X}\\) 是图的情况。构建方法使用图mapper（mapper on graphs，MOG）(Singh et al. 2007; Hajij, Wang, and Rosen 2018)，它是一种图骨架化算法，可用于为 \\(\\mathcal{X}\\) 增添秩为 2 的拓扑保持胞腔（topology-preserving）。（a）输入图\\(\\mathcal{X}\\)是维度为1的CC。（b）MOG算法的输入需要三个参数：图\\(\\mathcal{X}\\)，特征保持标量函数\\(g\\colon\\mathcal{X}^0\\to [a,b]\\)，范围\\([a,b]\\)的覆盖（cover）\\(\\mathcal{U}\\)（覆盖闭区间\\([a,b]\\)的开集的集合）。标量函数\\(g\\)用于将范围 \\([a,b]\\) 上的覆盖 \\(\\mathcal{U}\\) 拉回到 \\(\\mathcal{X}\\) 上的覆盖, 图 (b) 中节点的颜色表示 \\(g\\) 的标量值。图（b）中，\\(\\mathcal{X}\\)被分裂为四段，每段都对应于\\(\\mathcal{U}\\)的一个覆盖元素。（c）图中显示了拉回的覆盖元素的连接组件，每个相连的分量都被一个蓝色胞腔所包围，每个蓝色胞腔也都被视为秩为 2 的单元。蓝色胞腔用来增强\\(\\mathcal{X}\\)，以形成维度2的CC，增强后的\\(\\mathcal{X}\\)由原\\(\\mathcal{X}\\)上的0-cells、1-cells，以及增强的2-cells来组成实现。增强的CC用\\(\\mathcal{X}_{g,\\mathcal{U}}\\)来标记。(d): MOG 算法构建了一个图，其节点是覆盖 \\(\\mathcal{U}\\)的回拉过程中每个覆盖元素所包含的连通成分，边由这些连通成分组成。换句话说，MOG 生成的图概要了通过 MOG 算法添加的秩为 2的增强胞腔之间的连接情况。图（d）中给出的MOG生成图的邻接矩阵等价于\\(\\mathcal{X}_{g,\\mathcal{U}}\\)的胞腔\\(A_{2,2}\\)的邻接矩阵。当且仅当 \\(\\mathcal{X}\\) 中存在的 2-cells胞腔相交于一个节点时（后者发生的前提是当且仅当 MOG生成的图的节点之间存在一条边时），它们才是 2 相邻（adjacent）的 。给定CC结构，共链映射\\(B_{0,2}^T\\colon\\mathcal{C}^0(X_{g,\\mathcal{U}})\\to \\mathcal{C}^2(X_{g,\\mathcal{U}})\\)可用于诱导形状保持的操作。此外，\\(\\mathcal{X}\\)顶点集上支持的信号 \\(\\mathbf{H}_0\\)可以被前推和池化为增强2-cells上支持的信号\\(\\mathbf{H}_2\\)。这里给出的图受了文献 (Hajij, Wang, and Rosen 2018)的启发。 参考文献 Dey, Tamal K., Facundo Mémoli, and Yusu Wang. 2016. “Multiscale Mapper: Topological Summarization via Codomain Covers.” In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, 997–1013. SIAM. Hajij, Mustafa, Bei Wang, and Paul Rosen. 2018. “MOG: Mapper on Graphs for Relationship Preserving Clustering.” arXiv Preprint arXiv:1804.11242. Singh, Gurjeet, Facundo Mémoli, Gunnar E Carlsson, et al. 2007. “Topological Methods for the Analysis of High Dimensional Data Sets and 3d Object Recognition.” PBG@ Eurographics 2: 091–100. 使用mapper算法来图骨架化（graph skeletonization）的方法(Singh et al. 2007)已经在文献(Dey, Mémoli, and Wang 2016)中研究过，我们的实现和讨论依赖于文献(Hajij, Wang, and Rosen 2018)中提到的概念。↩︎ "],["hasse-graph-interpretation-of-ccnns-1.html", "第 8 章 CCNNs的Hasse图解释 8.1 CCNNs的Hasse图解释 8.2 CCNNs的等变性", " 第 8 章 CCNNs的Hasse图解释 在本节中，我们将通过与已有的 GNN研究成果建立联系，仔细研究拓扑学习机的特性。首先将CCs解释为专门的图，例如哈斯图（Hasse graphs），然后针对置换和方向的作用描述它们的等变特性，还将进一步将等变性定义与Hasse图表示法下的传统等变定义联系起来。 译者注：Hasse graphs，用来表示有限偏序集的一种数学图表，它是一种图形形式的对偏序集的传递简约。具体的说，对于偏序集合\\((S, ≤)\\)，把\\(S\\)的每个元素表示为平面上的顶点，并绘制从\\(x\\)到\\(y\\)向上的线段或弧线，只要\\(y\\) 覆盖\\(x\\)(就是说，只要\\(x &lt; y\\)并且没有\\(z\\)使得\\(x &lt; z &lt; y\\))。这些弧线可以相互交叉但不能触及任何非其端点的顶点。带有标注的顶点的这种图唯一确定这个集合的偏序。 8.1 CCNNs的Hasse图解释 我们将首先阐明，每个 CC 都可以归约为一个唯一且特定的图，即 Hasse 图。通过这种归约，就可以用基于图的模型来分析和理解 CCNN 的各种计算和概念。 8.1.1 CCs作为Hasse图 定义 4.2表明CC是偏序的（poset）, 是一个部分有序集合，其部分有序关系是集合包含关系。 这也意味着，当且仅当两个 CC 的 偏序关系 等价时，它们才是等价的7。 定义8.1 给CC引入了Hasse图 (Wachs 2006; Abramenko and Brown 2008)，它是和有限偏序关系关联的有向图。 定义 8.1 (Hasse 图) CC \\((S, \\mathcal{X},\\mbox{rk})\\) 的Hasse图是有向图 \\(\\mathcal{H}_{\\mathcal{X}}= (V (\\mathcal{H}_{\\mathcal{X}}), E(\\mathcal{H}_{\\mathcal{X}}) )\\)，顶点集为\\(V (\\mathcal{H}_{\\mathcal{X}})=\\mathcal{X}\\)，边集为 \\(E(\\mathcal{H}_{\\mathcal{X}})=\\{ (x,y) : x\\subsetneq y, \\mbox{rk}(x)=\\mbox{rk}(y)-1 \\}\\)。 CC \\((S, \\mathcal{X},\\mbox{rk})\\)的Hasse图 \\(\\mathcal{H}_{\\mathcal{X}}\\)的顶点是\\(\\mathcal{X}\\)中的胞腔，\\(\\mathcal{H}_{\\mathcal{X}}\\)的边由胞腔间的直接互连关系（immediate incidence）确定，图 8.1 提供了一个CC的Hasse图示例。 图 8.1: CC的Hasse图示例。 (a): 莫比乌斯环（\\(M\\ddot{o}bius\\) strip）的CC。 (b): CC的Hasse图，描述了胞腔之间的偏序结构。 (c): 带有边\\(A_{0,1}\\) and \\(coA_{2,1}\\)的增强Hasse图 CC 结构类（CC structure class）是根据定义 4.2确定的同构 CC 的集合，命题8.1为确定 CC 结构类提供了充分的标准，命题8.1的证明依赖于对由底层Hasse图表示确定的 CC 结构类的观察，Hasse图提供了与关联矩阵\\(\\{B_{k,k+1}\\}_{k=0}^{\\dim(\\mathcal{X})-1}\\)同样的信息。图8.2给出了命题8.1的证明的第二、三部分的可视化。 命题 8.1 (CC 结构的确定 ) 令 \\((S, \\mathcal{X},\\mbox{rk})\\)是CC，对于\\((S, \\mathcal{X},\\mbox{rk})\\)表示的CC结构类，需满足如下充分条件: CC结构类由关联矩阵\\(\\{B_{k,k+1}\\}_{k=0}^{ \\dim(\\mathcal{X}) -1}\\)确定。 CC结构类由邻接矩阵 \\(\\{A_{k,1}\\}_{k=0}^{\\dim(\\mathcal{X})-1}\\)确定。 CC结构类由共邻矩阵 \\(\\{coA_{k,1}\\}_{k=1}^{\\dim(\\mathcal{X})}\\)确定。 证明. 如果注意到 CC 的结构完全由其 Hasse 图表示法决定，那么命题三部分的证明就水到渠成了。 命题的第一部分源于这样一个事实：Hasse图中的边正是矩阵\\(\\{B_{k,k+1}\\}_{k=0}^{\\dim(\\mathcal{X}-1)}\\)的非零项。 第二部分是观察到，当且仅当存在一个\\(k\\)-cell \\(z^k\\)时（关联 \\(x^{k-1}\\) 和 \\(y^{k-1}\\)），\\((k-1)\\) cells \\(x^{k-1}\\) 和 \\(y^{k-1}\\) 是 1-adjacent。 第三步是注意到，当且仅当存在一个\\(k\\)-cell \\(z^k\\)（关联\\(x^{k+1}\\) 和 \\(y^{k+1}\\)）时， \\((k+1)\\)-cells \\(x^{k+1}\\) 和 \\(y^{k+1}\\) 是 1-coadjacent。 图 8.2: 在CC的Hasse图上，直接关联（immediate incidence）和（共）邻接之间的关系 。 (a): 当且仅当存在一个\\(k\\)-cell \\(z^k\\) (粉色顶点)时（关联\\(x^{k-1}\\) 和 \\(y^{k-1}\\)），\\((k-1)\\) cells \\(x^{k-1}\\) 和 \\(y^{k-1}\\) (橙色顶点) 是1-adjacent。 (b): 当且仅当存在一个\\(k\\)-cell \\(z^k\\) (粉色顶点)时（关联\\(x^{k+1}\\) 和 \\(y^{k+1}\\)），\\((k+1)\\) cells \\(x^{k+1}\\) 和 \\(y^{k+1}\\) (蓝色顶点) 是 1-coadjacent 8.1.2 增强的Hasse图 CC 的 Hasse 图非常有用，因为它表明高阶深度学习模型的计算可以归约为基于图的模型的计算。尤其，在 CC \\(\\mathcal{X}\\) 上处理的 \\(k\\)-cochain（信号）可视为关联的Hasse图 \\(\\mathcal{H}_{mathcal{X}}\\) 上对应顶点的信号。 由矩阵 \\(B_{k,k+1}\\) 指定的边决定了定义在 \\(\\mathcal{X}\\) 上的给定高阶模型的消息传递结构。然而，通过矩阵 \\(A_{r,k}\\) 确定的消息传递结构并不直接支持 \\(\\mathcal{H}_{mathcal{X}}\\) 的相应边。 因此，除了CC的poset偏序关系所指定的边之外，有时还需要用额外的边来增强Hasse图。按照这种思路，定义8.2引入了增强Hasse图（augmented Hasse graph）的概念。 定义 8.2 (增强Hasse图，Augmented Hasse graph) 令 \\(\\mathcal{X}\\) 是CC，\\(\\mathcal{H}_{\\mathcal{X}}\\) 是它的Hasse图，顶点集为 \\(V(\\mathcal{H}_{\\mathcal{X}})\\)，边集为 \\(E(\\mathcal{H}_{\\mathcal{X}})\\)；\\(\\mathcal{N}=\\{\\mathcal{N}_1,\\ldots,\\mathcal{N}_n\\}\\) 是定义在\\(\\mathcal{X}\\)上的邻域函数。如果存在\\(\\mathcal{N}_i \\in \\mathcal{N}\\)，使得\\(x \\in \\mathcal{N}_i(y)\\) 或 \\(y \\in \\mathcal{N}_i(x)\\)成立，那么称\\(\\mathcal{H}_{\\mathcal{X}}\\)有\\(\\mathcal{N}\\)诱导的增强边\\(e_{x,y}\\)。记\\(\\mathcal{N}\\)诱导的所有增强边集为\\(E_{\\mathcal{N}}\\)。\\(\\mathcal{N}\\)诱导的\\(\\mathcal{X}\\)的增强Hasse图定义了图\\(\\mathcal{H}_{\\mathcal{X}}(\\mathcal{N})= (V(\\mathcal{H}_{\\mathcal{X}}), E(\\mathcal{H}_{\\mathcal{X}}) \\cup E_{\\mathcal{N}})\\). 定义中的增强Hasse图可以更容易地理解为矩阵： 如果将\\(\\mathbf{G}=\\{G_1,\\ldots,G_n\\}\\) 和邻域函数 \\(\\mathcal{N}=\\{\\mathcal{N}_1,\\ldots,\\mathcal{N}_n\\}\\)联系起来，那么\\(\\mathcal{H}_{\\mathcal{X}}(\\mathcal{N})\\) 中的每条增强边将都对对应于某个\\(G_i\\in \\mathbf{G}\\)中的非零项。由于\\(\\mathcal{N}\\) 和 \\(\\mathbf{G}\\) 存储了同样的信息，可以用 \\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\) 来标记由\\(\\mathbf{G}\\)确定的边所诱导的增强 Hasse图。 例如， 图8.1(c)中给出的图可用 \\(\\mathcal{H}_{\\mathcal{X}}( A_{0,1},coA_{2,1})\\)标记。 8.1.3 CCNN对图模型的归约能力 在本节中，我们将展示任何基于 CCNN 的计算模型都可以通过底层 CC 的增强Hasse图的子图上的消息传递方案来实现。每个 CCNN 都是通过计算张量图确定的，而计算张量图可以使用基本的张量运算，即推前运算、聚合节点和分裂节点来构建。因此，通过证明这三种张量运算可以在增强的Hasse图上执行，就可以实现基于CCNN的计算对图上消息传递方案的归约能力。命题 8.2就表述了前推操作可以在增强的Hasse图上执行。 命题 8.2 (增强Hasse图上的计算，Computation over augmented Hasse graph) 令\\(\\mathcal{X}\\)是CC，\\(\\mathcal{F}_G \\colon \\mathcal{C}^i(\\mathcal{X})\\to \\mathcal{C}^j(\\mathcal{X})\\)是共链映射\\(G\\colon\\mathcal{C}^i(\\mathcal{X})\\to \\mathcal{C}^j(\\mathcal{X})\\)诱导的前推操作。任何通过\\(\\mathcal{F}_G\\)执行的计算都能归约为\\(\\mathcal{X}\\)的增强Hasse图\\(\\mathcal{H}_{\\mathcal{X}}(G)\\) 上的相应计算。 证明. 令 \\(\\mathcal{X}\\)是CC，\\(\\mathcal{H}_{\\mathcal{X}}(G)\\)是\\(G\\)确定的\\(\\mathcal{X}\\)的增强Hasse图。增强Hasse图的定义表明，在顶点\\(\\mathcal{H}_{\\mathcal{X}}(G)\\) 和\\(\\mathcal{X}\\)的胞腔之间有一对一的对应关系。 给定胞腔\\(x\\in \\mathcal{X}\\)，令\\(x^{\\prime}\\)是\\(\\mathcal{H}_{\\mathcal{X}}(G)\\)中的顶点，\\(y\\)是\\(\\mathcal{X}\\)中带有特征向量\\(\\mathbf{h}_y\\)的胞腔，该特征向量是由公式(5.2)指定的前推操作计算得到。回想一下，\\(\\mathbf{h}_y\\) 向量是通过聚合\\(x \\in \\mathcal{X}^i\\)中与\\(y\\)相邻的所有向量\\(\\mathbf{h}_x\\)计算出来的，这与邻域函数\\(\\mathcal{N}_{G^T}\\)有关。令\\(m_{x,y}\\)表示\\(\\mathcal{X}\\)中两个胞腔\\(x\\) 和\\(y\\)之间的计算（即：消息），该计算是前推操作\\(\\mathcal{F}_G\\)计算的一部分。根据增强 Hasse 图的定义，胞腔 \\(x\\) 和 \\(y\\) 在矩阵 \\(G\\) 中必须有相应的非零项。此外，这些非零项对应于\\(\\mathcal{H}_{\\mathcal{X}}(G)\\)中\\(x^{\\prime}\\) 和 \\(y^{\\prime}\\)之间的一条边。因此， \\(\\mathcal{X}\\)的胞腔 \\(x\\) 和 \\(y\\)之间的计算\\(m_{x,y}\\)可以看作\\(\\mathcal{H}_{\\mathcal{X}}(G)\\)的对应顶点\\(x^{\\prime}\\) 和 \\(y^{\\prime}\\)之间的计 （消息）\\(m_{x^{\\prime},y^{\\prime}}\\)。 同样，对任意聚合节点的计算都可以用对底层 CC 的增强 Hasse 图的子图的计算来描述。命题 8.3正式表述了这一说法。 命题 8.3 (聚合顶点到增强Hasse图的归约，Reduction of merge node to augmented Hasse graph) 正如公式(5.1)中那样，通过聚合顶点\\(\\mathcal{M}_{\\mathbf{G},\\mathbf{W}}\\)执行的任何计算都可以归约到底层CC的增强Hasse图\\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\)上的相应计算。 证明. 令 \\(\\mathcal{X}\\)是CC，\\(\\mathbf{G}=\\{ G_1,\\ldots,G_n\\}\\)是定义在\\(\\mathcal{X}\\)上的共链操作序列， \\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\) 是\\(\\mathbf{G}\\)确定的增强Hasse图。按照增强Hasse图的定义，\\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\)的顶点和\\(\\mathcal{X}\\)的胞腔之间有一对一的关系。对\\(x\\in \\mathcal{X}\\)中的每个胞腔，令\\(x^{\\prime}\\) 是\\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\)中的对应顶点，令 \\(m_{x,y}\\) 是 在\\(\\mathcal{X}\\)的两个胞腔\\(x\\) 和 \\(y\\)上执行的计算（消息），该计算是函数\\(\\mathcal{M}_{\\mathbf{G},W}\\)计算的一部分。因此，两个胞腔 \\(x\\) 和\\(y\\)必须在矩阵\\(G_i\\in\\mathbf{G}\\)上有相应的一对一非零项。根据增强Hasse图的定义，非零项对应于 \\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\)中 \\(x^{\\prime}\\) 和 \\(y^{\\prime}\\)之间的一条边。因此，在\\(\\mathcal{X}\\)的两个胞腔\\(x\\) 和 \\(y\\)之间执行计算 \\(m_{x,y}\\)就可以看作是在\\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\)的顶点\\(x^{\\prime}\\) 和 \\(y^{\\prime}\\)之间执行计算（消息）\\(m_{x^{\\prime},y^{\\prime}}\\)。 命题 8.2 和 8.3 确保了前推和聚合节点计算可以在增强Hasse图上实现。定理 8.1 泛化了命题 8.2 和8.3，表明张量图上的任何计算都可在增强哈塞图上实现. 定理 8.1 (张量图可归约到增强Hasse图，Reduction of tensor diagram to augmented Hasse graph) 通过张量图\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)执行的任何计算都可以归约到增强Hasse图\\(\\mathcal{H}_{\\mathcal{X}}(\\mathbf{G})\\)。 证明. 结论直接来自命题 8.3和 8.2，以及任何张量图都可以用三个基本张量运算来实现这一事实。 根据定理 8.1，张量图和它相应的增强Hasse图以两种可选的方式编码了同样的计算，图 8.3 演示了Hasse图提供了关联的CCNN的张量图表示的计算摘要。 图 8.3: 两个CCNNs的张量图以及他们相应的Hasse图。 为了避免杂乱，张量图中去掉了边标签，因为它们可以从相应的增强 Hasse 图中推断出来。(a): 从高阶信息传递方案中获得的张量图。 (b): 使用三种基本张量运算得到的张量图。 8.1.4 增强Hasse图和CC-pooling Hasse 图及其增强版本是底层 CC 的偏序结构的图表示。 针对这些图解释（反）池化操作（定义 7.1 和 7.2）是很有启发性的。定义 7.1中的 CC 池化操作将 偏序结构中的信号从低秩胞腔映射到高秩胞腔。另一方面，定义 7.2中的 CC-unpooling 操作将信号映射到相反的方向。图（fig:hasse-graph-pooling）展示了一个在底层 CC 的增强 Hasse 图上可视化的 执行CC（反）池化操作的示例。 图 8.4: CC增强Hasse图视角下的CC池化于反池化操作。 图中的顶点代表底层 CC 中的骨架。黑色边代表这些顶点之间 Hasse 图中的边，而红色边代表从增强 Hasse 图结构中获得的边。CC-pooling对应于将偏序集合结构中的信号从低秩顶点推向高秩顶点，而 CC-unpooling对应于将偏序集合结构中的信号从高秩顶点推向低秩顶点。 8.1.5 增强Hasse图消息传递和聚合节点 有两种方式可用来构造CCNN，一是用章节6.1给出的高阶消息传递图来 构造，二是使用章节 5.3 给出的三个基本张量操作来构造，这在章节6.3中已经阐明。尤其，章节 6.3中 提到，聚合节点 与高阶消息传递范式相比，聚合节点自然可以实现更灵活的计算框架。 这种灵活性体现在底层张量图以及所考虑的网络输入方面。如图 8.3所示，张量运算和高阶消息传递之间的区别也可以通过增强的 Hasse 图来突出显示。图 8.3(a)给出了从CCNN的高阶消息传递方案中获得的张量图。 我们观察到这种 CCNN 的两个关键特性：初始输入的共链在域的所有维度的所有胞腔上都能支持，而且 CCNN 在每次迭代时都会按照预定的邻域函数来更新域的所有维度的所有胞腔上支持的所有共链。因此，相应的增强 Hasse 图呈现出统一的拓扑结构。相比之下，图 8.3(b) 显示的是使用三种基本张量运算构建的张量图。由于高阶信息传递规则不施加限制，因此得到的增强 Hasse 图呈现出更灵活的结构。 8.1.6 高阶表征学习 定理8.1给出的增强Hasse图和CCs关系表明，许多基于的深度学习也有类似的CCs构造方法。在本节，我们将阐明高阶表征学习（higher-order representation learning）如何归约到图表示学习 (Hamilton, Ying, and Leskovec 2017)，将某些 CC 计算应用转为增强哈塞图计算。 图形表示法(graph representation)的目标是学习一种映射，将图形的顶点、边或子图嵌入欧几里得空间，使由此产生的嵌入能捕捉到图形的有用信息。类似的，高阶表征学习 (Hajij, Istvan, and Zamzmi 2020) 是学习一种嵌入，在保留拓扑域的主要结构属性的前提下，将给定拓扑域的胞腔嵌入到欧氏空间。更确切的说，对于给定复形\\(\\mathcal{X}\\)，高阶表征学习是要学习一对函数\\((enc, dec)\\)，其中，\\(enc\\)是编码器映射（encoder map） \\(enc \\colon \\mathcal{X}^k \\to \\mathbb{R}^d\\)，\\(dec\\)是解码器映射（decoder map） \\(dec \\colon \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\)。编码器函数会为 \\(\\mathcal{X}\\) 中的每个 \\(k\\)-cell \\(x^k\\) 关联一个特征向量 \\(enc(x^k)\\)，该特征向量会根据 \\(\\mathcal{X}\\) 中其他胞腔的结构对 \\(x^k\\) 的结构进行编码。另一方面，解码器函数给 另一方面，解码器函数为每一对胞腔嵌入关联了一个相似度量，它量化了相应胞腔之间的某种关系概念。我们使用特定上下文环境的相似性度量 \\(sim \\colon \\mathcal{X}^k \\times \\mathcal{X}^k \\to \\mathbb{R}\\) 和目标函数来优化可训练函数 \\((enc,dec)\\)，目标函数形式如下： \\[\\begin{equation} \\mathcal{L}_k=\\sum_{ x^k \\in \\mathcal{X}^k } l( dec( enc(x^{k}), enc(y^{k})),sim(x^{k},y^k)), \\tag{8.1} \\end{equation}\\] 其中， \\(l \\colon \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\) 是损失函数。高阶表征学习和图表征学习的确切关系在命题8.4中给出 命题 8.4 (高阶表征学习可作为图表征学习，Higher-order representation learning as graph representation learning) 高阶表征学习可以归约到图表征学习。 证明. 令 \\(sim\\colon \\mathcal{X}^k \\times \\mathcal{X}^k \\to \\mathbb{R}\\) 是相似性度量， 图\\(\\mathcal{G}_{\\mathcal{X}^k}\\)定义为：顶点集对应于\\(\\mathcal{X}^k\\)中的胞腔，边对应于\\(\\mathcal{X}^k \\times \\mathcal{X}^k\\)中通过函数\\(sim\\)映射成非零值的胞腔对。因此，一对 \\((enc, dec)\\) 就对应一对\\((enc_{\\mathcal{G}}, dec_{\\mathcal{G}})\\)，各函数的形式分别为\\(enc_{\\mathcal{G}}\\colon \\mathcal{G}_{\\mathcal{X}^k} \\to \\mathbb{R}\\) 和 \\(dec_{\\mathcal{G}}\\colon \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\)。所以，学习一对\\((enc, dec)\\)可以归约为学习一对 \\((enc_{\\mathcal{G}}, dec_{\\mathcal{G}})\\)。 TopoEmbedX, 是我们贡献的三个软件包中的一个，支持胞腔复形、单纯复形、CCs上的高阶表征学习，其主要计算原则就是命题8.4。 需要特别说的是, TopoEmbedX 首先把高阶域转换为相应增强Hasse图的子图，然后利用已有的图表示学习算法来计算子图的元素嵌入。 鉴于增强 Hasse 图的元素与原始高阶域之间的对应关系，这样就可以获得高阶域的嵌入。 备注. 根据我们对 Hasse 图的讨论，特别是将 CCNN 上的计算转换为（Hasse）图上的计算的能力，有人可能会说，GNNs 已经足够，不需要 CCNN 了。然而，这是一个误导性线索，因为任何计算都可以用计算图来表示。在 CC 的增强 Hasse 图上应用标准 GNN 并不等同于应用 CCNN。这一点将在第 8.2节中变得更加清晰，在这一节中，我们将介绍CCNN的等变性（equivariances）。. 8.2 CCNNs的等变性 与图类似，高阶深度学习模型，尤其是 CCNNs，应始终与其底层等变性 (Bronstein et al. 2021) 结合起来考虑。现在，我们为 CCNNs 的置换（permutation）和方向等变（orientation equivariance）提供新的定义，并提请注意它们与传统的GNNs等变性概念之间的关系。 8.2.1 CCNNs的置换等变 命题 8.1描述了CC的结构，受其启发，本节将介绍置换等变CCNNs。我们将首先定义置换群在共链映射空间上的行为。 定义 8.3 (共链映射空间上的置换，Permutation action on space of cochain maps) 令 \\(\\mathcal{X}\\)是CC，令\\(\\mbox{Sym}(\\mathcal{X}) = \\prod_{i=0}^{\\dim(\\mathcal{X})} \\mbox{Sym}(\\mathcal{X}^k)\\) 是 \\(\\mathcal{X}\\)上胞腔的秩保持置换（rank-preserving permutations）群，令 \\(\\mathbf{G}=\\{G_k\\}\\)是定义在\\(\\mathcal{X}\\)上的共链映射，\\(G_k \\colon \\mathcal{C}^{i_k}\\to \\mathcal{C}^{j_k}\\), \\(0\\leq i_k,j_k\\leq \\dim(\\mathcal{X})\\)的序列，令\\(\\mathcal{P}=(\\mathbf{P}_i)_{i=0}^{\\dim(\\mathcal{X})} \\in \\mbox{Sym}(\\mathcal{X})\\). 则，可定义\\(\\mathbf{G}\\)上\\(\\mathcal{P}\\)的置换行为（permutation (group) action） 为\\(\\mathcal{P}(\\mathbf{G}) = (\\mathbf{P}_{j_k} G_{k} \\mathbf{P}_{i_k}^T )_{i=0}^{\\dim(\\mathcal{X})}\\) . 在定义8.4中，我们用定义8.3中给出的群操作引入了置换等变CCNNs。定义8.4泛化了文献(T. Mitchell Roddenberry, Glaze, and Segarra 2021; Schaub et al. 2021)中的相关定义，更详细的讨论可参阅文献(Jogl 2022; Veličković 2022)。定义中，我们使用 \\(\\mbox{Proj}_k \\colon \\mathcal{C}^1\\times \\cdots \\times \\mathcal{C}^m \\to \\mathcal{C}^k\\) 来表示满足 \\(1\\leq k \\leq m\\)的标准 \\(k\\)-th 投影（projection ），该投影通常表述为 \\(\\mbox{Proj}_k ( \\mathbf{H}_{1},\\ldots, \\mathbf{H}_{k},\\ldots,\\mathbf{H}_{m})= \\mathbf{H}_{k}\\) ）. 定义 8.4 (置换等变CCNN，Permutation-equivariant CCNN) 令 \\(\\mathcal{X}\\)是CC，\\(\\mathbf{G}= \\{G_k\\}\\)是定义在\\(\\mathcal{X}\\)上的有限共链映射序列， \\(\\mathcal{P}=(\\mathbf{P}_i)_{i=0}^{\\dim(\\mathcal{X})} \\in \\mbox{Sym}(\\mathcal{X})\\)。 有如下形式的CCNN： \\[\\begin{equation*} \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n} \\end{equation*}\\] 称上述形式为置换等变CCNN，如果： \\[\\begin{equation} \\mbox{Proj}_k \\circ \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}(\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m})= \\mathbf{P}_{k} \\mbox{Proj}_k \\circ \\mbox{CCNN}_{\\mathcal{P}(\\mathbf{G});\\mathbf{W}}(\\mathbf{P}_{i_1} \\mathbf{H}_{i_1}, \\ldots ,\\mathbf{P}_{i_m} \\mathbf{H}_{i_m}) \\end{equation}\\] 对所有\\(1 \\leq k\\leq m\\) 和任意 \\((\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m}) \\in\\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m}\\) 都成立。 定义 8.4 泛化了GNNs的置换等变性的相应概念。对于一个有\\(n\\)个顶点和邻接矩阵 \\(A\\)的图，用\\(\\mathrm{GNN}_{A;W}\\)表示该图上的GNN。令 \\(H \\in \\mathbb{R}^{n \\times k}\\)是顶点特征，那么对于\\(P \\in \\mbox{Sym}(n)\\)有\\(P \\,\\mathrm{GNN}_{A;W}(H) = \\mathrm{GNN}_{PAP^{T};W}(PH)\\)， 则说\\(\\mathrm{GNN}_{A;W}\\) 是置换等变的。 一般来讲，使用 Definition 8.4可能很麻烦。用聚合节点来表征等变性更为简便。为此，请回想一下，张量图的高度是从任意源节点到任意目标节点的最长路径，且命题8.5允许我们用聚合节点来表达高度为1的张量图。 命题 8.5 (高度为1的张量图可看作聚合节点，Tensor diagrams of height one as merge nodes) 令 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n}\\)是张量图高度为1的CCNN，那么 \\[\\begin{equation} \\label{merge_lemma} \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}=( \\mathcal{M}_{\\mathbf{G}_{j_1};\\mathbf{W}_1},\\ldots, \\mathcal{M}_{\\mathbf{G}_{j_n};\\mathbf{W}_n}), \\tag{8.2} \\end{equation}\\] 其中， \\(\\mathbf{G}_k \\subseteq \\mathbf{G}\\). 证明. 令 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n}\\)是张量图高度为1的CCNN。由于函数 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)的共域是 \\(\\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\ldots \\times \\mathcal{C}^{j_n}\\)，那么 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)由\\(n\\)个函数 \\(F_k\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j_k}\\)确定（\\(1 \\leq k \\leq n\\)）。 由于\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)的张量图高度为1，那么每个函数 \\(F_k\\) 的高度也为1，因此根据定义它是一个聚合节点。结果得证。 命题 8.5 指出高度为1的张量图中的每个目标节点 \\(j_k\\) 都是由目标节点 \\(j_k\\) 的边的标签组成的算子 \\(\\mathbf{G}_{j_k}\\) 指定的聚合节点。定义8.4 引入了CCNNs置换不变性的一般性概念，定义8.5 引入了置换不变聚合节点的概念。由于聚合节点是CCNN，所以定义 8.5是定义8.4的特例。 定义 8.5 (置换不变聚合节点，Permutation-equivariant merge node) 令 \\(\\mathcal{X}\\)是CC，\\(\\mathbf{G}= \\{G_k\\}\\)是用\\(G_k\\colon C^{i_k}(\\mathcal{X})\\to C^{j}(\\mathcal{X})\\)定义在\\(\\mathcal{X}\\)上的共链算子的有限序列，\\(\\mathcal{P}=(\\mathbf{P}_i)_{i=0}^{\\dim(\\mathcal{X})} \\in \\mbox{Sym}(\\mathcal{X})\\)。称公式(5.1)中的聚合节点是 置换不变聚合节点，如果满足： \\[\\begin{equation} \\mathcal{M}_{\\mathbf{G};\\mathbf{W}}(\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m})= \\mathbf{P}_{j} \\mathcal{M}_{\\mathcal{P}(\\mathbf{G});\\mathbf{W}}(\\mathbf{P}_{i_1} \\mathbf{H}_{i_1}, \\ldots ,\\mathbf{P}_{i_1} \\mathbf{H}_{i_m}) \\end{equation}\\] 对任何\\((\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m}) \\in \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m}\\). 命题 8.6 (高度为1且有聚合节点的的CCNNs) 令 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n}\\)是张量图高度为1的CCNN，那么当且仅当公式(8.2)中的聚合节点\\(\\mathcal{M}_{\\mathbf{G}_{j_k};\\mathbf{W}_k}\\)，\\(1 \\leq k \\leq n\\)，是置换等变时，\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\) 是置换等变。 证明. 如果 CCNN高度为1，那么根据命题 8.5, 有\\(\\mbox{Proj}_k \\circ \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}(\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m})= \\mathcal{M}_{\\mathbf{G}_{j_k};\\mathbf{W}_k}\\)。从聚合节点等变（参见定义8.5）和CCNN置换等变（参见定义8.4）可自然得出这个结论。 最后, 定理 8.2 用聚合节点来描述CCNNs的置换等变性，从这角度来看，定理8.2提供了CCNNs置换等变的适用版本。 provides a practical version of permutation equivariance for CCNNs. 定理 8.2 (置换等变CCNN和聚合节点，Permutation-equivariant CCNN and merge nodes) 如果\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)是置换等变，当且仅当 \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)中每个聚合节点都是置换等变的。 证明. 命题 8.6 证明了高度为1的CCNNs的置换等变。对于高度为\\(n\\)的CCNNs，只需注意到高度为\\(n\\)的CCNN可由高度为1的CCNNs组合而得到，并且两个置换等变网络的组合也仍然是置换等变网络。 备注. 置换等变假定每维的所有胞腔都是使用独立的索引来标注，如果标记CC中的胞腔用 \\(\\mathcal{P}(S)\\)的子集的幂集\\(\\mathcal{P}(S)\\)，而不是用索引，那么，我们只需考虑幂级置换 胞腔的置换引起的幂集置换，以确保结构保持关系。因此，只需要考虑由 0-cell构成的的幂集排列，以确保排等边差关系。 备注. GNN 具有等变性，指图形顶点集和顶点集上的输入信号的置换都产生相同的 GNN 输出置换。 因此，在底层 CC 的增强 Hasse 图上应用标准 GNN 并不等同于应用 CCNN。 虽然信息传递结构相同，但标准 GNN 和 CCNN 的权重共享和置换等变性却是不同的。 尤其，定义 4.2 给出了额外的结构，在对增强Hasse图的顶点进行任意置换时，这种结构不会被保持。 因此, 为了将 CCNN 上的信息传递归约为相关的增强Hasse图上的信息传递，就必须小心谨慎。具体来说，我们只需考虑增强Hasse图中由相应 CC 中 0-cells的置换诱导的顶点标签的置换子群。 因此，采用拓扑学的丰富概念来思考分布式结构化学习架构是有价值的，因为拓扑结构有助于推理计算，而这些方式并不存在基于图的方法范围内。 备注. 请注意，命题 8.4)与前面的注释并不矛盾。事实上，命题8.4 中描述的计算是在 Hasse 图的一个特定子图上进行的，该子图的顶点是底层胞腔复形的 \\(k\\)-cells。一旦在计算过程中同时考虑不同维度，基于图的网络和 TDL 网络之间的差异就会开始显现。 8.2.2 CCNNs的方向等变 当 CC 被归约为正则胞腔复形时，也可以将方向等变引入 CCNN。类似于定义 8.3，我们将引入以下关于 CC 的方向作用的定义。 定义 8.6 (对角共链映射空间上的方向作用，Orientation action on space of diagonal-cochain maps) 令 \\(\\mathcal{X}\\)是CC， \\(\\mathbf{G}=\\{G_k\\}\\)是定义在\\(\\mathcal{X}\\)上的\\(G_k \\colon \\mathcal{C}^{i_k}\\to \\mathcal{C}^{j_k}\\), \\(0\\leq i_k,j_k\\leq \\dim(\\mathcal{X})\\)的共链算子序列。令 \\(O(\\mathcal{X})\\)是\\(\\mathcal{D}=(\\mathbf{D}_i)_{i=0}^{\\dim(\\mathcal{X})}\\) 对角矩阵群，对角线上的值为 \\(\\pm 1\\) ，矩阵大小为\\(|\\mathcal{X}^k| \\times |\\mathcal{X}^k|\\)，使得 \\(\\mathbf{D}_0=I\\)。 用\\(\\mathcal{D}(\\mathbf{G}) = (\\mathbf{D}_{j_k} G_{k} \\mathbf{D}_{i_k})_{i=0}^{\\dim(\\mathcal{X})}\\)来定义\\(\\mathbf{G}\\)上的 方向(群)作用\\(\\mathcal{D}\\)。 定义 8.7使用定义8.6引入的群作用来定义了CCNNs的方向等变， CCNNs 的方向等变性(定义 8.7) 用与CCNNs的置换等变类似的方式(定义 8.4)被提出。 定义 8.7 (方向等变CCNN，Orientation-equivariant CCNN) 令 \\(\\mathcal{X}\\)是CC，令 \\(\\mathbf{G}= \\{G_k\\}\\)定义在\\(\\mathcal{X}\\)上的有限共链算子序列，令 \\(\\mathcal{D} \\in O(\\mathcal{X})\\)，那么形如 \\[\\begin{equation*} \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\colon \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m} \\to \\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\cdots \\times \\mathcal{C}^{j_n} \\end{equation*}\\] 的CCNN被称为 方向等变CCNN ，如果满足 \\[\\begin{equation} \\mbox{Proj}_k \\circ \\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}(\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m})=\\mathbf{D}_{k} \\mbox{Proj}_k \\circ \\mbox{CCNN}_{\\mathcal{D}(\\mathbf{G});\\mathbf{W}}((\\mathbf{D}_{i_1} \\mathbf{H}_{i_1}, \\ldots ,\\mathbf{D}_{i_1} \\mathbf{H}_{i_m})) \\end{equation}\\] 对所有 \\(1 \\leq k\\leq m\\) 和任何 \\((\\mathbf{H}_{i_1},\\ldots ,\\mathbf{H}_{i_m}) \\in \\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\cdots \\times \\mathcal{C}^{i_m}\\). 命题 8.5 和 8.6 对于方向等变的情况也可以类比说明。我们在此跳过这些事实的陈述，只陈述用聚合节点表征 CCNN 方向等变的主要定理。 定理 8.3 (方向等变CCNN和聚合节点，Orientation-equivariant CCNN and merge nodes) \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\) 是方向等变的，当且仅当\\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\)中的每个聚合节点都是方向等变的。 证明. 定理 8.3的证明类似于定理 8.2的证明。 参考文献 Abramenko, Peter, and Kenneth S. Brown. 2008. Buildings: Theory and Applications. Vol. 248. Springer Science &amp; Business Media. Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” arXiv Preprint arXiv:2104.13478. Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Hamilton, William L., Rex Ying, and Jure Leskovec. 2017. “Representation Learning on Graphs: Methods and Applications.” IEEE Data Engineering Bulletin 40 (3): 52–74. Jogl, Fabian. 2022. “Do We Need to Improve Message Passing? Improving Graph Neural Networks with Graph Transformations.” PhD thesis, Vienna University of Technology. Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021. “Principled Simplicial Neural Networks for Trajectory Prediction.” In International Conference on Machine Learning. Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. “Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.” Signal Processing 187: 108149. Veličković, Petar. 2022. “Message Passing All the Way Up.” ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Wachs, Michelle L. 2006. “Poset Topology: Tools and Applications.” arXiv Preprint Math/0602226. 对于相关结构 (例如, 单纯形/胞腔/胞腔复形), 偏序关系一般称为 面偏序（face poset） (Wachs 2006)。↩︎ "],["implementation-and-numerical-results.html", "第 9 章 实现与实验 9.1 软件：TopoNetX, TopoEmbedX, and TopoModelX 9.2 数据集 9.3 形状分析：网格分割与分类 9.4 Pooling with mapper on graphs and data classification 9.5 Ablation studies", " 第 9 章 实现与实验 所提出的 CCNNs 可用于为不同的学习任务构建不同的神经网络架构。在本节中，我们将通过评估 CCNNs 在形状分析和图形学习任务中的预测性能来证明其通用性和有效性。在几何处理实验中，我们将 CCNNs 与最先进的方法进行了比较，这些方法针对特定任务进行了高度设计和训练。此外，我们还对几何数据处理中常用的各种数据模式（即点云和三维网格）进行了实验。我们还对图形数据进行了实验。在实验中，我们调整了三个主要部分：CCNN 架构的选择、学习率和数据增强中的副本数量。我们为每项学习任务选择的 CCNN 架构进行了论证。我们在 PyTorch 中实现了我们的流程，并在使用 Microsoft Windows 后端的单 GPU NVIDIA GeForce RTX 3060 Ti 上运行了实验。 9.1 软件：TopoNetX, TopoEmbedX, and TopoModelX 我们所有的软件开发和实验分析都是使用 Python 进行的。我们也开发了三个 Python 软件包，并用它们来运行我们的实验: TopoNetX，支持构建多种拓扑结构，包括胞腔复形、单纯复形和组合复形类。这些类分别提供了计算胞腔复形、单纯复形和组合复形上的边界算子（ boundary operators）、霍奇拉普拉斯（Hodge Laplacians）和高阶邻接算子的方法； TopoEmbedX ，支持对胞腔复形、单纯复形和组合复形的高阶关系进行表征学习（representation learning ）； TopoModelX， 支持计算定义在这些拓扑域上的深度学习模型。 除了所实现的软件包，我们还使用了 PyTorch (Paszke et al. 2017) 来训练本节中报告的神经网络。此外，我们还利用 Scikit-learn (Pedregosa et al. 2011) 计算了 1-Hodge Laplacians的特征向量。点云的法向量是使用点云工具包（Point Cloud Utils package）(Williams 2022)计算的。最后，在软件包的开发和计算过程中，我们使用了 NetworkX (Hagberg, Swart, and S Chult 2008) 和 HyperNetX (Joslyn et al. 2021)。 9.2 数据集 在CCNNs的评估实验中，我们使用了四种数据集：Human Body, COSEG, SHREC11, 以及一个用于图分类的标准数据集 (Bianchi, Gallicchio, and Micheli 2022)。数据集的摘要如下： 人体分割数据集，Human Body segmentation dataset. 文献(Atzmon, Maron, and Lipman 2018) 中提出的原始人体分割数据集包含相对较大的网格，网格顶点最多可达 12000 个。该数据集中提供的分割标签是按面(per-face)设置的，分割准确率被定义为正确分类的面数与整个数据集中面总数的比率。在本文项工作中，我们使用了 (Hanocka et al. 2019) 提供的原始人体数据集的简化版本，其中网格的节点数少于 1,000 个，分割标签被重新映射到边上。我们在第 9.3.1节的形状分析（例如，网格分割）任务中使用了这个简化版的人体数据集。 COSEG分割数据集，COSEG segmentation dataset. 原始 COSEG 数据集(Y. Wang et al. 2012)包含 11 组带有基准真值（ground-true）分割的形状。在本文工作中，我们使用了原始 COSEG 数据集的一个子集，其中包含相对较大的外星人、花瓶和椅子集。这三个数据集分别包含 200、300 和 400 个形状。我们使用这个自定义的 COSEG 数据集子集来完成第 9.3.1节中的形状分析（例如，网格分割）任务。 SHREC11分类数据集，SHREC11 classification dataset. SHREC 2011 (Lian et al. 2011), 简写为SHREC11, 是一个大型数据集，其中包含来自 30 个类别的 600 个非刚性变形形状（水密三角形网格，watertight triangel meshes8），每个类别包含相同数量的物体。这些类别包括手、灯、女人、男人、火烈鸟和兔子。该数据集分为训练集和测试集，分别包含 480 个和 120 个形状。我们使用 SHREC11 数据集来完成 9.3.2 和 9.4章节中的形状分析任务。 译者注：水密（watertight）网格通常描述由一个封闭曲面组成的网格，水密网格不包含孔洞并且内部定义明确 图分类基准数据集. 该数据集包含属于三个不同类别的图 (Bianchi, Gallicchio, and Micheli 2022)。对于每个图，每个顶点（0-cochain）上的特征向量都是大小为 5 的独热向量，它存储了图上顶点的相对位置。该数据集分为简易版和困难版，简易版包含高度连接的图，而困难版包含稀疏的图。我们在第 9.3.3节的图分类任务中使用了这个数据集。 9.3 形状分析：网格分割与分类 用于形状分析实验（网格分割和分类）的 CC 结构是由网格的三角剖分简单诱导出来的。具体来说，0-、1-和 2-cells分别是网格的顶点、边和面。用于 CCNN 的矩阵是 \\(B_{0,1},~B_{0,2}\\)、它们的转置矩阵以及（共）邻接矩阵 \\(A_{1,1}\\)、\\(coA_{1,1}\\) 和 \\(coA_{2,1}\\)。 CCNN 将共链向量作为输入特征。对于形状分析任务，我们考虑直接从底层网格的顶点坐标建立特征的共链，我们也注意到还有其他选择（例如 文献(Mejia, Ruiz-Salguero, and Cadavid 2017) 中基于光谱的共链）也可以包括在内。我们的形状分析任务有三个输入共链：顶点共链、边共链和面共链，每个顶点共链有两个输入特征：与顶点相关的位置和法向量（normal vector）。与(Hanocka et al. 2019)类似，每个边共链由五个特征组成：每个面的边长、二面角（dihedral angle）、两个内角和两个边长比。最后，每个输入面共链由三个输入特征组成：面面积、面法线和三个面角度。 9.3.1 网格分割 对于人体数据集 (Maron et al. 2017)，我们构建了一个 CCNN，它能产生一个边类。架构的张量图如图9.1(a)所示。对于 COSEG 数据集(Y. Wang et al. 2012)，我们构建了一个 CCNN，结合我们提出的定义在顶点、边和面上的特征向量来学习最终的面类。如图 9.1(b)所示，该架构使用关联矩阵以及（共）邻接矩阵来构建信号流。具体来说，张量图显示了三个非平方注意块（non-squared attention-blocks）和三个平方注意块（squared attention blocks）。如图 9.1(b)所示，模型的深度选择为2。 图 9.1: 实验中所用的CCNNs的张量图 (a): 用于网格分割任务的 CCNNs. 尤其, \\(\\mbox{CCNN}_{HB}\\) 和 \\(\\mbox{CCNN}_{COSEG}\\)分别是 是人体数据集(Atzmon, Maron, and Lipman 2018)和COSEG dataset (Y. Wang et al. 2012)使用的架构。 (b): SHREC11 数据集 (Lian et al. 2011)使用网格分类CCNN. (c): 数据集(Bianchi, Gallicchio, and Micheli 2022)使用图分类CCNN。 (d): 在 SHREC11 数据集上网格/点云分类 CCNNs与 MOG 算法结合使用. Note that the architectures chosen for the COSEG and for the Human Body datasets have the same number and types of building blocks; compare Figures 9.1(a) and (b). We use a random 85%-15% train-test split. For both of these architectures, a softmax activation is applied to the output tensor. All our segmentation models are trained for 600 epochs using a learning rate of 0.0001 and the standard cross-entropy loss. These results are consistent across Human Body and Shape COSEG datasets. We test the proposed CCNNs on mesh segmentation using the Human Body (Maron et al. 2017) and the Shape COSEG (vase, chair, and alien) (Y. Wang et al. 2012) datasets. For each mesh in these datasets, the utilized CC structure is the one induced by the triangulation of the meshes, although other variations in the CC structure yield comparable results. Further, three \\(k\\)-cochains are constructed for \\(0\\leq k \\leq 2\\) and are utilized in CCNN training. As shown in Table 9.1, CCNNs outperform three neural networks tailored to mesh analysis (HodgeNet (Smirnov and Solomon 2021), PD-MeshNet (Milano et al. 2020) and MeshCCN (Hanocka et al. 2019)) on two out of four datasets, and are among the best two neural networks on all four datasets. 表 9.1: Predictive accuracy on test sets related to shape analysis, namely on Human Body and COSEG (vase, chair, alien) datasets. The results reported here are based on the \\(\\mbox{CCNN}_{COSEG}\\) and \\(\\mbox{CCNN}_{HB}\\) architectures. In particular, the result for \\(\\mbox{CCNN}_{HB}\\) is reported in the first column, whereas the results for \\(\\mbox{CCNN}_{COSEG}\\) are reported in the second, third and forth columns. Method Human Body COSEG vase COSEG chair COSEG alien HodgeNet 85.03 90.30 95.68 96.03 PD-MeshNet 85.61 95.36 97.23 98.18 MeshCNN 85.39 92.36 92.99 96.26 CCNN 87.30 93.40 98.30 93.70 Architecture of \\(\\mbox{CCNN}_{COSEG}\\) and \\(\\mbox{CCNN}_{HB}\\). In \\(\\mbox{CCNN}_{COSEG}\\), as shown in Figure 9.1(a), we choose a CCNN pooling architecture as given in Definition 7.5, which pushes signals from vertices, edges and faces, and aggregates their information towards the final face prediction class. We choose \\(\\mbox{CCNN}_{HB}\\) similarly, except that the predicted signal is an edge class. The reason for this choice is that the Human Body dataset (Atzmon, Maron, and Lipman 2018) encodes the segmentation information on edges. 9.3.2 Mesh and point cloud classification We evaluate our method on mesh classification using the SHREC11 dataset (Lian et al. 2011) based on the same cochains and CC structure used in the segmentation experiment of Section 9.3.1. The CCNN architecture for our mesh classification task, denoted by \\(\\mbox{CCNN}_{SHREC}\\), is demonstrated in Figure 9.1(b). The final layer of \\(\\mbox{CCNN}_{SHREC}\\), depicted as a grey node in Figure 9.1(b), is a simple pooling operation that sums all embeddings of the CC after mapping them to the same Euclidean space. The \\(\\mbox{CCNN}_{SHREC}\\) is trained for 40 epochs with both tanh and identity activation functions using a learning rate of 0.005 and the standard cross-entropy loss. We use anisotropic scaling and random rotations for data augmentation. Each mesh is augmented 30 times, is centered around the vertex center of the mass, and is rescaled to fit inside the unit cube. The \\(\\mbox{CCNN}_{SHREC}\\) with identity activations and \\(\\tanh\\) activations achieve predictive accuracies of 96.67% and 99.17%, respectively. Table 9.2 shows that CCNNs outperform two neural networks tailored to mesh analysis (HodgeNet and MeshCCN), being the second best model behind PD-MeshNet in mesh and point cloud classification. It is worth mentioning that the mesh classification CCNN requires a significantly lower number of epochs to train (40 epochs) as compared to the mesh segmentation CCNNs (600 epochs). 表 9.2: Predictive accuracy on the SHREC11 test dataset. The left and right column report the mesh and point cloud classification results, respectively. The CCNN for mesh classification is \\(\\mbox{CCNN}_{SHREC}\\), while the CCNN for point cloud classification is \\(\\mbox{CCNN}_{MOG2}\\). Method Mesh Point cloud HodgeNet 99.10 94.70 PD-MeshNet 99.70 99.10 MeshCNN 98.60 91.00 CCNN 99.17 95.20 Architecture of \\(\\mbox{CCNN}_{SHREC}\\). The \\(\\mbox{CCNN}_{SHREC}\\) has two layers and is chosen as a pooling CCNN in the sense of Definition 7.5, similar to \\(\\mbox{CCNN}_{COSEG}\\) and \\(\\mbox{CCNN}_{HB}\\). The main difference is that the final layer of \\(\\mbox{CCNN}_{SHREC}\\), represented by the grey point in Figure 9.1(b), is a global pooling function that sums all embeddings of all dimensions (zero, one and two) of the underlying CC after mapping them to the same Euclidean space. 9.3.3 Graph classification For the graph classification task, we use the graph classification benchmark provided in (Bianchi, Gallicchio, and Micheli 2022); the dataset consists of graphs with three different labels. For each graph, the feature vector on each vertex (the 0-cochain) is a one-hot vector of size five, and it stores the relative position of the vertex on the graph. To construct the CC structure, we use the 2-clique complex of the input graph. We then proceed to build the CCNN for graph classification, denoted by \\(\\mbox{CCNN}_{Graph}\\), which is visualized in Figure 9.1(c). The matrices used for the construction of \\(\\mbox{CCNN}_{Graph}\\) are \\(B_{0,1},~B_{1,2},~B_{0,2}\\), their transpose matrices, and the (co)adjacency matrices \\(A_{0,1},A_{1,1},~coA_{2,1}\\). The cochains of \\(\\mbox{CCNN}_{Graph}\\) are constructed as follows. For each graph in the dataset, we set the 0-cochain to be the one-hot vector of size 5 provided by the dataset. This one-hot vector stores the relative position of the vertex on the graph. We also construct the 1-cochain and 2-cochain on the 2-clique complex of the graph by considering the coordinate-wise max value of the one-hot vectors attached to the vertices of each cell. The input to \\(\\mbox{CCNN}_{graoh}\\) consists of the 0-cochain provided as a part of the dataset as well as the constructed 1 and 2-cochains. The grey node in Figure 9.1(c) indicates a simple mean pooling operation. We train this network with a learning rate of 0.005 and no data augmentation. Table 9.3 reports the results on the easy and the hard versions of the datasets9, and compares them to six state-of-the-art GNNs. As shown in Table 9.3, CCNNs outperform all six GNNs on the hard dataset, and five of the GNNs on the easy dataset. The proposed CCNN outperforms MinCutPool on the hard dataset, while it attains comparable performance to MinCutPool on the easy dataset. 表 9.3: Predictive accuracy on the test set of (Bianchi, Gallicchio, and Micheli 2022) related to graph classification. All results are reported using the \\(\\mbox{CCNN}_{Graph}\\) architecture. Dataset Graclus NDP DiffPool Top-K SAGPool MinCutPool CCNN Easy 97.81 97.93 98.64 82.47 84.23 99.02 98.90 Hard 69.08 72.67 69.98 42.80 37.71 73.80 75.59 Architecture of \\(\\mbox{CCNN}_{Graph}\\). In the \\(\\mbox{CCNN}_{Graph}\\) displayed in Figure 9.1(c) we choose a CCNN pooling architecture as given in Definition 7.5 that pushes signals from vertices, edges and faces, and aggregate their information towards the higher-order cells before making making the final prediction. For the dataset of (Bianchi, Gallicchio, and Micheli 2022), we experiment with two architectures; the first one is identical to the \\(\\mbox{CCNN}_{SHREC}\\) shown in Figure 9.1(b), and the second one is the \\(\\mbox{CCNN}_{Graph}\\) shown in Figure 9.1(c). We report the results for \\(\\mbox{CCNN}_{Graph}\\), as it provides superior performance. Note that when this neural network is conducted on an underlying simplicial complex, the neighborhood matrices \\(B_{0,1}\\) and \\(B_{1,3}\\) are typically not considered, hence the CC-structure equipped with these additional incidence matrices improves the generalization performance of the \\(\\mbox{CCNN}_{Graph}\\). 9.4 Pooling with mapper on graphs and data classification We perform experiments to measure the effectiveness of the MOG pooling strategy discussed in Section 7.4. Recall that the MOG algorithm requires two pieces of input: the 1-skeleton of a CC \\(\\mathcal{X}\\), and a scalar function on the vertices of \\(\\mathcal{X}\\). Our choice for the input scalar function is the average geodesic distance (AGD) (V. G. Kim et al. 2010), which is suitable for shape detection as it is invariant to reflection and rotation. For two entities \\(u\\) and \\(v\\) on a graph, the geodesic distance between \\(u\\) and \\(v\\), denoted by \\(d(v,u)\\), is computed using Dijkstra’s shortest path algorithm. The AGD is given by the following equation: \\[\\begin{equation} AGD(v)=\\frac{1}{|V|}\\sum_{u\\in V}d(v,u). \\tag{9.1} \\end{equation}\\] From Equation (9.1), it is immediate that the vertices near the center of the graph are likely to have low function values, while points on the periphery are likely to have high values. This observation has been utilized to study graph symmetry (V. G. Kim et al. 2010), and it provides a justification for selecting the AGD for the MOG pooling strategy. Figure 9.2 presents a few examples of applying the MOG pooling strategy using AGD on the SHREC11 dataset. 图 9.2: Examples of applying the MOG algorithm on the SHREC11 dataset (Lian et al. 2011). In each figure, we show the original mesh graph on the left and the mapper graph on the right. The scalar function chosen for the MOG algorithm is the average geodesic distance (AGD). We observe that the pooled mapper graph has similar overall shape to the original graphs. In order to demonstrate the effectiveness of our MOG pooling approach, we conduct three experiments on the SHREC11 dataset: mesh classification based on CC-pooling with input vertex and edge features (Section 9.4.1), mesh classification based on CC-pooling with input vertex features only (Section 9.4.2), and point cloud classification based on CC-pooling with input vertex features only (Section 9.4.3). The experiments in Sections 9.4.1 and 9.4.2 utilize the mesh structure in the SHREC11 dataset, whereas the experiment in Section 9.4.3 utilizes its own point cloud version. In particular, we choose two simple CCNN architectures shown in Figure 9.1(d), denoted by \\(\\mbox{CCNN}_{MOG1}\\) and \\(\\mbox{CCNN}_{MOG2}\\), as opposed to the more complicated architecture of \\(\\mbox{CCNN}_{SHREC}\\) in Figure 9.1(b). The main difference between \\(\\mbox{CCNN}_{MOG1}\\) and \\(\\mbox{CCNN}_{MOG2}\\) is the choice of the input feature vectors as described next. 9.4.1 Mesh classification: CC-pooling with input vertex and edge features In this experiment, we consider the vertex feature vector to be the position concatenated with the normal vectors for each vertex in the underlying mesh. For the edge features, we compute the first ten eigenvectors of the 1-Hodge Laplacian (Dodziuk 1976; Eckmann 1944) and attach a 10-dimensional feature vector to the edges of the underlying mesh. The CC that we consider here is 3-dimensional, as it consists of the triangular mesh (vertices, edges and faces) and of 3-cells. The 3-cells are obtained using the MOG algorithm, and are used for augmenting each mesh. We calculate the 3-cells via the MOG algorithm using the AGD scalar function as input. We conduct this experiment using the CCNN defined via the tensor diagram \\(\\mbox{CCNN}_{MOG1}\\) given in Figure 9.1(d). During training, we augment each mesh with ten additional meshes, with each of these additional meshes being obtained by a random rotation as well as 0.1% noise perturbation to the vertex positions. We train \\(\\mbox{CCNN}_{MOG1}\\) for 100 epochs using a learning rate of 0.0002 and the standard cross-entropy loss, and obtain an accuracy of 98.1%. While the accuracy of \\(\\mbox{CCNN}_{MOG1}\\) is lower than the one we report for \\(\\mbox{CCNN}_{SHREC}\\) (99.17%) in Table 9.2, we note that \\(\\mbox{CCNN}_{MOG1}\\) requires a significantly smaller number of replications for mesh augmentation to achieve a similar accuracy (\\(\\mbox{CCNN}_{MOG1}\\) requires 10, whereas \\(\\mbox{CCNN}_{SHREC}\\) required 30 replications). Architecture of \\(\\mbox{CCNN}_{MOG1}\\). The tensor diagram \\(\\mbox{CCNN}_{MOG1}\\) of Figure 9.1(d) corresponds to a pooling CCNN. In particular, \\(\\mbox{CCNN}_{MOG1}\\) pushes forward the signal towards two different higher-order cells: the faces of the mesh as well as the 3-cells obtained from the MOG algorithm. 9.4.2 Mesh classification: CC-pooling with input vertex features only In this experiment, we consider the position and the normal vectors of the input vertices. The CC structure that we consider is the underlying graph structure obtained from each mesh; i.e., we only use the vertices and the edges, and ignore the faces. We augment this structure by 2-cells obtained via the MOG algorithm using the AGD scalar function as input. We choose the network architecture to be relatively simpler than \\(\\mbox{CCNN}_{MOG1}\\), and report it in Figure 9.1(d) as \\(\\mbox{CCNN}_{MOG2}\\). During training we augment each mesh with 10 additional meshes, with each of these additional meshes being obtained by a random rotation as well as 0.05% noise perturbation to the vertex positions. We train \\(\\mbox{CCNN}_{MOG2}\\) for 100 epochs using a learning rate of 0.0003 and the standard cross-entropy loss, and obtain an accuracy of 97.1%. Architecture of \\(\\mbox{CCNN}_{MOG2}\\) for mesh classification. The tensor diagram \\(\\mbox{CCNN}_{MOG2}\\) of Figure 9.1(d) corresponds to a pooling CCNN. In particular, \\(\\mbox{CCNN}_{MOG2}\\) pushes forward the signal towards a single 2-cell obtained from the MOG algorithm. Observe that the overall architecture of \\(\\mbox{CCNN}_{MOG2}\\) is similar in principle to AlexNet (Krizhevsky, Sutskever, and Hinton 2017), where convolutional layers are followed by pooling layers. 9.4.3 Point cloud classification: CC-pooling with input vertex features only In this experiment, we consider point cloud classification on the SHREC11 dataset. The setup is similar in principle to the one studied in Section 9.4.2 where we consider only the features supported on the vertices of the point cloud as input. Specifically, for each mesh in the SHREC11 dataset, we sample 1,000 points from the surface of the mesh. Additionally, we estimate the normal vectors of the resulting point clouds using the Point Cloud Utils package (Williams 2022). To build the CC structure, we first consider the \\(k\\)-nearest neighborhood graph obtained from each point cloud using \\(k=7\\). We then augment this graph by 2-cells obtained via the MOG algorithm using the AGD scalar function as input. We train the \\(\\mbox{CCNN}_{MOG2}\\) shown in Figure 9.1(d). During training, we augment each point cloud with 12 additional instances, each one of these instances being obtained by random rotation. We train \\(\\mbox{CCNN}_{MOG2}\\) for 100 epochs using a learning rate of 0.0003 and the standard cross-entropy loss, and obtain an accuracy of 95.2% (see Table 9.2). 9.5 Ablation studies In this section, we perform two ablation studies. The first ablation study reveals that pooling strategies in CCNNs have a crucial effect on predictive performance. The second ablation study demonstrates that CCNNs have better predictive capacity than GNNs; the advantage of CCNNs arises from their topological pooling operations and from their ability to learn from topological features. Pooling strategies in CCNNs. To evaluate the impact of the choice of pooling strategy on predictive performance, we experiment with two pooling strategies using the SHREC11 classification dataset. The first pooling strategy is the MOG algorithm described in Section 9.4; the results of this pooling strategy based on \\(\\mbox{CCNN}_{MOG2}\\) are discussed in Section 9.4.2 (97.1%). The second pooling strategy is briefly described as follows. For each mesh, we consider the 2-dimensional CC obtained by considering each 1-hop neighborhood to be the 1-cells in the CC and each 2-hop neighborhood to be the 2-cells in the CC. We train \\(\\mbox{CCNN}_{MOG2}\\), and obtain an accuracy of 89.2%, which is lower than 97.1%. These experiments suggest that the choice of pooling strategy has a crucial effect on predictive performance. Comparing CCNNs to GNNs in terms of predictive performance. Observe that \\(\\mbox{CCNN}_{SHREC}\\) has topological features of dimension one and two as inputs. On the other hand, \\(\\mbox{CCNN}_{MOG2}\\) has only vertex features as input, but it learns the higher-order cell latent features by using the push-forward operation that pushes the signal from 0-cells to the 2-cells obtained from the MOG algorithm. In both cases, using a higher-order structure is essential for improving predictive performance, even though two different strategies towards exploiting the higher-order structures are utilized. To support our claim, we run an experiment in which we replace the pooling layer in \\(\\mbox{CCNN}_{MOG2}\\) by the cochain operator induced by \\(A_{0,1}\\), effectively rendering the neural network as a GNN. In this setting, using the same setup as in experiment 9.4.2, we obtain an accuracy of 84.56%. This experiment reveals the performance advantages of employing higher-order structures, either by utilizing the input topological features supported on higher-order cells or via pooling strategies that augment higher-order cells. 参考文献 Atzmon, Matan, Haggai Maron, and Yaron Lipman. 2018. “Point Convolutional Neural Networks by Extension Operators.” ACM Transactions on Graphics 37 (4). Bianchi, Filippo Maria, Claudio Gallicchio, and Alessio Micheli. 2022. “Pyramidal Reservoir Graph Neural Network.” Neurocomputing 470: 389–404. Dodziuk, Jozef. 1976. “Finite-Difference Approach to the Hodge Theory of Harmonic Forms.” American Journal of Mathematics 98 (1): 79–104. Eckmann, Beno. 1944. “Harmonische Funktionen Und Randwertaufgaben in Einem Komplex.” Commentarii Mathematici Helvetici 17 (1): 240–55. Hagberg, Aric, Pieter Swart, and Daniel S Chult. 2008. “Exploring Network Structure, Dynamics, and Function Using NetworkX.” Los Alamos National Lab (LANL), Los Alamos, NM (United States). Hanocka, Rana, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. 2019. “MeshCNN: A Network with an Edge.” ACM Transactions on Graphics 38 (4): 1–12. Joslyn, Cliff A, Sinan G Aksoy, Tiffany J Callahan, Lawrence E Hunter, Brett Jefferson, Brenda Praggastis, Emilie Purvine, and Ignacio J Tripodi. 2021. “Hypernetwork Science: From Multidimensional Networks to Computational Topology.” In Unifying Themes in Complex Systems x: Proceedings of the Tenth International Conference on Complex Systems, 377–92. Springer. Kim, Vladimir G, Yaron Lipman, Xiaobai Chen, and Thomas Funkhouser. 2010. “Möbius Transformations for Global Intrinsic Symmetry Analysis.” Computer Graphics Forum 29 (5): 1689–1700. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2017. “Imagenet Classification with Deep Convolutional Neural Networks.” Communications of the ACM 60 (6): 84–90. Lian, Z., A. Godil, B. Bustos, M Daoudi, J. Hermans, S. Kawamura, Y. Kurita, G. Lavoua, P. Dp Suetens, et al. 2011. “Shape Retrieval on Non-Rigid 3D Watertight Meshes.” In Eurographics Workshop on 3d Object Retrieval (3DOR). Citeseer. Maron, Haggai, Meirav Galun, Noam Aigerman, Miri Trope, Nadav Dym, Ersin Yumer, Vladimir G Kim, and Yaron Lipman. 2017. “Convolutional Neural Networks on Surfaces via Seamless Toric Covers.” ACM Transactions on Graphics 36 (4): 71–71. Mejia, Daniel, Oscar Ruiz-Salguero, and Carlos A. Cadavid. 2017. “Spectral-Based Mesh Segmentation.” International Journal on Interactive Design and Manufacturing 11 (3): 503–14. Milano, Francesco, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, and Luca Carlone. 2020. “Primal-Dual Mesh Convolutional Neural Networks.” Conference on Neural Information Processing Systems 33: 952–63. Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. “Automatic Differentiation in PyTorch.” In NIPS Workshop. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Jmlr 12: 2825–30. Smirnov, Dmitriy, and Justin Solomon. 2021. “HodgeNet: Learning Spectral Geometry on Triangle Meshes.” ACM Transactions on Graphics 40 (4): 1–11. Wang, Yunhai, Shmulik Asafi, Oliver Van Kaick, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen. 2012. “Active Co-Analysis of a Set of Shapes.” ACM Transactions on Graphics 31 (6): 1–10. Williams, Francis. 2022. “Point Cloud Utils.” 水密（watertight）网格通常描述由一个封闭曲面组成的网格，水密网格不包含孔洞并且内部定义明确↩︎ The difficulty in these datasets is controlled by the compactness degree of the graph clusters; clusters in the ‘easy’ data have more in-between cluster connections, while clusters in the `hard’ data are more isolated (Bianchi, Gallicchio, and Micheli 2022).↩︎ "],["related-work.html", "第 10 章 Related work 10.1 Graph-based models 10.2 Higher-order deep learning models 10.3 Attention-based models 10.4 Graph-based pooling 10.5 Applied algebraic topology", " 第 10 章 Related work Topological deep learning (TDL) has recently emerged as a new research frontier that lies at the intersection of several areas, including geometric and topological machine learning, and network science. To demonstrate where TDL fits in the existing literature, we review a broad spectrum of prior works, and categorize them into graph-based models, higher-order deep learning models, graph-based pooling, attention-based models, and applied algebraic topology. 10.1 Graph-based models Graph-based models have been widely used for modeling pairwise interactions (edges) between elements (vertices) of different systems, including social systems (e.g., social network analysis) and biological systems (e.g., protein-protein interactions), see (Knoke and Yang 2019; Jha, Saha, and Singh 2022). Based on their edge or vertex properties, graphs can be classified as unweighted graphs (unweighted edges), weighted graphs (weighted edges), signed graphs (signed edges), undirected or directed graphs (undirected or directed edges), and spatio-temporal graphs (spatio-temporal vertices), as discussed in (Goyal and Ferrara 2018; Zonghan Wu et al. 2020). Each of these graph types can be combined with neural networks to form graph neural networks and model different interactions in various systems (Goyal and Ferrara 2018; Zonghan Wu et al. 2020). For example, unweighted and undirected graph-based models have been used for omic data mapping (Amar and Shamir 2014) and mutual friendship detection in social networks (Tabassum et al. 2018); weighted graph-based models have been widely used with systems related to traffic forecasting (Halaoui 2010; Q. Zhang et al. 2018) and epidemiological modeling/forecasting (Linka et al. 2020; Manrı́quez, Guerrero-Nancuante, and Taramasco 2021); signed graph-based models are suitable for tasks such as segmentation (Bailoni et al. 2022) and clustering (Kunegis et al. 2010; Gallier 2016); spatio-temporal graph-based models can describe systems that are spatio-temporal in nature, such as human activity and different types of motion (Yan, Xiong, and Lin 2018; Bhattacharya et al. 2020; Plizzari, Cannici, and Matteucci 2021). As graph-based approaches that utilize single-layer or monolayer graphs cannot model multiple types of relations between vertices in a network (Goyal and Ferrara 2018; Zonghan Wu et al. 2020), multilayer or multiplex networks have been proposed (Kivelä et al. 2014; W. Zhang et al. 2020; Chang et al. 2022). Similar to monolayer graphs, multiplex networks contain vertices and edges, but the edges exist in separate layers, where each layer represents a specific type of interaction or relation. Multiplex networks have been used in various applications, including multilayer modeling of the human brain (De Domenico 2017; Anand and Chung 2023) and online gaming (Chang et al. 2022). All these types of networks can only model pairwise relations between vertices, motivating the need for higher-order networks, as discussed in Section 2.2. 10.2 Higher-order deep learning models In recent years, there has been an increasing interest in higher-order networks (Mendel 1991; Battiston et al. 2020; Bick et al. 2021) due to the ability of these networks to adequately capture higher-order interactions. Hodge-theoretic approaches, message passing schemes, and skip connections have been developed for higher-order networks in the signal processing and deep learning literature. A Hodge-theoretic approach (Lim 2020) over simplicial complexes has been introduced by (Barbarossa and Sardellitti 2020a; Schaub et al. 2021). This effort has been extended to hypergraphs by (Barbarossa and Tsitsvero 2016; Schaub et al. 2021) and to cell complexes by (T. Mitchell Roddenberry, Schaub, and Hajij 2022; Sardellitti, Barbarossa, and Testa 2021). The work of (T. Mitchell Roddenberry and Segarra 2019) has defined an edge-based convolutional neural network by exploiting the 1-Hodge Laplacian operator for linear filtering (Barbarossa, Sardellitti, and Ceci 2018; Schaub and Segarra 2018; Barbarossa and Sardellitti 2020a, 2020b; Schaub et al. 2021). Convolutional operators and message-passing algorithms have been developed for higher-order neural networks. For example, a convolutional operator on hypergraphs has been proposed by (Arya and Worring 2018; Feng et al. 2019; J. Jiang et al. 2019) and has been investigated further by (J. Jiang et al. 2019; Y. Gao et al. 2020; S. Bai, Zhang, and Torr 2021; J. Bai et al. 2021; L. Giusti, Battiloro, Testa, et al. 2022; H. Wu and Ng 2022; Gong, Higham, and Zygalakis 2023). A unifying framework for learning on graphs and hypergraphs has been proposed recently in (Jing Huang and Yang 2021). The authors in (Y. Gao et al. 2022) have introduced the so-called general hypergraph neural networks, which constitute a multi-modal/multi-type data correlation modeling framework. As for message passing on complexes, the work of (Hajij, Istvan, and Zamzmi 2020) has introduced a higher-order message-passing framework that encompasses those proposed by (Gilmer et al. 2017; Bunch et al. 2020; Ebli, Defferrard, and Spreemann 2020; Hayhoe et al. 2022) and has utilized various local neighborhood aggregation schemes. In (E. C. Mitchell et al. 2022), recurrent simplicial neural networks have been proposed and applied to trajectory prediction. The authors in (Calmon, Schaub, and Bianconi 2022) have addressed the challenge of processing signals supported on multiple cell dimensions concurrently, by introducing a coupling multi-signal approach on higher-order networks that utilizes the Dirac operator. Several simplicial and cellular neural networks have been introduced recently, including (Burns and Fukai 2023; Bodnar et al. 2021; T. Mitchell Roddenberry, Schaub, and Hajij 2022; Sardellitti, Barbarossa, and Testa 2021; Sardellitti and Barbarossa 2022; Battiloro et al. 2023; Yang and Isufi 2023). For more details, the reader is referred to the recent survey of (Papillon et al. 2023) on TDL. A generalization of skip connections (Ronneberger, Fischer, and Brox 2015; He et al. 2016) to simplicial complexes has been introduced by (Hajij, Ramamurthy, et al. 2022), which allows the training of higher-order deep neural networks. The authors in (Morris et al. 2019) have proposed a higher-order graph neural network that takes into account higher-order graph structures at multiple scales. While these methods allow for multi-way hierarchical coupling, the coupling is isotropic and weight differences within a particular multi-way connection can not be learned. These limitations can be alleviated by attention-based models. Higher-order models have achieved promising performance in several real-world applications, including link prediction (Hajij, Ramamurthy, et al. 2022; Piaggesi, Panisson, and Petri 2022; Yuzhou Chen, Gel, and Poor 2022), action recognition (C. Wang et al. 2023), visual classification (Shi et al. 2018), optimal homology generator detection (Keros, Nanda, and Subr 2022), time series (Andrea Santoro et al. 2023), dynamical systems (Majhi, Perc, and Ghosh 2022), spectral clustering (Reddy, Chepuri, and Borgnat 2023), node classification (Hajij, Ramamurthy, et al. 2022), and trajectory prediction (Benson et al. 2018; T. Mitchell Roddenberry, Glaze, and Segarra 2021). 10.3 Attention-based models Real-world relational data is large, unstructured, sparse and noisy. As a result, graph neural networks (GNNs) may learn suboptimal data representations, and therefore may exhibit compromised performance (Zonghan Wu et al. 2020; Asif et al. 2021; Dai, Aggarwal, and Wang 2021). To address these issues, various attention mechanisms (Chaudhari et al. 2021) have been incorporated in GNNs, which allow to learn neural architectures that detect the most relevant parts of a given graph while ignoring irrelevant parts. Based on the used attention mechanism, existing graph attention approaches can be divided into weight-based attention, similarity-based attention, and attention-guided walk (J. B. Lee et al. 2019). The majority of attention-based mechanisms, with the exception of (S. Bai, Zhang, and Torr 2021; E.-S. Kim et al. 2020; Georgiev, Brockschmidt, and Allamanis 2022; L. Giusti, Battiloro, Testa, et al. 2022; L. Giusti, Battiloro, Di Lorenzo, et al. 2022; Goh, Bodnar, and Lio 2022), are designed for graphs. For example, the attention model proposed by (Goh, Bodnar, and Lio 2022) is a generalization of the graph attention model of (Veličković et al. 2018). In (L. Giusti, Battiloro, Di Lorenzo, et al. 2022), the authors have utilized a model based on Hodge decomposition, similar to the one suggested in (T. Mitchell Roddenberry, Glaze, and Segarra 2021), to introduce an attention model for simplicial complexes. The hypergraph attention models introduced in (E.-S. Kim et al. 2020; S. Bai, Zhang, and Torr 2021) provide alternative generalizations of the graph attention model of (Veličković et al. 2018). The aforementioned attention models neither allow nor combine higher-order attention blocks of entities of different dimensions. This limits the space of neural architectures and the scope of applications of existing attention models. 10.4 Graph-based pooling Several attempts have been made to emulate the success of image-based pooling layers in the context of graphs. Some of the early work employs popular graph clustering algorithms (Kushnir, Galun, and Brandt 2006; Dhillon, Guan, and Kulis 2007) to achieve graph-based pooling architectures (Bruna et al. 2014). Coarsening operations have been applied to graphs to attain the invariance properties needed in learning tasks (Ying et al. 2018; Mesquita, Souza, and Kaski 2020; H. Gao, Liu, and Ji 2021). The current state-of-the-art graph-based pooling approaches mostly rely on dynamically learning the pooling needed for the learning task (Grattarola et al. 2022). This includes spectral methods (Ma et al. 2019), clustering methods such as DiffPool (Ying et al. 2018) and MinCut (Bianchi, Grattarola, and Alippi 2020), top-K methods (H. Gao and Ji 2019; J. Lee, Lee, and Kang 2019; Z. Zhang et al. 2021), and hierarchical graph pooling (Jingjia Huang et al. 2019; J. Lee, Lee, and Kang 2019; Z. Zhang et al. 2019, 2021; J. Li et al. 2020; Pang, Zhao, and Li 2021). Pooling on higher-order networks remains unstudied, with the exception of a general simplicial complex pooling strategy developed by (Cinque, Battiloro, and Di Lorenzo 2022) along the lines of the proposal made by (Grattarola et al. 2022). 10.5 Applied algebraic topology Although algebraic topology (Hatcher 2005) is a relatively old field, applications of this field have only recently started to crystallize (G. Carlsson 2009; Edelsbrunner and Harer 2010). Indeed, topological constructions have been found to be natural tools for the formulation of longstanding problems in many fields. For instance, persistent homology (Edelsbrunner and Harer 2010) has been successful at finding solutions to various complex data problems (Boyell and Ruston 1963; Kweon and Kanade 1994; Bajaj, Pascucci, and Schikore 1997; Attene, Biasotti, and Spagnuolo 2003; Carr, Snoeyink, and Panne 2004; H. Lee et al. 2011b, 2011a, 2012a, 2012b; Dabaghian et al. 2012; Nicolau, Levine, and Carlsson 2011; Lum et al. 2013; C. Giusti, Ghrist, and Bassett 2016; Curto 2017; Rosen et al. 2017). Recent years have witnessed increased interest in the role of topology in machine learning and data science (Hensel, Moor, and Rieck 2021; Dey and Wang 2022b). Topology-based machine learning models have been applied in many areas, including topological signatures of data (Biasotti et al. 2008; G. Carlsson et al. 2005; Rieck and Leitte 2015), neuroscience (H. Lee et al. 2011b, 2011a, 2012a, 2012b; Dabaghian et al. 2012; C. Giusti, Ghrist, and Bassett 2016; Curto 2017), bioscience (DeWoskin et al. 2010; Nicolau, Levine, and Carlsson 2011; Chan, Carlsson, and Rabadan 2013; Taylor et al. 2015; Topaz, Ziegelmeier, and Halverson 2015; Lo and Park 2016), the study of graphs (Horak, Maletić, and Rajković 2009; Weinan, Jianfeng, and Yuan 2013; Bampasidou and Gentimis 2014; Carstens and Horadam 2013; Petri et al. 2013a, 2013b; Rieck, Bock, and Borgwardt 2019; Hajij and Rosen 2020), time series forcasting (Zeng et al. 2021), Trojan detection (Hu et al. 2022), image segmentation (Hu et al. 2019), 3D reconstruction (Waibel et al. 2022), and time-varying setups (Edelsbrunner et al. 2004; Perea et al. 2015; Maletić, Zhao, and Rajković 2016; Rieck et al. 2020). Topological data analysis (TDA) (G. Carlsson 2009; Edelsbrunner and Harer 2010; Ghrist 2014; Love et al. 2023b; Dey and Wang 2022b) has emerged as a scientific area that harnesses topological tools to analyze data and develop machine learning algorithms. TDA has found many applications in machine learning, including enhancing existing machine learning models (BenTaieb and Hamarneh 2016; Hofer et al. 2017; Clough et al. 2019; Gabrielsson et al. 2020; F. Wang et al. 2020; Leventhal et al. 2023), improving the explainability of deep learning models (G. Carlsson and Gabrielsson 2020; Elhamdadi, Canavan, and Rosen 2021; Love et al. 2023a), dimensionality reduction (Moor et al. 2020), filtration learning (Hofer et al. 2020), and topological layers constructions (K. Kim et al. 2020). A notable research trend has been the vectorization of persistence diagrams. Vector representations of persistence diagrams are constructed in order to be utilized in downstream machine learning tasks. These methods include Betti curves (Umeda 2017), persistence landscapes (Bubenik 2015), persistence images (Adams et al. 2017), and other vectorization constructions (Y.-C. Chen et al. 2015; Kusano, Hiraoka, and Fukumizu 2016; Berry et al. 2020). A unification of these methods has been proposed recently in (Carriere et al. 2020). Our work introduces combinatorial complexes (CCs) as a generalized higher-order network on which deep learning models can be defined and studied in a unifying manner. Hence, our work expands TDA by formalizing deep learning notions in topological terms and by realizing constructions in TDA, e.g., mapper (Singh et al. 2007), in terms of our TDL framework. The construction of CCs and of combinatorial complex neural networks (CCNNs), which are neural networks defined on CCs, is inspired by classical notions in algebraic topology (Hatcher 2005) and in topological quantum field theory (Turaev 2016), and by recent advances in TDA (Collins et al. 2004; G. Carlsson et al. 2005, 2008; E. Carlsson, Carlsson, and De Silva 2006; G. Carlsson and Mémoli 2008; G. Carlsson and Zomorodian 2009; G. Carlsson 2009) as applied to machine learning (Pun, Xia, and Lee 2018; Dey and Wang 2022b). 参考文献 Adams, Henry, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. 2017. “Persistence Images: A Stable Vector Representation of Persistent Homology.” Jmlr 18 (1): 218–52. Amar, David, and Ron Shamir. 2014. “Constructing Module Maps for Integrated Analysis of Heterogeneous Biological Networks.” Nucleic Acids Research 42 (7): 4208–19. Anand, D. V., and Moo K. Chung. 2023. “Hodge Laplacian of Brain Networks.” IEEE Transactions on Medical Imaging. Arya, Devanshu, and Marcel Worring. 2018. “Exploiting Relational Information in Social Networks Using Geometric Deep Learning on Hypergraphs.” In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, 117–25. Asif, Nurul A., Yeahia Sarker, Ripon K. Chakrabortty, Michael J. Ryan, Md. Hafiz Ahamed, Dip K. Saha, Faisal R. Badal, et al. 2021. “Graph Neural Network: A Comprehensive Review on Non-Euclidean Space.” IEEE Access 9: 60588–606. https://doi.org/10.1109/ACCESS.2021.3071274. Attene, Marco, Silvia Biasotti, and Michela Spagnuolo. 2003. “Shape Understanding by Contour-Driven Retiling.” The Visual Computer 19 (2): 127–38. Bai, Junjie, Biao Gong, Yining Zhao, Fuqiang Lei, Chenggang Yan, and Yue Gao. 2021. “Multi-Scale Representation Learning on Hypergraph for 3D Shape Retrieval and Recognition.” IEEE Transactions on Image Processing 30: 5327–38. Bai, Song, Feihu Zhang, and Philip H. S. Torr. 2021. “Hypergraph Convolution and Hypergraph Attention.” Pattern Recognition 110: 107637. Bailoni, Alberto, Constantin Pape, Nathan Hütsch, Steffen Wolf, Thorsten Beier, Anna Kreshuk, and Fred A Hamprecht. 2022. “GASP, a Generalized Framework for Agglomerative Clustering of Signed Graphs and Its Application to Instance Segmentation.” In Cvpr, 11645–55. Bajaj, Chandrajit L., Valerio Pascucci, and Daniel R. Schikore. 1997. “The Contour Spectrum.” In Proceedings of the 8th Conference on Visualization ’97, 167–ff. IEEE Computer Society Press. Bampasidou, Maria, and Thanos Gentimis. 2014. “Modeling Collaborations with Persistent Homology.” arXiv Preprint arXiv:1403.5346 abs/1403.5346. Barbarossa, Sergio, and Stefania Sardellitti. 2020a. “Topological Signal Processing over Simplicial Complexes.” IEEE Transactions on Signal Processing 68: 2992–3007. ———. 2020b. “Topological Signal Processing: Making Sense of Data Building on Multiway Relations.” IEEE Signal Processing Magazine 37 (6): 174–83. Barbarossa, Sergio, Stefania Sardellitti, and Elena Ceci. 2018. “Learning from Signals Defined over Simplicial Complexes.” In 2018 IEEE Data Science Workshop (DSW), 51–55. IEEE. Barbarossa, Sergio, and Mikhail Tsitsvero. 2016. “An Introduction to Hypergraph Signal Processing.” In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6425–29. IEEE. Battiloro, Claudio, Stefania Sardellitti, Sergio Barbarossa, and Paolo Di Lorenzo. 2023. “Topological Signal Processing over Weighted Simplicial Complexes.” arXiv Preprint arXiv:2302.08561. Battiston, Federico, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. 2020. “Networks Beyond Pairwise Interactions: Structure and Dynamics.” Physics Reports 874: 1–92. Benson, Austin R., Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. 2018. “Simplicial Closure and Higher-Order Link Prediction.” Proceedings of the National Academy of Sciences 115 (48): E11221–30. BenTaieb, Aicha, and Ghassan Hamarneh. 2016. “Topology Aware Fully Convolutional Networks for Histology Gland Segmentation.” In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, 460–68. Springer. Berry, Eric, Yen-Chi Chen, Jessi Cisewski-Kehe, and Brittany Terese Fasy. 2020. “Functional Summaries of Persistence Diagrams.” J. Appl. Comput. Topol. 4 (2): 211–62. Bhattacharya, Uttaran, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, and Dinesh Manocha. 2020. “STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits.” Proceedings of the AAAI Conference on Artificial Intelligence 34 (02): 1342–50. https://doi.org/10.1609/aaai.v34i02.5490. Bianchi, Filippo Maria, Daniele Grattarola, and Cesare Alippi. 2020. “Spectral Clustering with Graph Neural Networks for Graph Pooling.” In Icml, 874–83. PMLR. Biasotti, Silvia, Leila De Floriani, Bianca Falcidieno, Patrizio Frosini, Daniela Giorgi, Claudia Landi, Laura Papaleo, and Michela Spagnuolo. 2008. “Describing Shapes by Geometrical-Topological Properties of Real Functions.” ACM Computing Surveys (CSUR) 40 (4): 12. Bick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T Schaub. 2021. “What Are Higher-Order Networks?” arXiv Preprint arXiv:2104.11329. Bodnar, Cristian, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. 2021. “Weisfeiler and Lehman Go Cellular: CW Networks.” In Advances in Neural Information Processing Systems. Boyell, Roger L., and Henry Ruston. 1963. “Hybrid Techniques for Real-Time Radar Simulation.” In Proceedings of the November 12-14, 1963, Fall Joint Computer Conference, 445–58. ACM. Bruna, Joan, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. “Spectral Networks and Locally Connected Networks on Graphs.” In Proceedings of the 2nd International Conference on Learning Representations, edited by Yoshua Bengio and Yann LeCun. ICLR 2014. Banff, AB, Canada. Bubenik, Peter. 2015. “Statistical Topological Data Analysis Using Persistence Landscapes.” Jmlr 16 (1): 77–102. Bunch, Eric, Qian You, Glenn Fung, and Vikas Singh. 2020. “Simplicial 2-Complex Convolutional Neural Nets.” NeurIPS Workshop on Topological Data Analysis and Beyond. Burns, Thomas F., and Tomoki Fukai. 2023. “Simplicial Hopfield Networks.” In The Eleventh International Conference on Learning Representations. Calmon, Lucille, Michael T. Schaub, and Ginestra Bianconi. 2022. “Higher-Order Signal Processing with the Dirac Operator.” In Asilomar Conference on Signals, Systems, and Computers. Carlsson, Erik, Gunnar Carlsson, and Vin De Silva. 2006. “An Algebraic Topological Method for Feature Identification.” International Journal of Computational Geometry &amp; Applications 16 (04): 291–314. Carlsson, Gunnar. 2009. “Topology and Data.” Bulletin of the American Mathematical Society 46 (2): 255–308. Carlsson, Gunnar, and Rickard Brüel Gabrielsson. 2020. “Topological Approaches to Deep Learning.” In Topological Data Analysis: The Abel Symposium 2018, 119–46. Springer; Springer. Carlsson, Gunnar, Tigran Ishkhanov, Vin De Silva, and Afra Zomorodian. 2008. “On the Local Behavior of Spaces of Natural Images.” Ijcv 76 (1): 1–12. Carlsson, Gunnar, and Facundo Mémoli. 2008. “Persistent Clustering and a Theorem of J. Kleinberg.” arXiv Preprint arXiv:0808.2241. Carlsson, Gunnar, and Afra Zomorodian. 2009. “The Theory of Multidimensional Persistence.” Discrete &amp; Computational Geometry 42 (1): 71–93. Carlsson, Gunnar, Afra Zomorodian, Anne Collins, and Leonidas J Guibas. 2005. “Persistence Barcodes for Shapes.” International Journal of Shape Modeling 11 (02): 149–87. Carr, Hamish, Jack Snoeyink, and Michiel van de Panne. 2004. “Simplifying Flexible Isosurfaces Using Local Geometric Measures.” In IEEE Visualization, 497–504. IEEE. Carriere, Mathieu, Frederic Chazal, Yuichi Ike, Theo Lacombe, Martin Royer, and Yuhei Umeda. 2020. “PersLay: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2786–96. PMLR. Carstens, C. J., and K. J. Horadam. 2013. “Persistent Homology of Collaboration Networks.” Mathematical Problems in Engineering 2013. Chan, Joseph Minhow, Gunnar Carlsson, and Raul Rabadan. 2013. “Topology of Viral Evolution.” Proceedings of the National Academy of Sciences 110 (46): 18566–71. Chang, Yaomin, Lin Shu, Erxin Du, Chuan Chen, Ziyang Zhang, Zibin Zheng, Yuzhao Huang, and Xingxing Xing. 2022. “GraphRR: A Multiplex Graph Based Reciprocal Friend Recommender System with Applications on Online Gaming Service.” Knowledge-Based Systems 251: 109187. Chaudhari, Sneha, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. 2021. “An Attentive Survey of Attention Models.” ACM Transactions on Intelligent Systems and Technology (TIST) 12 (5): 1–32. Chen, Yen-Chi, Daren Wang, Alessandro Rinaldo, and Larry Wasserman. 2015. “Statistical Analysis of Persistence Intensity Functions.” arXiv Preprint arXiv:1510.02502. Chen, Yuzhou, Yulia R. Gel, and H. Vincent Poor. 2022. “BScNets: Block Simplicial Complex Neural Networks.” Proceedings of the AAAI Conference on Artificial Intelligence 36 (6): 6333–41. https://doi.org/10.1609/aaai.v36i6.20583. Cinque, Domenico Mattia, Claudio Battiloro, and Paolo Di Lorenzo. 2022. “Pooling Strategies for Simplicial Convolutional Networks.” arXiv Preprint arXiv:2210.05490. Clough, James R., Ilkay Oksuz, Nicholas Byrne, Julia A. Schnabel, and Andrew P. King. 2019. “Explicit Topological Priors for Deep-Learning Based Image Segmentation Using Persistent Homology.” In Information Processing in Medical Imaging: 26th International Conference, IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26, 16–28. Springer. Collins, Anne, Afra Zomorodian, Gunnar Carlsson, and Leonidas J Guibas. 2004. “A Barcode Shape Descriptor for Curve Point Cloud Data.” Computers &amp; Graphics 28 (6): 881–94. Curto, Carina. 2017. “What Can Topology Tell Us about the Neural Code?” Bulletin of the American Mathematical Society 54 (1): 63–78. Dabaghian, Y., F. Mémoli, L. Frank, and G. Carlsson. 2012. “A Topological Paradigm for Hippocampal Spatial Map Formation Using Persistent Homology.” PLoS Computational Biology 8 (8): e1002581. Dai, Enyan, Charu Aggarwal, and Suhang Wang. 2021. “NRGNN: Learning a Label Noise Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs.” In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 227–36. De Domenico, Manlio. 2017. “Multilayer modeling and analysis of human brain networks.” GigaScience 6 (5). https://doi.org/10.1093/gigascience/gix004. DeWoskin, D., J. Climent, I. Cruz-White, M. Vazquez, C. Park, and J. Arsuaga. 2010. “Applications of Computational Homology to the Analysis of Treatment Response in Breast Cancer Patients.” Topology and Its Applications 157 (1): 157–64. ———. 2022b. Computational Topology for Data Analysis. Cambridge University Press. Dhillon, Inderjit S., Yuqiang Guan, and Brian Kulis. 2007. “Weighted Graph Cuts Without Eigenvectors a Multilevel Approach.” Pami 29 (11): 1944–57. Ebli, Stefania, Michaël Defferrard, and Gard Spreemann. 2020. “Simplicial Neural Networks.” NeurIPS Workshop on Topological Data Analysis and Beyond. Edelsbrunner, Herbert, and John Harer. 2010. Computational Topology: An Introduction. American Mathematical Soc. Edelsbrunner, Herbert, John Harer, Ajith Mascarenhas, and Valerio Pascucci. 2004. “Time-Varying Reeb Graphs for Continuous Space-Time Data.” In Proceedings of the Twentieth Annual Symposium on Computational Geometry, 366–72. ACM. Elhamdadi, Hamza, Shaun Canavan, and Paul Rosen. 2021. “AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing.” IEEE Transactions on Visualization and Computer Graphics 28 (1): 769–79. Feng, Yifan, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. “Hypergraph Neural Networks.” Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 3558–65. Gabrielsson, Rickard Brüel, Bradley J. Nelson, Anjan Dwaraknath, and Primoz Skraba. 2020. “A Topology Layer for Machine Learning.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, edited by Silvia Chiappa and Roberto Calandra, 108:1553–63. #PMLR#. PMLR. Gallier, Jean. 2016. “Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: A Survey.” arXiv Preprint arXiv:1601.04692. Gao, Hongyang, and Shuiwang Ji. 2019. “Graph U-Nets.” In Icml, 2083–92. PMLR. Gao, Hongyang, Yi Liu, and Shuiwang Ji. 2021. “Topology-Aware Graph Pooling Networks.” Pami 43 (12): 4512–18. Gao, Yue, Yifan Feng, Shuyi Ji, and Rongrong Ji. 2022. “HGNN+: General Hypergraph Neural Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence. Gao, Yue, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing Zou. 2020. “Hypergraph Learning: Methods and Practices.” IEEE Transactions on Pattern Analysis and Machine Intelligence. Georgiev, Dobrik, Marc Brockschmidt, and Miltiadis Allamanis. 2022. “HEAT: Hyperedge Attention Networks.” Transactions on Machine Learning Research. Ghrist, Robert W. 2014. Elementary Applied Topology. Vol. 1. Createspace Seattle. Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. “Neural Message Passing for Quantum Chemistry.” In International Conference on Machine Learning. Giusti, Chad, Robert Ghrist, and Danielle S. Bassett. 2016. “Two’s Company, Three (or More) Is a Simplex: Algebraic-Topological Tools for Understanding Higher-Order Structure in Neural Data.” Journal of Computational Neuroscience 41: 1. Giusti, Lorenzo, Claudio Battiloro, Paolo Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. 2022. “Simplicial Attention Networks.” arXiv Preprint arXiv:2203.07485. Giusti, Lorenzo, Claudio Battiloro, Lucia Testa, Paolo Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. 2022. “Cell Attention Networks.” arXiv Preprint arXiv:2209.08179. Goh, Christopher Wei Jin, Cristian Bodnar, and Pietro Lio. 2022. “Simplicial Attention Networks.” In ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Gong, Xue, Desmond J. Higham, and Konstantinos Zygalakis. 2023. “Generative Hypergraph Models and Spectral Embedding.” Scientific Reports 13 (1): 540. Goyal, Palash, and Emilio Ferrara. 2018. “Graph Embedding Techniques, Applications, and Performance: A Survey.” Knowledge-Based Systems 151: 78–94. Grattarola, Daniele, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. 2022. “Understanding Pooling in Graph Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems. Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Hajij, Mustafa, Karthikeyan Natesan Ramamurthy, Aldo Saenz, and Ghada Zamzmi. 2022. “High Skip Networks: A Higher Order Generalization of Skip Connections.” In ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Hajij, Mustafa, and Paul Rosen. 2020. “An Efficient Data Retrieval Parallel Reeb Graph Algorithm.” Algorithms 13 (10): 258. Halaoui, Hatem F. 2010. “Smart Traffic Online System (STOS): Presenting Road Networks with Time-Weighted Graphs.” In 2010 International Conference on Information Society, 349–56. IEEE. Hatcher, Allen. 2005. Algebraic Topology. Cambridge University Press. Hayhoe, Mikhail, Hans Riess, Victor M Preciado, and Alejandro Ribeiro. 2022. “Stable and Transferable Hyper-Graph Neural Networks.” arXiv Preprint arXiv:2211.06513. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. https://doi.org/10.1109/CVPR.2016.90. Hensel, Felix, Michael Moor, and Bastian Rieck. 2021. “A Survey of Topological Machine Learning Methods.” Frontiers in Artificial Intelligence 4: 681108. Hofer, Christoph, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. 2020. “Graph Filtration Learning.” In International Conference on Machine Learning, 4314–23. PMLR. Hofer, Christoph, Roland Kwitt, Marc Niethammer, and Andreas Uhl. 2017. “Deep Learning with Topological Signatures.” In Neurips, 1634–44. Horak, Danijela, Slobodan Maletić, and Milan Rajković. 2009. “Persistent Homology of Complex Networks.” Journal of Statistical Mechanics: Theory and Experiment, P03034. Hu, Xiaoling, Fuxin Li, Dimitris Samaras, and Chao Chen. 2019. “Topology-Preserving Deep Image Segmentation.” In Advances in Neural Information Processing Systems. Vol. 32. Curran Associates, Inc. Hu, Xiaoling, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. 2022. “Trigger Hunting with a Topological Prior for Trojan Detection.” In International Conference on Learning Representations. Huang, Jingjia, Zhangheng Li, Nannan Li, Shan Liu, and Ge Li. 2019. “AttPool: Towards Hierarchical Feature Representation in Graph Convolutional Networks via Attention Mechanism.” In Iccv, 6480–89. Huang, Jing, and Jie Yang. 2021. “UniGNN: A Unified Framework for Graph and Hypergraph Neural Networks.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI. Jha, Kanchan, Sriparna Saha, and Hiteshi Singh. 2022. “Prediction of Protein–Protein Interaction Using Graph Neural Networks.” Scientific Reports 12 (1): 1–12. Jiang, Jianwen, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. 2019. “Dynamic Hypergraph Neural Networks.” In IJCAI, 2635–41. Keros, Alexandros D., Vidit Nanda, and Kartic Subr. 2022. “Dist2Cycle: A Simplicial Neural Network for Homology Localization.” Proceedings of the AAAI Conference on Artificial Intelligence 36 (7): 7133–42. https://doi.org/10.1609/aaai.v36i7.20673. Kim, Eun-Sol, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak Zhang. 2020. “Hypergraph Attention Networks for Multimodal Learning.” In Cvpr, 14581–90. Kim, Kwangho, Jisu Kim, Manzil Zaheer, Joon Kim, Frédéric Chazal, and Larry Wasserman. 2020. “Pllay: Efficient Topological Layer Based on Persistent Landscapes.” Advances in Neural Information Processing Systems 33: 15965–77. Kivelä, Mikko, Alex Arenas, Marc Barthelemy, James P Gleeson, Yamir Moreno, and Mason A Porter. 2014. “Multilayer Networks.” Journal of Complex Networks 2 (3): 203–71. Knoke, David, and Song Yang. 2019. Social Network Analysis. SAGE publications. Kunegis, Jérôme, Stephan Schmidt, Andreas Lommatzsch, Jürgen Lerner, Ernesto W De Luca, and Sahin Albayrak. 2010. “Spectral Analysis of Signed Graphs for Clustering, Prediction and Visualization.” In Proceedings of the 2010 SIAM International Conference on Data Mining, 559–70. SIAM. Kusano, Genki, Yasuaki Hiraoka, and Kenji Fukumizu. 2016. “Persistence Weighted Gaussian Kernel for Topological Data Analysis.” In Icml, 2004–13. Kushnir, Dan, Meirav Galun, and Achi Brandt. 2006. “Fast Multiscale Clustering and Manifold Identification.” Pattern Recognition 39 (10): 1876–91. Kweon, In So, and Takeo Kanade. 1994. “Extracting Topographic Terrain Features from Elevation Maps.” CVGIP: Image Understanding 59 (2): 171–82. Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Boong-Nyun Kim, and Dong Soo Lee. 2011a. “Computing the Shape of Brain Networks Using Graph Filtration and Gromov-Hausdorff Metric.” International Conference on Medical Image Computing and Computer Assisted Intervention, 302–9. Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Bung-Nyun Kim, and Dong Soo Lee. 2011b. “Discriminative Persistent Homology of Brain Networks.” IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 841–44. Lee, Hyekyoung, Hyejin Kang, Moo K. Chung, Bung-Nyun Kim, and Dong Soo Lee. 2012a. “Persistent Brain Network Homology from the Perspective of Dendrogram.” IEEE Transactions on Medical Imaging 31 (12): 2267–77. ———. 2012b. “Weighted Functional Brain Network Modeling via Network Filtration.” NIPS Workshop on Algebraic Topology and Machine Learning. Lee, John Boaz, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. 2019. “Attention Models in Graphs: A Survey.” ACM Transactions on Knowledge Discovery from Data 13 (6): 1–25. Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. “Self-Attention Graph Pooling.” In Icml, 3734–43. PMLR. Leventhal, Samuel, Attila Gyulassy, Mark Heimann, and Valerio Pascucci. 2023. “Exploring Classification of Topological Priors with Machine Learning for Feature Extraction.” IEEE Transactions on Visualization and Computer Graphics. Li, Juanhui, Yao Ma, Yiqi Wang, Charu Aggarwal, Chang-Dong Wang, and Jiliang Tang. 2020. “Graph Pooling with Representativeness.” In 2020 IEEE International Conference on Data Mining (ICDM), 302–11. IEEE. Lim, Lek-Heng. 2020. “Hodge Laplacians on Graphs.” SIAM Review 62 (3): 685–715. Linka, Kevin, Mathias Peirlinck, Francisco Sahli Costabal, and Ellen Kuhl. 2020. “Outbreak Dynamics of COVID-19 in Europe and the Effect of Travel Restrictions.” Computer Methods in Biomechanics and Biomedical Engineering 23 (11): 710–17. Lo, Derek, and Briton Park. 2016. “Modeling the Spread of the Zika Virus Using Topological Data Analysis.” arXiv Preprint arXiv:1612.03554. Love, Ephy R., Benjamin Filippenko, Vasileios Maroulas, and Gunnar Carlsson. 2023a. “Topological Convolutional Layers for Deep Learning.” Jmlr 24 (59): 1–35. ———. 2023b. “Topological Convolutional Layers for Deep Learning.” Jmlr 24 (59): 1–35. Lum, P. Y., G. Singh, A. Lehman, T. Ishkanov, Mikael Vejdemo-Johansson, M. Alagappan, J. Carlsson, and G. Carlsson. 2013. “Extracting Insights from the Shape of Complex Data Using Topology.” Scientific Reports 3: 1236. Ma, Yao, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019. “Graph Convolutional Networks with Eigenpooling.” In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 723–31. Majhi, Soumen, Matjaž Perc, and Dibakar Ghosh. 2022. “Dynamics on Higher-Order Networks: A Review.” Journal of the Royal Society Interface 19 (188): 20220043. Maletić, Slobodan, Yi Zhao, and Milan Rajković. 2016. “Persistent Topological Features of Dynamical Systems.” Chaos: An Interdisciplinary Journal of Nonlinear Science 26 (5): 053105. Manrı́quez, Ronald, Camilo Guerrero-Nancuante, and Carla Taramasco. 2021. “Protection Strategy Against an Epidemic Disease on Edge-Weighted Graphs Applied to a COVID-19 Case.” Biology 10 (7): 667. Mendel, Jerry M. 1991. “Tutorial on Higher-Order Statistics (Spectra) in Signal Processing and System Theory: Theoretical Results and Some Applications.” Proceedings of the IEEE 79 (3): 278–305. Mesquita, Diego, Amauri Souza, and Samuel Kaski. 2020. “Rethinking Pooling in Graph Neural Networks.” Neurips 33: 2220–31. Mitchell, Edward C., Brittany Story, David Boothe, Piotr J. Franaszczuk, and Vasileios Maroulas. 2022. “A Topological Deep Learning Framework for Neural Spike Decoding.” arXiv Preprint arXiv:2212.05037. Moor, Michael, Max Horn, Bastian Rieck, and Karsten Borgwardt. 2020. “Topological Autoencoders.” In International Conference on Machine Learning, 7045–54. PMLR. Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. Nicolau, Monica, Arnold J. Levine, and Gunnar Carlsson. 2011. “Topology Based Data Analysis Identifies a Subgroup of Breast Cancers with a Unique Mutational Profile and Excellent Survival.” Proceedings of the National Academy of Sciences 108 (17): 7265–70. Pang, Yunsheng, Yunxiang Zhao, and Dongsheng Li. 2021. “Graph Pooling via Coarsened Graph Infomax.” In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2177–81. Papillon, Mathilde, Sophia Sanborn, Mustafa Hajij, and Nina Miolane. 2023. “Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.” arXiv Preprint arXiv:2304.10031. Perea, Jose A., Anastasia Deckard, Steve B. Haase, and John Harer. 2015. “SW1PerS: Sliding Windows and 1-Persistence Scoring; Discovering Periodicity in Gene Expression Time Series Data.” BMC Bioinformatics 16 (1): 257. Petri, Giovanni, Martina Scolamiero, Irene Donato, and Francesco Vaccarino. 2013a. “Networks and Cycles: A Persistent Homology Approach to Complex Networks.” Proceedings European Conference on Complex Systems 2012, Springer Proceedings in Complexity, 93–99. ———. 2013b. “Topological Strata of Weighted Complex Networks.” PLoS ONE 8 (6). Piaggesi, Simone, André Panisson, and Giovanni Petri. 2022. “Effective Higher-Order Link Prediction and Reconstruction from Simplicial Complex Embeddings.” In Learning on Graphs Conference, 55–51. PMLR. Plizzari, Chiara, Marco Cannici, and Matteo Matteucci. 2021. “Spatial Temporal Transformer Network for Skeleton-Based Action Recognition.” In Icpr, 694–701. Springer. Pun, Chi Seng, Kelin Xia, and Si Xian Lee. 2018. “Persistent-Homology-Based Machine Learning and Its Applications–a Survey.” arXiv Preprint arXiv:1811.00252. Reddy, Thummaluru Siddartha, Sundeep Prabhakar Chepuri, and Pierre Borgnat. 2023. “Clustering with Simplicial Complexes.” arXiv Preprint arXiv:2303.07646. Rieck, Bastian, Christian Bock, and Karsten Borgwardt. 2019. “A Persistent Weisfeiler-Lehman Procedure for Graph Classification.” In International Conference on Machine Learning, 5448–58. PMLR. Rieck, Bastian, and Heike Leitte. 2015. “Persistent Homology for the Evaluation of Dimensionality Reduction Schemes.” Computer Graphics Forum 34 (3): 431–40. Rieck, Bastian, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy Wolf, Nick Turk-Browne, and Smita Krishnaswamy. 2020. “Uncovering the Topology of Time-Varying fMRI Data Using Cubical Persistence.” In Advances in Neural Information Processing Systems (NeurIPS), edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6900–6912. Curran Associates, Inc. Roddenberry, T Mitchell, and Santiago Segarra. 2019. “HodgeNet: Graph Neural Networks for Edge Data.” In 2019 53rd Asilomar Conference on Signals, Systems, and Computers, 220–24. IEEE. Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021. “Principled Simplicial Neural Networks for Trajectory Prediction.” In International Conference on Machine Learning. Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. “Signal Processing on Cell Complexes.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” In International Conference on Medical Image Computing and Computer-Assisted Intervention, 234–41. Springer. Rosen, Paul, Bei Wang, Anil Seth, Betsy Mills, Adam Ginsburg, Julia Kamenetzky, Jeff Kern, and Chris R Johnson. 2017. “Using Contour Trees in the Analysis and Visualization of Radio Astronomy Data Cubes.” arXiv Preprint arXiv:1704.04561, 1–7. Santoro, Andrea, Federico Battiston, Giovanni Petri, and Enrico Amico. 2023. “Higher-Order Organization of Multivariate Time Series.” Nature Physics, 1–9. Sardellitti, Stefania, and Sergio Barbarossa. 2022. “Topological Signal Representation and Processing over Cell Complexes.” arXiv Preprint arXiv:2201.08993. Sardellitti, Stefania, Sergio Barbarossa, and Lucia Testa. 2021. “Topological Signal Processing over Cell Complexes.” Proceeding IEEE Asilomar Conference. Signals, Systems and Computers. Schaub, Michael T., and Santiago Segarra. 2018. “Flow Smoothing and Denoising: Graph Signal Processing in the Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39. Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. “Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.” Signal Processing 187: 108149. Shi, Heyuan, Yubo Zhang, Zizhao Zhang, Nan Ma, Xibin Zhao, Yue Gao, and Jiaguang Sun. 2018. “Hypergraph-Induced Convolutional Networks for Visual Classification.” IEEE Transactions on Neural Networks and Learning Systems 30 (10): 2963–72. Singh, Gurjeet, Facundo Mémoli, Gunnar E Carlsson, et al. 2007. “Topological Methods for the Analysis of High Dimensional Data Sets and 3d Object Recognition.” PBG@ Eurographics 2: 091–100. Tabassum, Shazia, Fabiola SF Pereira, Sofia Fernandes, and João Gama. 2018. “Social Network Analysis: An Overview.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8 (5): e1256. Taylor, Dane, Florian Klimm, Heather A Harrington, Miroslav Kramár, Konstantin Mischaikow, Mason A Porter, and Peter J Mucha. 2015. “Topological Data Analysis of Contagion Maps for Examining Spreading Processes on Networks.” Nature Communications 6: 7723. Topaz, Chad M, Lori Ziegelmeier, and Tom Halverson. 2015. “Topological Data Analysis of Biological Aggregation Models.” PloS One 10 (5): e0126383. Turaev, Vladimir G. 2016. Quantum Invariants of Knots and 3-Manifolds. Vol. 18. Walter de Gruyter GmbH &amp; Co KG. Umeda, Yuhei. 2017. “Time Series Classification via Topological Data Analysis.” Information and Media Technologies 12: 228–39. Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. “Graph Attention Networks.” In International Conference on Learning Representations. Waibel, Dominik J. E., Scott Atwell, Matthias Meier, Carsten Marr, and Bastian Rieck. 2022. “Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction.” In Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, edited by Linwei Wang, Qi Dou, P. Thomas Fletcher, Stefanie Speidel, and Shuo Li, 150–59. Lecture Notes in Computer Science. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-16440-8_15. Wang, Cheng, Nan Ma, Zhixuan Wu, Jin Zhang, and Yongqiang Yao. 2023. “Survey of Hypergraph Neural Networks and Its Application to Action Recognition.” In Artificial Intelligence: Second CAAI International Conference, CICAI 2022, Beijing, China, August 27–28, 2022, Revised Selected Papers, Part II, 387–98. Springer. Wang, Fan, Huidong Liu, Dimitris Samaras, and Chao Chen. 2020. “Topogan: A Topology-Aware Generative Adversarial Network.” In Eccv, 118–36. Springer. Weinan, E., Luan Jianfeng, and Yao Yuan. 2013. “The Landscape of Complex Networks: Critical Nodes and a Hierarchical Decomposition.” Methods and Applications of Analysis 20: 383–404. Wu, Hanrui, and Michael K. Ng. 2022. “Hypergraph Convolution on Nodes-Hyperedges Network for Semi-Supervised Node Classification.” ACM Transactions on Knowledge Discovery from Data (TKDD) 16 (4): 1–19. Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. “A Comprehensive Survey on Graph Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems 32 (1): 4–24. Yan, Sijie, Yuanjun Xiong, and Dahua Lin. 2018. “Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition.” In Thirty-Second AAAI Conference on Artificial Intelligence. Yang, Maosheng, and Elvin Isufi. 2023. “Convolutional Learning on Simplicial Complexes.” arXiv Preprint arXiv:2301.11163. Ying, Zhitao, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. “Hierarchical Graph Representation Learning with Differentiable Pooling.” Neurips 31. Zeng, Sebastian, Florian Graf, Christoph Hofer, and Roland Kwitt. 2021. “Topological Attention for Time Series Forecasting.” In Advances in Neural Information Processing Systems, 34:24871–82. Curran Associates, Inc. Zhang, Qi, Qizhao Jin, Jianlong Chang, Shiming Xiang, and Chunhong Pan. 2018. “Kernel-Weighted Graph Convolutional Network: A Deep Learning Approach for Traffic Forecasting.” In 2018 24th International Conference on Pattern Recognition (ICPR), 1018–23. IEEE. Zhang, Weifeng, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. “Multiplex Graph Neural Networks for Multi-Behavior Recommendation.” In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, 2313–16. Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Zhao Li, Chengwei Yao, Huifen Dai, Zhi Yu, and Can Wang. 2021. “Hierarchical Multi-View Graph Pooling with Structure Learning.” IEEE Transactions on Knowledge and Data Engineering 35 (1): 545–59. Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. 2019. “Hierarchical Graph Pooling with Structure Learning.” arXiv Preprint arXiv:1911.05954. "],["conclusions.html", "第 11 章 Conclusions", " 第 11 章 Conclusions We have established a topological deep learning (TDL) framework that enables the learning of representations for data supported on topological domains. To this end, we have introduced combinatorial complexes (CCs) as a new topological domain to model and characterize the main components of TDL. Our framework provides a unification for many concepts that may be perceived as separate in the current literature. Specifically, we can reduce most deep learning architectures presented thus far in the literature to particular instances of combinatorial complex neural networks (CCNNs), based on computational operations defined on CCs. Our framework thus provides a platform for a more systematic exploration and comparison of the large space of deep learning protocols on topological spaces. Limitations. This work has laid out the foundations of a novel TDL framework. While TDL has great potential, similar to other novel learning frameworks, there are limitations, many of which are still not well-understood. Specifically, some known limitations involve: Computational complexity: The primary challenge for moving from graphs to richer topological domains is the combinatorial increase in the complexity to store and process data defined on such domains. Training a TDL network can be a computationally intensive task, requiring careful consideration of neighborhood functions and generalization performance. TDL networks also require a large amount of memory, especially when working with a large number of matrices during network construction. The topology of the network can also increase the computational complexity of training. The choice of the neural network architecture: Choosing the appropriate neural network architecture for a given dataset and a given learning task can be challenging. The performance of a TDL network can be highly dependent on the choice of architecture and its hyperparameters. Interpretability and explainability: The architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions. Limited availability of datasets: TDL networks require topological data, which we have found to be of limited availability. Ad hoc conversions of existing data to include higher-order relations may not always be ideal. Future work. Aforementioned limitations leave ample room for future studies, making the realization of the full potential of TDL an interesting endeavour. While the flexible definition of CCs, as compared to, e.g., simplicial complexes, already provides some mitigation to the associated computational challenges, improving the scaling of CCNNs even further will require the exploration of sparsification techniques, randomization and other algorithmic improvements. Besides addressing the aforementioned limitations, promising directions not treated within this paper include explorations of directed (Ausiello and Laura 2017), weighted (Battiloro et al. 2023), multilayer (Menichetti, Dall’Asta, and Bianconi 2016), and time-varying dynamic topological domains (Torres et al. 2021; Anwar and Ghosh 2022; Yin et al. 2022). There are also several issues related to the selection of the most appropriate topological domain for a given dataset in the first place, which need further exploration in the future. Additionally, there is a need, but also a research opportunity, to better understand CCNN architectures from a theoretical perspective. This could in turn lead to better architectures. To illustrate this point, consider graph neural networks (GNNs) based on message passing (Gilmer et al. 2017), which have recently been shown to be as powerful as the Weisfeiler–Lehman isomorphism test10. (K. Xu et al. 2018; Maron et al. 2019; Morris et al. 2019). This connection between GNNs and classical graph-based combinatorial invariants has driven theoretical developments of the graph isomorphism problem and has inspired new architectures (K. Xu et al. 2018; Maron et al. 2019; Arvind et al. 2020; Bouritsas et al. 2023). We expect that connecting similar developments will also be important for TDL. The topological viewpoint we adopt brings about many interesting properties. For example, we are able to model other types of topological inductive biases in our computational architectures, such as the properties that do not change under different discretizations of the underlying domain, e.g., the Euler characteristic that is commonly used to distinguish topological spaces. While isomorphisms are the primary equivalence relation in graph theory, homeomorphisms and topological equivalence are more relevant for data defined on topological spaces, and invariants under homeomorphisms have different machine learning applications11. Homeomorphism equivalence is more relevant in various applications in which domain discretization is an artifact of data processing, and not an intrinsic part of the data (Sharp et al. 2022). Further, homeomorphism equivalence translates to a similarity question between two structures. Indeed, topological data analysis has been extensively utilized towards addressing the problem of similarity between meshes (Dey et al. 2010; Hajij et al. 2018; Rieck, Bock, and Borgwardt 2019). In geometric data processing, neural network architectures that are agnostic to mesh discretization are often desirable and perform better in practice (Sharp et al. 2022). We anticipate that the development of TDL models will open up new avenues to explore topological invariants across topological domains. 参考文献 Anwar, Md Sayeed, and Dibakar Ghosh. 2022. “Stability of Synchronization in Simplicial Complexes with Multiple Interaction Layers.” Phys. Rev. E 106 (September): 034314. Arvind, Vikraman, Frank Fuhlbrück, Johannes Köbler, and Oleg Verbitsky. 2020. “On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties.” Journal of Computer and System Sciences 113: 42–59. Ausiello, Giorgio, and Luigi Laura. 2017. “Directed Hypergraphs: Introduction and Fundamental Algorithms-a Survey.” Theoretical Computer Science 658: 293–306. Battiloro, Claudio, Stefania Sardellitti, Sergio Barbarossa, and Paolo Di Lorenzo. 2023. “Topological Signal Processing over Weighted Simplicial Complexes.” arXiv Preprint arXiv:2302.08561. Bouritsas, Giorgos, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. 2023. “Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting.” IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (1): 657–68. Dey, Tamal K., Kuiyu Li, Chuanjiang Luo, Pawas Ranjan, Issam Safa, and Yusu Wang. 2010. “Persistent Heat Signature for Pose-Oblivious Matching of Incomplete Models.” Computer Graphics Forum 29 (5): 1545–54. Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. “Neural Message Passing for Quantum Chemistry.” In International Conference on Machine Learning. Hajij, Mustafa, Bei Wang, Carlos Scheidegger, and Paul Rosen. 2018. “Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology.” In 2018 IEEE Pacific Visualization Symposium (PacificVis), 125–34. IEEE. Maron, Haggai, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. 2019. “Provably Powerful Graph Networks.” arXiv Preprint arXiv:1905.11136. Menichetti, Giulia, Luca Dall’Asta, and Ginestra Bianconi. 2016. “Control of Multilayer Networks.” Scientific Reports 6 (1): 1–8. Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. Rieck, Bastian, Christian Bock, and Karsten Borgwardt. 2019. “A Persistent Weisfeiler-Lehman Procedure for Graph Classification.” In International Conference on Machine Learning, 5448–58. PMLR. Sharp, Nicholas, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. 2022. “DiffusionNet: Discretization Agnostic Learning on Surfaces.” Tog 41 (3): 1–16. Torres, Leo, Ann S Blevins, Danielle Bassett, and Tina Eliassi-Rad. 2021. “The Why, How, and When of Representations for Complex Systems.” SIAM Review 63 (3): 435–85. Weisfeiler, Boris, and Andrei Leman. 1968. “The Reduction of a Graph to Canonical Form and the Algebra Which Appears Therein.” NTI, Series 2 (9): 12–16. Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. “How Powerful Are Graph Neural Networks?” arXiv Preprint arXiv:1810.00826. Yin, Nan, Fuli Feng, Zhigang Luo, Xiang Zhang, Wenjie Wang, Xiao Luo, Chong Chen, and Xian-Sheng Hua. 2022. “Dynamic Hypergraph Convolutional Network.” In 2022 IEEE 38th International Conference on Data Engineering (ICDE), 1621–34. IEEE. The Weisfeiler–Lehman isomorphism test (Weisfeiler and Leman 1968) is a widely-used graph isomorphism algorithm that provides a coloring of a graph’s vertices, and this coloring gives a necessary condition for two graphs to be isomorphic.↩︎ Intuitively, two topological spaces are equivalent if one of them can be deformed to the other via a continuous transformation.↩︎ "],["glossary.html", "附录 A 术语", " 附录 A 术语 本表列出了本文所涉及的相关概念 符号标记 描述 \\(S\\) 抽象实体的非空有限集 \\(P_{S}\\) 索引集 \\(\\mathcal{P}(S)\\) 集合\\(\\)的幂集 \\((S,\\mathcal{N})\\) 非空集 \\(S\\) 和邻域拓扑 \\(\\mathcal{N}\\) 构成的拓扑空间 \\(\\mathcal{N}_{a}(x)\\) 胞腔\\(x\\)的邻接集合 \\(\\mathcal{N}_{co}(x)\\) 胞腔\\(x\\)的共邻集合 \\(\\mathcal{N}_{\\{ G_1,\\ldots,G_n\\}}(x)\\) 由邻域矩阵\\(\\{G_1,\\dots,G_n\\}\\)确定的\\(x\\)的邻居 \\(\\mathcal{N}_{\\searrow}(x)\\) Set of down-incidence of a cell \\(x\\) \\(\\mathcal{N}_{\\nearrow}(x)\\) Set of up-incidence of a cell \\(x\\) \\(\\mathcal{N}_{\\searrow,k}(x)\\) Set of \\(k\\)-down incidence of a cell \\(x\\) \\(\\mathcal{N}_{\\nearrow,k}(x)\\) Set of \\(k\\)-up incidence of a cell \\(x\\) \\(\\mathbb{N}\\) and \\(\\mathbb{Z}_{\\ge 0}\\) Set of positive integers and non-negative integers, respectively \\(\\mathcal{G}\\) Graph \\(x^k\\) Cell \\(x\\) of rank \\(k\\) \\(\\mbox{rk}\\) Rank function \\((S, \\mathcal{X}, \\mbox{rk})\\) CC, consisting of a set \\(S\\), a subset \\(\\mathcal{X}\\) of \\(\\mathcal{P}(S)\\setminus\\{\\emptyset\\}\\), and a rank function \\(\\mbox{rk}\\) \\(\\dim (\\mathcal{X})\\) Dimension of a CC \\(\\mathcal{X}\\) \\(\\{c_\\alpha\\}_{\\alpha \\in I}\\) Partition into subspaces (cells) indexed by an index set \\(I\\) \\(\\mbox{int}(x)\\) Interior of a cell \\(x\\) in a regular cell complex \\(n_\\alpha\\in \\mathbb{N}\\) Dimension of a cell in a regular cell complex \\(0\\)-cells Vertices of a CC \\(1\\)-cells Edges of a CC \\(k\\)-cells Cells with rank \\(k\\) \\(\\mathcal{X}^{(k)}\\) \\(k\\)-skeleton of \\(\\mathcal{X}\\), formed by \\(i\\)-cells in \\(\\mathcal{X}\\) with \\(i\\leq k\\) \\(\\mathcal{X}^k\\) Set of k-cells of \\(\\mathcal{X}\\) \\(|\\mathcal{X}^k|\\) Cardinality of \\(\\mathcal{X}^k\\), that is number of \\(k\\)-cells of \\(\\mathcal{X}\\) \\(\\mathcal{X}_{n-hop}(G)\\) \\(n\\)-hop CC of a graph \\(G\\) \\(\\mathcal{X}_p(G)\\) Path-based CC of a graph \\(G\\) \\(\\mathcal{X}_{loop}(G)\\) Loop-based CC of a graph \\(G\\) \\(\\mathcal{X}_{SC}(\\mathcal{Y})\\) Coface CC of a simplicial complex/CC \\(\\mathcal{Y}\\) \\(B_{r,k}\\) Incidence matrices between \\(r\\)-cells and \\(k\\)-cells \\(A_{r,k}\\) Adjacency matrices among the cells of \\(\\mbox{X}^{r}\\) with respect to the cells of \\(\\mbox{X}^{k}\\) \\(coA_{r,k}\\) Coadjacency matrices among the cells of \\(\\mbox{X}^{r}\\) with respect to the cells of \\(\\mbox{X}^{k}\\) \\(\\mathbf{W}\\) Trainable parameter \\(\\mathcal{C}^k(\\mathcal{X},\\mathbb{R}^d)\\) \\(k\\)-cochain space with features in \\(\\mathbb{R}^d\\) \\(\\mathcal{C}^k\\) \\(k\\)-cochain space with features in some Euclidean space \\(\\mathbf{G}= \\{G_1,\\ldots,G_m\\}\\) Set of cochain maps \\(G_i\\) defined on a complex \\(\\mathcal{M}_{ \\mathbf{G};\\mathbf{W}}\\) Merge node \\(G:C^{s}(\\mathcal{X})\\to C^{t}(\\mathcal{X})\\) Cochain map \\((\\mathbf{x}_{i_1},\\ldots, \\mathbf{x}_{i_m})\\) Vector of cochains \\(att^{l}: C^{s}(\\mathcal{X})\\to C^{s}(\\mathcal{X})\\) Higher-order attention matrix \\(\\mathcal{N}_{\\mathcal{Y}_0}=\\{\\mathcal{Y}_1,\\ldots,\\sigma_{|\\mathcal{N}_{\\mathcal{Y}_0}|}\\}\\) Set of a complex object in the vicinity of \\(\\mathcal{Y}_0\\) \\(a: {\\mathcal{Y}_0}\\times \\mathcal{N}_{\\mathcal{Y}_0}\\to [0,1]\\) Higher-order attention function \\(\\mbox{CCNN}_{\\mathbf{G};\\mathbf{W}}\\) CCNN or its tensor diagram representation \\(\\mathcal{H}_{\\mathcal{X}}= (V (\\mathcal{H}_{\\mathcal{X}}), E(\\mathcal{H}_{\\mathcal{X}}) )\\) Hasse graph with vertices \\(V (\\mathcal{H}_{\\mathcal{X}})\\) and edges \\(E(\\mathcal{H}_{\\mathcal{X}})\\); see Definition 8.1 The table below summarizes the paper’s acronyms. Acronym Description AGD Average geodesic distance CC Combinatorial complex CCANN Combinatorial complex attention neural network CCCNN Combinatorial complex convolutional neural network CCNN Combinatorial complex neural network CNN Convolutional neural network DEC Discrete exterior calculus GDL Geometric deep learning GNN Graph neural network MOG Mapper on graphs RNN Recurrent neural network SCoNe Simplicial complex network Sub-CC sub-combinatorial complex TDA Topological data analysis TDL Topological deep learning TQFT Topological quantum field theory "],["lifting-maps.html", "附录 B Lifting maps B.1 n-hop CC of a graph B.2 Path-based and subgraph-based CC of a graph B.3 Loop-based CC of a graph B.4 Coface CC of a simplicial complex or of a CC B.5 Augmentation of CCs by higher-rank cells", " 附录 B Lifting maps Lifting refers to the process of mapping a featured domain to another featured domain via a well-defined procedure. This section shows how we can lift a given domain to a CC or cell complex. Such lifting is useful as it allows CCNNs to be applied to common topological domains, including graphs and cell/simplicial complexes. This section only scratches the surface, as there remain many lifting constructions to be explored. We refer the reader to (Ferri, Bergomi, and Zu 2018) for examples of lifting graphs to simplicial complexes. B.1 n-hop CC of a graph Let \\(\\mathcal{G}=(V(\\mathcal{G}),E(\\mathcal{G}))\\) be a graph and \\(n\\geq 2\\) an integer. The \\(n\\)-hop CC of \\(\\mathcal{G}\\), denoted by \\(\\mbox{CC}_{n-\\mbox{hop}}(\\mathcal{G})\\), is the CC whose \\(0\\)-cells, \\(1\\)-cells, and \\(n\\)-cells are the nodes of \\(\\mathcal{G}\\), edges of \\(\\mathcal{G}\\), and set nodes in \\(n\\)-hop neighborhoods of the nodes in \\(\\mathcal{G}\\), respectively. It is easy to verify that \\(\\mbox{CC}_{n-\\mbox{hop}}(\\mathcal{G})\\) is a CC of dimension \\(n\\). Figure B.1(a) visualizes the \\(1\\)-hop CC of a graph. 图 B.1: Examples of lifting domains to CCs and cell complexes. (a): The \\(1\\)-hop neighborhood of the red node can be considered as a 2-cell that we can augment to the graph. Adding such 2-cells to a graph yields a CC called the 1-hop neighborhood of the graph. (b): A path on a graph of length more than two can be considered as a 2-cell that we can augment to the graph. Adding such 2-cells to a graph yields a CC called a path-based CC of the graph. (c): A loop in a graph (i.e., a closed path with no repeating edges) can be considered as a 2-cell that we can augment to the graph. Adding such 2-cells to a graph yields a CC called a loop-based CC of the graph. (d): For every blue 2-cell of a simplicial complex, we introduce a green 3-cell obtained by considering the 1-coface of the 2-cell. Adding such 3-cells to a simplicial complex yields a CC of dimension three called the coface CC of the simplicial complex. B.2 Path-based and subgraph-based CC of a graph Let \\(\\mathcal{G}=(V(\\mathcal{G}),E(\\mathcal{G}))\\) be a graph. A natural CC structure on \\(\\mathcal{G}\\) considers paths of \\(\\mathcal{G}\\). We define a path-based CC of \\(\\mathcal{G}\\), denoted by \\(\\mbox{CC}_p(\\mathcal{G})\\), to be a CC consisting of \\(0\\)-cells, \\(1\\)-cells and \\(2\\)-cells specified as follows. First, \\(\\mathcal{X}^0\\) and \\(\\mathcal{X}^1\\) in \\(\\mbox{CC}_p(\\mathcal{G})\\) are the sets of nodes and edges of \\(\\mathcal{G}\\), respectively. We now explain how to construct a \\(2\\)-cell in \\(\\mbox{CC}_p(\\mathcal{G})\\). Let \\(P\\) be a path in \\(\\mathcal{G}\\) with length larger than or equal to two (i.e., with two or more edges). An element \\(x_P\\) in \\(\\mathcal{X}^2\\) induced by \\(P\\) is defined to be \\(x_P=\\cup_{v\\in P }\\{v\\}\\). The set \\(\\mathcal{X}^2\\) in \\(\\mbox{CC}_p(\\mathcal{G})\\) is a non-empty collection of elements \\(x_P\\). It is easy to verify that \\(\\mbox{CC}_p(\\mathcal{G})\\) is a CC with \\(\\dim(\\mbox{CC}_p(\\mathcal{G}))=2\\). Note that we may replace the path \\(P\\) by a tree/subgraph of graph \\(\\mathcal{G}\\) and obtain a similar CC structure induced by the tree/subgraph of \\(\\mathcal{G}\\). Figure B.1(b) shows an example of a path-based CC of a graph. B.3 Loop-based CC of a graph Let \\(\\mathcal{G}=(V(\\mathcal{G}),E(\\mathcal{G}))\\) be a graph. We associate a CC structure with \\(\\mathcal{G}\\) that considers loops in \\(\\mathcal{G}\\). We define a loop-based CC of \\(\\mathcal{G}\\), denoted by \\(\\mbox{CC}_{loop}(\\mathcal{G})\\), to be a CC consisting of \\(0\\)-cells, \\(1\\)-cells and \\(2\\)-cells specified as follows. First, we set \\(\\mathcal{X}^0\\) and \\(\\mathcal{X}^1\\) in \\(\\mbox{CC}_{loop}(\\mathcal{G})\\) to be the nodes and edges of \\(\\mathcal{G}\\), respectively. We now explain how to construct a \\(2\\)-cell in \\(\\mbox{CC}_{loop}(\\mathcal{G})\\). A 2-cell in \\(\\mbox{CC}_{loop}(\\mathcal{G})\\) is a set \\(C=\\{x^0_1, \\ldots , x^0_k\\} \\subset \\mathcal{X}^0\\) such that \\(\\{x^0_i,x^0_{i+1}\\}\\), \\(1 \\leq i \\leq k - 1\\), and \\(\\{x^0_k, x^0_1\\}\\) are the only edges in \\(\\mathcal{X}^1 \\cap C\\). The set \\(\\mathcal{X}^2\\) in \\(\\mbox{CC}_{loop}(\\mathcal{G})\\) is a nonempty collection of elements \\(C\\). It is easy to verify that \\(\\mbox{CC}_{loop}(\\mathcal{G})\\) is a CC with \\(\\dim(\\mbox{CC}_{loop}(\\mathcal{G}))=2\\). Note that the sequence \\((x^0_1, \\ldots , x^0_k)\\) defines a loop in \\(\\mathcal{G}\\). This loop is called the loop that characterizes the 2-cell \\(C=\\{x^0_1, \\ldots , x^0_k\\}\\). Similar constructions are suggested in (Aschbacher 1996; Basak 2010; Savoy 2021; T. Mitchell Roddenberry, Schaub, and Hajij 2022). In fact, it is easy to confirm that every 2-dimensional regular cell complex can be constructed in this manner (T. Mitchell Roddenberry, Schaub, and Hajij 2022). Figure B.1(c) shows an example of a loop-based CC of a graph. B.4 Coface CC of a simplicial complex or of a CC Here, we describe a method to lift a simplicial complex of dimension two to a CC of dimension three. This method can be easily generalized to other dimensions. For a simplicial complex \\(\\mathcal{Y}\\) of dimension two, the coface CC of \\(\\mathcal{Y}\\), denoted by \\(\\mbox{CC}_{SC}( \\mathcal{Y})\\), is defined as follows. \\(\\mathcal{Y}^0\\), \\(\\mathcal{Y}^1\\), and \\(\\mathcal{Y}^2\\) in \\(\\mbox{CC}_{SC}( \\mathcal{Y})\\) are the nodes, the edges, and the triangles in \\(\\mathcal{Y}\\), respectively. We now explain how to construct a \\(3\\)-cell in \\(\\mbox{CC}_{SC}( \\mathcal{Y})\\). Let \\(x^2\\) be a 2-cell in \\(\\mathcal{Y}\\). The 3-cell in \\(\\mbox{CC}_{SC}( \\mathcal{Y})\\) associated with \\(x^2\\) is the union of all 0-cells in \\(\\mathcal{N}_{co,1}(x^2) \\cup x^2\\). The set \\(\\mathcal{Y}^3\\) in \\(\\mbox{CC}_{SC}( \\mathcal{Y})\\) is defined as the set of all 3-cells associated with all 2-cells \\(x^2\\) in \\(\\mathcal{Y}\\). It is easy to verify that \\(\\mbox{CC}_{SC}(\\mathcal{Y})\\) is a CC with \\(\\dim(\\mbox{CC}_{SC}(\\mathcal{Y}) )=3\\). A similar lifting construction can be defined to augment any CC of dimension \\(n\\) with \\((n+1)\\)-cells in order to obtain a CC of dimension \\(n+1\\). B.5 Augmentation of CCs by higher-rank cells The lifting methods proposed in Sections B.2, B.3, and B.4 can be described abstractly under a single general lifting construction. Specifically, the essence of all these lifting methods is to augment the underlying CC \\(\\mathcal{X}\\) with new cells that have a rank of \\(\\dim(\\mathcal{X})+1\\). Proposition B.1 formalizes the general lifting construction. 命题 B.1 (Augmenting a CC) Let \\(S\\) be a nonempty set and \\((S,\\mathcal{X},\\mbox{rk})\\) a CC of dimension \\(n\\) defined on \\(S\\). Consider a set \\(\\mathcal{X}^{n+1} \\subset \\mathcal{P}(S)\\) such that if \\(x\\in\\mathcal{X}\\) and \\(y \\in \\mathcal{X}^{n+1}\\) with \\(x\\subseteq y\\), then \\(x\\subsetneq y\\). Further, consider a map \\(\\hat{\\mbox{rk}}\\colon \\mathcal{X}\\cup \\mathcal{X}^{n+1}\\to \\mathbb{Z}_{\\ge 0}\\) that satisfies \\(\\hat{\\mbox{rk}}(x)= \\mbox{rk}(x)\\) for all \\(x\\in\\mathcal{X}\\) and \\(\\hat{\\mbox{rk}}(x)=n+1\\) for all \\(x \\in \\mathcal{X}^{n+1}\\). For \\(\\mathcal{X}^{n+1}\\) and \\(\\hat{\\mbox{rk}}\\) satisfying such conditions, \\((S,\\mathcal{X}\\cup \\mathcal{X}^{n+1},\\hat{\\mbox{rk}} )\\) is a CC of dimension \\(n+1\\). 证明. The proof follows directly from Definition 4.1. Given a CC \\(\\mathcal{X}\\), we call a CC of the form \\((S,\\mathcal{X}\\cup \\mathcal{X}^{n+1},\\hat{\\mbox{rk}} )\\), as constructed in Proposition B.1, a highest-rank augmented CC of \\(\\mathcal{X}\\). Note that Proposition B.1 provides a constructive and iterative method to build a CC of arbitrary dimension from a nonempty set \\(S\\) of abstract points. 参考文献 Aschbacher, Michael. 1996. “Combinatorial Cell Complexes.” In Progress in Algebraic Combinatorics, 1–80. Mathematical Society of Japan. Basak, Tathagata. 2010. “Combinatorial Cell Complexes and Poincaré Duality.” Geometriae Dedicata 147 (1): 357–87. Ferri, Massimo, Dott Mattia G. Bergomi, and Lorenzo Zu. 2018. “Simplicial Complexes from Graphs Towards Graph Persistence.” arXiv Preprint arXiv:1805.10716. Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. “Signal Processing on Cell Complexes.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Savoy, Maxime. 2021. “Combinatorial Cell Complexes: Duality, Reconstruction and Causal Cobordisms.” PhD thesis, École Polytechnique Fédérale de Lausanne. "],["ccnn-architecture-search-and-topological-quantum-field-theories.html", "附录 C CCNN architecture search and topological quantum field theories", " 附录 C CCNN architecture search and topological quantum field theories The problem of CCNN architecture search for a given TDL task can be cast as a hyperparameter optimization problem over the space of CCNNs. More precisely, consider the query of searching for an optimal CCNN between two fixed Cartesian products \\(\\mathcal{C}^{i_1}\\times\\mathcal{C}^{i_2}\\times \\ldots \\times \\mathcal{C}^{i_m}\\) and \\(\\mathcal{C}^{j_1}\\times\\mathcal{C}^{j_2}\\times \\ldots \\times \\mathcal{C}^{j_n}\\) of cochain spaces. In practice, this query poses a challenging problem. Rather than performing a computationally expensive CCNN search directly, an alternative approach is to conduct a search in the simpler space of marked trivalent graphs between marked points \\(\\{i_1,\\ldots,i_m\\}\\) and \\(\\{j_1,\\ldots,j_n\\}\\), and then map the resulting marked trivalent graph to a corresponding CCNN architecture. In the present section, we briefly sketch such a graph-based search method using tools from topological quantum field theory (TQFT) (Turaev 2016), and accordingly we assume some familiarity with the basics of category theory. Tensor diagrams draw their inspiration from TQFT, in which arbitrary maps between topological spaces are constructed from simpler and more manageable building blocks. The main workflow in TQFT constructions involves breaking down the topological spaces under consideration into simpler subspaces, and subsequently utilizing maps between the subspaces to construct maps between the initial topological spaces; see Figure C.1(a). Thus, the topological properties of maps between topological spaces are better understood via the topological properties of simpler constituent maps between respective subspaces. This can be especially useful in applications such as knot theory or the study of three-dimensional manifolds, where understanding the topological properties of involved maps is the main interest (Turaev 2016). 图 C.1: A sketch of the main idea of a TQFT construction in (a) and a functor \\(F_{tri}\\) for constructing tensor diagrams in (b). (a): In the depicted example, the goal is to construct a map \\(f\\colon A\\to B\\) between two topological spaces \\(A\\) and \\(B\\). We first decompose \\(A\\) and \\(B\\) into simpler sub-spaces, say \\(A = A_1 \\cup A_2\\) and \\(B = B_1 \\cup B_2\\), so that \\({A_1,A_2}\\) and \\({B_1,B_2}\\) are `more elementary spaces’ than \\(A\\) and \\(B\\), respectively. We then construct two maps \\(f_1 \\colon A_1 \\to B_1\\) and \\(f_2 \\colon A_2 \\to B_2\\), and use them to construct \\(f\\). (b): A visual example of a functor \\(F_{tri}\\) that pairs each marked trivalent graph with a corresponding tensor diagram. This example sends a marked trivalent graph with marked points \\(\\{0,1,2\\}\\) at the bottom and marked points \\(\\{1,2\\}\\) at the top to a tensor diagram with domain \\(\\mathcal{C}^{0} \\times \\mathcal{C}^{1}\\times \\mathcal{C}^{2}\\) and codomain \\(\\mathcal{C}^{1}\\times \\mathcal{C}^{2}\\). Once the push-forward, the merge and the split nodes are defined, the functor \\(F_{tri}\\) attaches a well-defined tensor diagram to each trivalent graph. A CCNN can be then constructed from the tensor diagram. Before we introduce the relation between tensor diagrams and TQFTs, we sketch two required preliminaries, namely marked trivalent graphs and TQFTs. First, we define the objects and morphisms of the category of marked trivalent graphs \\(Tri\\). A marked trivalent graph is intuitively described via its layout. Consider the 2d-disk \\([0,1]\\times [0,1]\\) with a collection of marked points \\(\\{i_1,\\ldots,i_m\\}\\) at the bottom and a collection of marked points \\(\\{j_1,\\ldots,j_n\\}\\) at the top. A marked trivalent graph is a graph that is drawn inside the disk in such a way that all edges flow within the disk, connecting the marked points at the top with the marked points at the bottom, and meeting at each vertex with exactly three edges. In the category \\(Tri\\), a trivalent graph with marked points \\(\\{i_1,\\ldots,i_m\\}\\) and \\(\\{j_1,\\ldots,j_n\\}\\) at the bottom and top, respectively, plays the role of a morphism between the object \\(\\{i_1,\\ldots,i_m\\}\\) and the object \\(\\{j_1,\\ldots,j_n\\}\\). The composition of two such morphisms, when admissible, is defined to be the vertical concatenation of their corresponding marked trivalent graphs. The vertical concatenation of marked trivalent graphs yields a marked trivalent graph. Finally, any trivalent graph can be built by concatenating horizontally and vertically three elementary trivalent graphs similar to the graph shown in Figure C.1(b), ignoring the labels in the figure. At a high level, a TQFT is a functor \\(F\\colon Bord_{n}\\to Vec_{K}\\) that assigns a \\(K\\)-vector space \\(F(x)\\in Vec_{K}\\) to each \\(n\\)-manifold \\(x \\in Bord_{n}\\), and a linear map \\(F(f)\\) to each \\((n+1)\\)-cobordism \\(f \\in Bord_{n}\\). It is noted that a \\((n+1)\\)-cobordism is an \\((n+1)\\)-manifold representing a morphism between two \\(n\\)-manifolds. The functor \\(F\\) is typically defined on a few elementary morphisms in \\(Bord_{n}\\), but can be extended to arbitrary morphisms in \\(Bord_{n}\\) by preserving the structure of the elementary maps when mapped via \\(F\\). Such a functor enables the study of \\(n\\)-manifolds and \\((n+1)\\)-cobordisms defined between them by mapping \\(n\\)-manifolds to their corresponding simpler vector spaces in \\(Vec_{K}\\) and by mapping \\((n+1)\\)-cobordisms to linear maps between the corresponding vector spaces. To establish the relation between marked trivalent graphs and tensor diagrams, we introduce a new TQFT \\(F_{tri}\\) that sends each morphism (marked trivalent graph) in \\(Tri\\) to a corresponding tensor diagram by labeling marked trivalent graphs with appropriate cochain maps. Figure C.1(b) shows an illustration of such a functor \\(F_{tri}\\). Through the lens of \\(F_{tri}\\), the relation between tensor diagrams and marked trivalent graphs becomes clear; a tensor diagram is a marked trivalent graph whose edges are labeled via cochain maps. The functor \\(F_{tri}\\) allows us to search the space of marked trivalent graphs as an equivalent way of conducting CCNN architecture search, since CCNNs are represented by tensor diagrams. 参考文献 Turaev, Vladimir G. 2016. Quantum Invariants of Knots and 3-Manifolds. Vol. 18. Walter de Gruyter GmbH &amp; Co KG. "],["learning-discrete-exterior-calculus-operators-with-ccanns.html", "附录 D Learning discrete exterior calculus operators with CCANNs", " 附录 D Learning discrete exterior calculus operators with CCANNs The operator \\(G_{tr}=G\\odot att\\) of Equations (5.4) and (5.5) has an advantageous cross-cutting interpretation. First, recall that \\(G_{tr}\\) has the same shape as the original operator \\(G\\). More importantly, \\(G_{tr}\\) can be viewed as a learnt version of \\(G\\). For instance, if \\(G\\) is the \\(k\\)-Hodge Laplacian \\(\\mathbf{L}_k\\), then the learnt attention version \\(G_{tr}\\) of it represents a \\(k\\)-Hodge Laplacian that is adapted to the domain \\(\\mathcal{X}\\) for the learning task at hand. This perspective converts our attention framework to a tool for learning discrete exterior calculus (DEC) operators (Desbrun, Kanso, and Tong 2008). We refer the interested reader to recent works along these lines (Smirnov and Solomon 2021; Trask, Huang, and Hu 2022), where neural networks are used to learn Laplacian operators in various shape analysis tasks. Concretely, one of the main building blocks of DEC is a collection of linear operators of the form \\(\\mathcal{A} \\colon \\mathcal{C}^i(\\mathcal{X}) \\to \\mathcal{C}^j(\\mathcal{X})\\) that act on a cochain \\(\\mathbf{H}\\) to produce another cochain \\(\\mathcal{A}(\\mathbf{H})\\). An example of an operator \\(\\mathcal{A}\\) is the graph Laplacian. There are seven primitive DEC operators, including the discrete exterior derivative, the hodge star and the wedge product. These seven primitive operators can be combined together to form other operators. In our setting, the discrete exterior derivatives are precisely a signed version of the incidence matrices defined in the context of cell/simplicial complexes. We denote the \\(k\\)-signed incidence matrix defined on a cell/simplicial complex by \\(\\mathbf{B}_k\\). It is common in the context of discrete exterior calculus (Desbrun, Kanso, and Tong 2008) to refer to \\(\\mathbf{B}_k^T\\) as the \\(k^{th}\\) discrete exterior derivative \\(d^k\\). So, from a DEC point of view, the matrices \\(\\mathbf{B}_0^T, \\mathbf{B}_1^T\\) and \\(\\mathbf{B}_2^T\\) are regarded as the discrete exterior derivatives \\(d^0(\\mathbf{H})\\), \\(d^1 (\\mathbf{H})\\), and \\(d^2 (\\mathbf{H})\\) of some 0-, 1-, and 2-cochains defined on \\(\\mathcal{\\mathcal{X}}\\), which in turn are the discrete analogs of the gradient \\(\\nabla \\mathbf{H}\\), curl \\(\\nabla\\times \\mathbf{H}\\) and divergence \\(\\nabla \\cdot \\mathbf{H}\\) of a smooth function defined on a smooth surface. We refer the reader to (Desbrun, Kanso, and Tong 2008) for a coherent list of DEC operators and their interpretation. Together, cochains and the operators that act on them provide a concrete framework that facilitates computing a cochain of interest, such as a cochain obtained by solving a partial differential equation on a discrete surface. Our attention framework can be viewed as a non-linear version of the DEC based on linear operators \\(\\mathcal{A}\\), and can be used to learn the DEC operators on a domain \\(\\mathcal{X}\\) for a particular learning task. Specifically, a linear operator \\(\\mathcal{A}\\), as it appears in classical DEC, can be considered as a special case of Equations (5.4) and (5.5). Unlike existing work (Smirnov and Solomon 2021; Trask, Huang, and Hu 2022), our DEC learning approach based on CCANNs generalizes and applies to all domains in which DEC is typically applicable; examples of such domains include triangular and polygonal meshes (Crane et al. 2013). In contrast, existing operator learning methods are defined only for particular types of DEC operators, and therefore cannot be used to learn arbitrary types of DEC operators. 参考文献 Crane, Keenan, Fernando De Goes, Mathieu Desbrun, and Peter Schröder. 2013. “Digital Geometry Processing with Discrete Exterior Calculus.” In ACM SIGGRAPH 2013 Courses, 1–126. Association for Computing Machinery. Desbrun, Mathieu, Eva Kanso, and Yiying Tong. 2008. “Discrete Differential Forms for Computational Modeling.” In Discrete Differential Geometry, 287–324. Springer. Smirnov, Dmitriy, and Justin Solomon. 2021. “HodgeNet: Learning Spectral Geometry on Triangle Meshes.” ACM Transactions on Graphics 40 (4): 1–11. Trask, Nathaniel, Andy Huang, and Xiaozhe Hu. 2022. “Enforcing Exact Physics in Scientific Machine Learning: A Data-Driven Exterior Calculus on Graphs.” Journal of Computational Physics. "],["a-mapper-induced-topology-preserving-cc-pooling-operation.html", "附录 E A mapper-induced topology-preserving CC-pooling operation", " 附录 E A mapper-induced topology-preserving CC-pooling operation In this section, we give an example of constructing a shape-preserving pooling operation on a CC \\(\\mathcal{X}\\). Specifically, we demonstrate the case in which \\(\\mathcal{X}\\) is a graph, and utilize the mapper on graphs (MOG) construction (Hajij, Wang, and Rosen 2018), which is a graph skeletonization algorithm that can be used to augment \\(\\mathcal{X}\\) with topology-preserving higher-rank cells, as demonstrated in Figure 2.3. Although we only demonstrate the shape-preserving pooling construction on graphs, the method suggested herein can be easily extended to CCs. Let \\(\\mathcal{X}\\) be a connected graph and \\(g\\colon\\mathcal{X}^{0} \\to [0,1]\\) a scalar function. Let \\(\\mathcal{U}=\\{U_\\alpha\\}_{\\alpha \\in I}\\) be a finite collection of open sets that covers the interval \\([0,1]\\). The MOG construction of a graph \\(MOG(V_{MOG},E_{MOG})\\) based on the triplet (\\(\\mathcal{X}\\), \\(g\\), \\(\\mathcal{U}\\)) consists of the following steps: We first use the cover \\(\\mathcal{U}\\) to construct the pull-back cover \\(g^{\\star}(\\mathcal{U})=\\{g^{-1}(U_{\\alpha})\\}_{\\alpha \\in I}\\). The vertex set \\(V_{MOG}\\) is formed by considering the connected components (i.e., maximal connected subgraphs) induced by \\(g^{-1}(U_\\alpha)\\) for each \\(\\alpha\\). The edge set \\(E_{MOG}\\) is formed by considering the intersection among the connected components computed in step 2. Figure E.1 shows an illustrative example of applying the MOG algorithm to a graph. Figure E.1(a) shows the graph on which the MOG algorithm is applied, while Figure E.1(b) visualizes the scalar function \\(g\\) and the covering \\(\\mathcal{U}\\), which consists of four covering elements depicted in red, orange, yellow and blue colors. 图 E.1: An illustration of the MOG algorithm. The input to the MOG algorithm is a triplet \\((\\mathcal{X}, g, \\mathcal{U})\\), where \\(\\mathcal{X}\\) is a graph, \\(g\\colon\\mathcal{X}^0\\to [0,1]\\) is a scalar function defined on the vertex set of \\(\\mathcal{X}\\), and \\(\\mathcal{U}\\) is a cover of \\([0,1]\\). (a): An input graph \\(\\mathcal{X}\\). (b): The scalar function \\(g\\colon\\mathcal{X}\\to [0,1]\\) is visualized by color-mapping its scalar values according to the displayed color bar. Figure (b) also shows the covering \\(\\mathcal{U}= \\{U_1,U_2, U_3, U_4\\}\\) depicted in red, orange, yellow and blue colors. (c): We pull back via \\(g\\) each cover element \\(U_i\\) in \\(\\mathcal{U}\\), and we compute the connected components in \\(g^{-1}(U_i)\\). (d): The vertex set in the graph generated by the MOG algorithm consists of the connected components induced by \\(g^{-1}(U_i)\\), while the edge set is formed by considering the intersection among the connected components. Note that the graph generated by the MOG algorithm approximates the shape of the input graph. The connected components obtained from the MOG algorithm can be used to augment a graph \\(\\mathcal{X}\\) with a skeleton \\(\\mathcal{X}^2\\) of dimension 2, thus constructing a CC, as described in Proposition B.1. We denote the resulting CC by \\(\\mathcal{X}_{g,\\mathcal{U}}\\). Figure E.2 demonstrates the construction of a CC using this augmentation process based on the MOG algorithm. 图 E.2: A visual example of obtaining a CC from a graph via the MOG algorithm. (a): An input graph \\(\\mathcal{X}\\). (b): The graph \\(\\mathcal{X}\\) is augmented by the \\(2\\)-cells formed via the connected components obtained from applying the MOG algorithm to \\(\\mathcal{X}\\). (c): The CC \\(X_{g,\\mathcal{U}}\\) obtained by augmenting \\(\\mathcal{X}\\) with these \\(2\\)-cells. MOG is a topology-preserving graph skeletonization algorithm, which can be used to coarsen the size of an input graph \\(\\mathcal{X}\\). Such coarsening typically occurs when constructing a pooling layer. So, the skeleton \\(\\mathcal{X}^2\\) generated by the MOG algorithm can be utilized to obtain a feature-preserving pooling operation. Figure E.3 illustrates that the MOG algorithm coarsens an input graph to produce a graph that preserves topological features of the input graph. 图 E.3: Illustration of coarsening a graph via the MOG algorithm. (a): Visualization of a graph and of a scalar function defined on it, which are used as inputs to the MOG algorithm. The scalar function is visualized by color-mapping its scalar values. (b): The resulting MOG graph. Notice how the MOG construction preserves the overall shape of the original graph. To recap, the MOG algorithm receives an input graph, and outputs a coarsened graph along with connected components. The connected components yield in turn a CC. The coarsened graph and the adjacency relation between the \\(2\\)-cells of the associated CC are related, as elaborated in Proposition E.1. Figure E.4 conveys visually Proposition E.1. 命题 E.1 (MOG as a CC) Let \\((\\mathcal{X}, g, \\mathcal{U})\\) be a triplet consisting of a graph \\(\\mathcal{X}\\), a scalar function \\(g\\colon\\mathcal{X}^0\\to [0,1]\\) defined on the vertex set of \\(\\mathcal{X}\\), and a cover \\(\\mathcal{U}\\) of \\([0,1]\\). Let \\(MOG(\\mathcal{X}\\), \\(g\\), \\(\\mathcal{U})\\) be the graph generated by the MOG algorithm upon receiving the triplet \\((\\mathcal{X}, g, \\mathcal{U})\\) as input. Furthermore, let \\(\\mathcal{X}_{g,\\mathcal{U}}\\) be the CC constructed from the connected components that are generated by the MOG algorithm. The adjacency matrix of the graph \\(MOG(\\mathcal{X}\\), \\(g\\), \\(\\mathcal{U})\\) is equivalent to the adjacency matrix \\(A_{2,2}\\) of the CC \\(\\mathcal{X}_{g,\\mathcal{U}}\\). 证明. The proof follows by observing that the \\(2\\)-cells, which are augmented to \\(\\mathcal{X}\\) to generate the CC \\(\\mathcal{X}_{g,\\mathcal{U}}\\), are 2-adjacent if and only if they intersect on a vertex. Moreover, two cells intersect on a vertex if and only if there is an edge between them in the graph \\(MOG(\\mathcal{X}, g, \\mathcal{U})\\) outputted by the MOG algorithm. 图 E.4: Visual demonstration of how a MOG can be viewed as a CC. (a): Visualization of a CC \\(\\mathcal{X}_{g,\\mathcal{U}},\\) which highlights the intersection between blue 2-cells. (b): The corresponding \\(MOG(\\mathcal{X}, g, \\mathcal{U})\\) graph. There is a one-to-one correspondence between the blue 2-cells of \\(\\mathcal{X}_{g,\\mathcal{U}}\\) on the left-hand side and the blue vertices of \\(MOG(\\mathcal{X}, g, \\mathcal{U})\\) on the right-hand side. Further, two blue 2-cells of \\(\\mathcal{X}_{g,\\mathcal{U}}\\) (left) intersect on an orange vertex if and only if there is an orange edge (right) that connects the corresponding two vertices of \\(MOG(\\mathcal{X}, g, \\mathcal{U})\\). Given a CC \\(X_{g,\\mathcal{U}}\\) obtained from a graph \\(\\mathcal{X}\\) via the MOG algorithm, the map \\(B_{0,2}^T\\colon\\mathcal{C}^0(X_{g,\\mathcal{U}})\\to \\mathcal{C}^2(X_{g,\\mathcal{U}})\\) can be used to induce a shape-preserving CC-pooling operation (Definition 7.1). More specifically, a signal \\(\\mathbf{H}_0\\) supported on the vertices of \\(\\mathcal{X}\\) can be push-forwarded and pooled to a signal \\(\\mathbf{H}_2\\) supported on the 2-cells of \\(X_{g,\\mathcal{U}}\\). 参考文献 Hajij, Mustafa, Bei Wang, and Paul Rosen. 2018. “MOG: Mapper on Graphs for Relationship Preserving Clustering.” arXiv Preprint arXiv:1804.11242. "],["参考文献.html", "参考文献", " 参考文献 Abramenko, Peter, and Kenneth S. Brown. 2008. Buildings: Theory and Applications. Vol. 248. Springer Science &amp; Business Media. Adams, Henry, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. 2017. “Persistence Images: A Stable Vector Representation of Persistent Homology.” Jmlr 18 (1): 218–52. Adesso, Gerardo. 2023. “Towards the Ultimate Brain: Exploring Scientific Discovery with ChatGPT AI.” AI Magazine 44 (3): 328–42. https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12113. Amar, David, and Ron Shamir. 2014. “Constructing Module Maps for Integrated Analysis of Heterogeneous Biological Networks.” Nucleic Acids Research 42 (7): 4208–19. Anand, D. V., and Moo K. Chung. 2023. “Hodge Laplacian of Brain Networks.” IEEE Transactions on Medical Imaging. Anwar, Md Sayeed, and Dibakar Ghosh. 2022. “Stability of Synchronization in Simplicial Complexes with Multiple Interaction Layers.” Phys. Rev. E 106 (September): 034314. Arvind, Vikraman, Frank Fuhlbrück, Johannes Köbler, and Oleg Verbitsky. 2020. “On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties.” Journal of Computer and System Sciences 113: 42–59. Arya, Devanshu, and Marcel Worring. 2018. “Exploiting Relational Information in Social Networks Using Geometric Deep Learning on Hypergraphs.” In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, 117–25. Aschbacher, Michael. 1996. “Combinatorial Cell Complexes.” In Progress in Algebraic Combinatorics, 1–80. Mathematical Society of Japan. Asif, Nurul A., Yeahia Sarker, Ripon K. Chakrabortty, Michael J. Ryan, Md. Hafiz Ahamed, Dip K. Saha, Faisal R. Badal, et al. 2021. “Graph Neural Network: A Comprehensive Review on Non-Euclidean Space.” IEEE Access 9: 60588–606. https://doi.org/10.1109/ACCESS.2021.3071274. Attene, Marco, Silvia Biasotti, and Michela Spagnuolo. 2003. “Shape Understanding by Contour-Driven Retiling.” The Visual Computer 19 (2): 127–38. Atzmon, Matan, Haggai Maron, and Yaron Lipman. 2018. “Point Convolutional Neural Networks by Extension Operators.” ACM Transactions on Graphics 37 (4). Ausiello, Giorgio, and Luigi Laura. 2017. “Directed Hypergraphs: Introduction and Fundamental Algorithms-a Survey.” Theoretical Computer Science 658: 293–306. Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv Preprint arXiv:1409.0473. https://arxiv.org/abs/1409.0473. Bai, Junjie, Biao Gong, Yining Zhao, Fuqiang Lei, Chenggang Yan, and Yue Gao. 2021. “Multi-Scale Representation Learning on Hypergraph for 3D Shape Retrieval and Recognition.” IEEE Transactions on Image Processing 30: 5327–38. Bai, Song, Feihu Zhang, and Philip H. S. Torr. 2021. “Hypergraph Convolution and Hypergraph Attention.” Pattern Recognition 110: 107637. Bailoni, Alberto, Constantin Pape, Nathan Hütsch, Steffen Wolf, Thorsten Beier, Anna Kreshuk, and Fred A Hamprecht. 2022. “GASP, a Generalized Framework for Agglomerative Clustering of Signed Graphs and Its Application to Instance Segmentation.” In Cvpr, 11645–55. Bajaj, Chandrajit L., Valerio Pascucci, and Daniel R. Schikore. 1997. “The Contour Spectrum.” In Proceedings of the 8th Conference on Visualization ’97, 167–ff. IEEE Computer Society Press. Bampasidou, Maria, and Thanos Gentimis. 2014. “Modeling Collaborations with Persistent Homology.” arXiv Preprint arXiv:1403.5346 abs/1403.5346. Bao, Xiaoge, Qitong Hu, Peng Ji, Wei Lin, Jürgen Kurths, and Jan Nagler. 2022. “Impact of Basic Network Motifs on the Collective Response to Perturbations.” Nature Communications 13 (1): 5301. Barabási, Albert-László. 2013. “Network Science.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 371 (1987): 20120375. Barbarossa, Sergio, and Stefania Sardellitti. 2020a. “Topological Signal Processing over Simplicial Complexes.” IEEE Transactions on Signal Processing 68: 2992–3007. ———. 2020b. “Topological Signal Processing: Making Sense of Data Building on Multiway Relations.” IEEE Signal Processing Magazine 37 (6): 174–83. Barbarossa, Sergio, Stefania Sardellitti, and Elena Ceci. 2018. “Learning from Signals Defined over Simplicial Complexes.” In 2018 IEEE Data Science Workshop (DSW), 51–55. IEEE. Barbarossa, Sergio, and Mikhail Tsitsvero. 2016. “An Introduction to Hypergraph Signal Processing.” In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6425–29. IEEE. Basak, Tathagata. 2010. “Combinatorial Cell Complexes and Poincaré Duality.” Geometriae Dedicata 147 (1): 357–87. Battaglia, Peter W., Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, et al. 2018. “Relational Inductive Biases, Deep Learning, and Graph Networks.” arXiv Preprint arXiv:1806.01261. Battaglia, Peter, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu. 2016. “Interaction Networks for Learning about Objects, Relations and Physics.” In Proceedings of the 30th International Conference on Neural Information Processing Systems, 4509–17. NIPS’16. Red Hook, NY, USA: Curran Associates Inc. Battiloro, Claudio, Stefania Sardellitti, Sergio Barbarossa, and Paolo Di Lorenzo. 2023. “Topological Signal Processing over Weighted Simplicial Complexes.” arXiv Preprint arXiv:2302.08561. Battiston, Federico, Enrico Amico, Alain Barrat, Ginestra Bianconi, Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, et al. 2021. “The Physics of Higher-Order Interactions in Complex Systems.” Nature Physics 17 (10): 1093–98. Battiston, Federico, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri. 2020. “Networks Beyond Pairwise Interactions: Structure and Dynamics.” Physics Reports 874: 1–92. Beaini, Dominique, Saro Passaro, Vincent Létourneau, Will Hamilton, Gabriele Corso, and Pietro Liò. 2021. “Directional Graph Networks.” In International Conference on Machine Learning. Benson, Austin R., Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. 2018. “Simplicial Closure and Higher-Order Link Prediction.” Proceedings of the National Academy of Sciences 115 (48): E11221–30. Benson, Austin R., David F. Gleich, and Desmond J. Higham. 2021. “Higher-Order Network Analysis Takes Off, Fueled by Classical Ideas and New Data.” arXiv Preprint arXiv:2103.05031. Benson, Austin R., David F. Gleich, and Jure Leskovec. 2016. “Higher-Order Organization of Complex Networks.” Science 353 (6295): 163–66. BenTaieb, Aicha, and Ghassan Hamarneh. 2016. “Topology Aware Fully Convolutional Networks for Histology Gland Segmentation.” In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, 460–68. Springer. Berry, Eric, Yen-Chi Chen, Jessi Cisewski-Kehe, and Brittany Terese Fasy. 2020. “Functional Summaries of Persistence Diagrams.” J. Appl. Comput. Topol. 4 (2): 211–62. Bhattacharya, Uttaran, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, and Dinesh Manocha. 2020. “STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits.” Proceedings of the AAAI Conference on Artificial Intelligence 34 (02): 1342–50. https://doi.org/10.1609/aaai.v34i02.5490. Bianchi, Filippo Maria, Claudio Gallicchio, and Alessio Micheli. 2022. “Pyramidal Reservoir Graph Neural Network.” Neurocomputing 470: 389–404. Bianchi, Filippo Maria, Daniele Grattarola, and Cesare Alippi. 2020. “Spectral Clustering with Graph Neural Networks for Graph Pooling.” In Icml, 874–83. PMLR. Bianconi, Ginestra. 2021. Higher-Order Networks. Cambridge University Press. Biasotti, Silvia, Leila De Floriani, Bianca Falcidieno, Patrizio Frosini, Daniela Giorgi, Claudia Landi, Laura Papaleo, and Michela Spagnuolo. 2008. “Describing Shapes by Geometrical-Topological Properties of Real Functions.” ACM Computing Surveys (CSUR) 40 (4): 12. Bick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T Schaub. 2021. “What Are Higher-Order Networks?” arXiv Preprint arXiv:2104.11329. Birdal, Tolga, Aaron Lou, Leonidas J Guibas, and Umut Simsekli. 2021. “Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks.” Advances in Neural Information Processing Systems. Bodnar, Cristian, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. 2021. “Weisfeiler and Lehman Go Cellular: CW Networks.” In Advances in Neural Information Processing Systems. Boscaini, Davide, Jonathan Masci, Simone Melzi, Michael M Bronstein, Umberto Castellani, and Pierre Vandergheynst. 2015. “Learning Class-Specific Descriptors for Deformable Shapes Using Localized Spectral Convolutional Networks.” Computer Graphics Forum 34 (5): 13–23. Boscaini, Davide, Jonathan Masci, Emanuele Rodolà, and Michael Bronstein. 2016. “Learning Shape Correspondence with Anisotropic Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, 3189–97. Bouritsas, Giorgos, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. 2023. “Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting.” IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (1): 657–68. Boyell, Roger L., and Henry Ruston. 1963. “Hybrid Techniques for Real-Time Radar Simulation.” In Proceedings of the November 12-14, 1963, Fall Joint Computer Conference, 445–58. ACM. Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” arXiv Preprint arXiv:2104.13478. Bronstein, Michael M., Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. “Geometric Deep Learning: Going Beyond Euclidean Data.” IEEE Signal Processing Magazine 34 (4): 18–42. Brown, Ronald. 2006. Topology and Groupoids. BookSurge Publishing. Bruna, Joan, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. “Spectral Networks and Locally Connected Networks on Graphs.” In Proceedings of the 2nd International Conference on Learning Representations, edited by Yoshua Bengio and Yann LeCun. ICLR 2014. Banff, AB, Canada. Bubenik, Peter. 2015. “Statistical Topological Data Analysis Using Persistence Landscapes.” Jmlr 16 (1): 77–102. Bunch, Eric, Qian You, Glenn Fung, and Vikas Singh. 2020. “Simplicial 2-Complex Convolutional Neural Nets.” NeurIPS Workshop on Topological Data Analysis and Beyond. Burns, Thomas F., and Tomoki Fukai. 2023. “Simplicial Hopfield Networks.” In The Eleventh International Conference on Learning Representations. Calmon, Lucille, Michael T. Schaub, and Ginestra Bianconi. 2022. “Higher-Order Signal Processing with the Dirac Operator.” In Asilomar Conference on Signals, Systems, and Computers. Cao, Wenming, Zhiyue Yan, Zhiquan He, and Zhihai He. 2020. “A Comprehensive Survey on Geometric Deep Learning.” IEEE Access 8: 35929–49. Carlsson, Erik, Gunnar Carlsson, and Vin De Silva. 2006. “An Algebraic Topological Method for Feature Identification.” International Journal of Computational Geometry &amp; Applications 16 (04): 291–314. Carlsson, Gunnar. 2009. “Topology and Data.” Bulletin of the American Mathematical Society 46 (2): 255–308. Carlsson, Gunnar, and Rickard Brüel Gabrielsson. 2020. “Topological Approaches to Deep Learning.” In Topological Data Analysis: The Abel Symposium 2018, 119–46. Springer; Springer. Carlsson, Gunnar, Tigran Ishkhanov, Vin De Silva, and Afra Zomorodian. 2008. “On the Local Behavior of Spaces of Natural Images.” Ijcv 76 (1): 1–12. Carlsson, Gunnar, and Facundo Mémoli. 2008. “Persistent Clustering and a Theorem of J. Kleinberg.” arXiv Preprint arXiv:0808.2241. Carlsson, Gunnar, and Afra Zomorodian. 2009. “The Theory of Multidimensional Persistence.” Discrete &amp; Computational Geometry 42 (1): 71–93. Carlsson, Gunnar, Afra Zomorodian, Anne Collins, and Leonidas J Guibas. 2005. “Persistence Barcodes for Shapes.” International Journal of Shape Modeling 11 (02): 149–87. Carr, Hamish, Jack Snoeyink, and Michiel van de Panne. 2004. “Simplifying Flexible Isosurfaces Using Local Geometric Measures.” In IEEE Visualization, 497–504. IEEE. Carriere, Mathieu, Frederic Chazal, Yuichi Ike, Theo Lacombe, Martin Royer, and Yuhei Umeda. 2020. “PersLay: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2786–96. PMLR. Carstens, C. J., and K. J. Horadam. 2013. “Persistent Homology of Collaboration Networks.” Mathematical Problems in Engineering 2013. Chan, Joseph Minhow, Gunnar Carlsson, and Raul Rabadan. 2013. “Topology of Viral Evolution.” Proceedings of the National Academy of Sciences 110 (46): 18566–71. Chang, Yaomin, Lin Shu, Erxin Du, Chuan Chen, Ziyang Zhang, Zibin Zheng, Yuzhao Huang, and Xingxing Xing. 2022. “GraphRR: A Multiplex Graph Based Reciprocal Friend Recommender System with Applications on Online Gaming Service.” Knowledge-Based Systems 251: 109187. Chaudhari, Sneha, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. 2021. “An Attentive Survey of Attention Models.” ACM Transactions on Intelligent Systems and Technology (TIST) 12 (5): 1–32. Chen, Yen-Chi, Daren Wang, Alessandro Rinaldo, and Larry Wasserman. 2015. “Statistical Analysis of Persistence Intensity Functions.” arXiv Preprint arXiv:1510.02502. Chen, Yunpeng, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng, Jiashi Feng, and Yannis Kalantidis. 2019. “Graph-Based Global Reasoning Networks.” In Conference on Computer Vision and Pattern Recognition. Chen, Yuzhou, Yulia R. Gel, and H. Vincent Poor. 2022. “BScNets: Block Simplicial Complex Neural Networks.” Proceedings of the AAAI Conference on Artificial Intelligence 36 (6): 6333–41. https://doi.org/10.1609/aaai.v36i6.20583. Choi, Edward, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng Sun. 2017. “GRAM: Graph-Based Attention Model for Healthcare Representation Learning.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Cinque, Domenico Mattia, Claudio Battiloro, and Paolo Di Lorenzo. 2022. “Pooling Strategies for Simplicial Convolutional Networks.” arXiv Preprint arXiv:2210.05490. Clough, James R., Ilkay Oksuz, Nicholas Byrne, Julia A. Schnabel, and Andrew P. King. 2019. “Explicit Topological Priors for Deep-Learning Based Image Segmentation Using Persistent Homology.” In Information Processing in Medical Imaging: 26th International Conference, IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26, 16–28. Springer. Collins, Anne, Afra Zomorodian, Gunnar Carlsson, and Leonidas J Guibas. 2004. “A Barcode Shape Descriptor for Curve Point Cloud Data.” Computers &amp; Graphics 28 (6): 881–94. Crane, Keenan, Fernando De Goes, Mathieu Desbrun, and Peter Schröder. 2013. “Digital Geometry Processing with Discrete Exterior Calculus.” In ACM SIGGRAPH 2013 Courses, 1–126. Association for Computing Machinery. Curto, Carina. 2017. “What Can Topology Tell Us about the Neural Code?” Bulletin of the American Mathematical Society 54 (1): 63–78. Dabaghian, Y., F. Mémoli, L. Frank, and G. Carlsson. 2012. “A Topological Paradigm for Hippocampal Spatial Map Formation Using Persistent Homology.” PLoS Computational Biology 8 (8): e1002581. Dai, Enyan, Charu Aggarwal, and Suhang Wang. 2021. “NRGNN: Learning a Label Noise Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs.” In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 227–36. De Domenico, Manlio. 2017. “Multilayer modeling and analysis of human brain networks.” GigaScience 6 (5). https://doi.org/10.1093/gigascience/gix004. De Domenico, Manlio, Clara Granell, Mason A Porter, and Alex Arenas. 2016. “The Physics of Spreading Processes in Multilayer Networks.” Nature Physics 12 (10): 901–6. Deng, Haowen, Tolga Birdal, and Slobodan Ilic. 2018. “PPFNet: Global Context Aware Local Features for Robust 3D Point Matching.” In Cvpr, 195–205. Deng, Songgaojun, Shusen Wang, Huzefa Rangwala, Lijing Wang, and Yue Ning. 2020. “Cola-GNN: Cross-Location Attention Based Graph Neural Networks for Long-Term ILI Prediction.” In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, 245–54. Desbrun, Mathieu, Eva Kanso, and Yiying Tong. 2008. “Discrete Differential Forms for Computational Modeling.” In Discrete Differential Geometry, 287–324. Springer. DeWoskin, D., J. Climent, I. Cruz-White, M. Vazquez, C. Park, and J. Arsuaga. 2010. “Applications of Computational Homology to the Analysis of Treatment Response in Breast Cancer Patients.” Topology and Its Applications 157 (1): 157–64. Dey, Tamal K., Kuiyu Li, Chuanjiang Luo, Pawas Ranjan, Issam Safa, and Yusu Wang. 2010. “Persistent Heat Signature for Pose-Oblivious Matching of Incomplete Models.” Computer Graphics Forum 29 (5): 1545–54. Dey, Tamal K., Facundo Mémoli, and Yusu Wang. 2016. “Multiscale Mapper: Topological Summarization via Codomain Covers.” In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, 997–1013. SIAM. Dey, Tamal K., and Yusu Wang. 2022a. Computational Topology for Data Analysis. Cambridge University Press. ———. 2022b. Computational Topology for Data Analysis. Cambridge University Press. Dhillon, Inderjit S., Yuqiang Guan, and Brian Kulis. 2007. “Weighted Graph Cuts Without Eigenvectors a Multilevel Approach.” Pami 29 (11): 1944–57. Dodziuk, Jozef. 1976. “Finite-Difference Approach to the Hodge Theory of Harmonic Forms.” American Journal of Mathematics 98 (1): 79–104. Dupuis, Benjamin, George Deligiannidis, and Umut Şimşekli. 2023. “Generalization Bounds with Data-Dependent Fractal Dimensions.” arXiv Preprint arXiv:2302.02766. Ebli, Stefania, Michaël Defferrard, and Gard Spreemann. 2020. “Simplicial Neural Networks.” NeurIPS Workshop on Topological Data Analysis and Beyond. Eckmann, Beno. 1944. “Harmonische Funktionen Und Randwertaufgaben in Einem Komplex.” Commentarii Mathematici Helvetici 17 (1): 240–55. Edelsbrunner, Herbert, and John Harer. 2010. Computational Topology: An Introduction. American Mathematical Soc. Edelsbrunner, Herbert, John Harer, Ajith Mascarenhas, and Valerio Pascucci. 2004. “Time-Varying Reeb Graphs for Continuous Space-Time Data.” In Proceedings of the Twentieth Annual Symposium on Computational Geometry, 366–72. ACM. Efthymiou, Athanasios, Stevan Rudinac, Monika Kackovic, Marcel Worring, and Nachoem Wijnberg. 2021. “Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings.” arXiv Preprint arXiv:2105.08190. Elhamdadi, Hamza, Shaun Canavan, and Paul Rosen. 2021. “AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing.” IEEE Transactions on Visualization and Computer Graphics 28 (1): 769–79. Feng, Yifan, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. “Hypergraph Neural Networks.” Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 3558–65. Ferri, Massimo, Dott Mattia G. Bergomi, and Lorenzo Zu. 2018. “Simplicial Complexes from Graphs Towards Graph Persistence.” arXiv Preprint arXiv:1805.10716. Fey, Matthias, and Jan Eric Lenssen. 2019. “Fast Graph Representation Learning with PyTorch Geometric.” arXiv Preprint arXiv:1903.02428. Gabrielsson, Rickard Brüel, Bradley J. Nelson, Anjan Dwaraknath, and Primoz Skraba. 2020. “A Topology Layer for Machine Learning.” In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, edited by Silvia Chiappa and Roberto Calandra, 108:1553–63. #PMLR#. PMLR. Gallicchio, Claudio, and Alessio Micheli. 2010. “Graph Echo State Networks.” In The 2010 International Joint Conference on Neural Networks (IJCNN), 1–8. IEEE. Gallier, Jean. 2016. “Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: A Survey.” arXiv Preprint arXiv:1601.04692. Gao, Hongyang, and Shuiwang Ji. 2019. “Graph U-Nets.” In Icml, 2083–92. PMLR. Gao, Hongyang, Yi Liu, and Shuiwang Ji. 2021. “Topology-Aware Graph Pooling Networks.” Pami 43 (12): 4512–18. Gao, Yue, Yifan Feng, Shuyi Ji, and Rongrong Ji. 2022. “HGNN+: General Hypergraph Neural Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence. Gao, Yue, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing Zou. 2020. “Hypergraph Learning: Methods and Practices.” IEEE Transactions on Pattern Analysis and Machine Intelligence. Georgiev, Dobrik, Marc Brockschmidt, and Miltiadis Allamanis. 2022. “HEAT: Hyperedge Attention Networks.” Transactions on Machine Learning Research. Ghrist, Robert W. 2014. Elementary Applied Topology. Vol. 1. Createspace Seattle. Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. “Neural Message Passing for Quantum Chemistry.” In International Conference on Machine Learning. Girault, Benjamin, Shrikanth S. Narayanan, and Antonio Ortega. 2017. “Towards a Definition of Local Stationarity for Graph Signals.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Giusti, Chad, Robert Ghrist, and Danielle S. Bassett. 2016. “Two’s Company, Three (or More) Is a Simplex: Algebraic-Topological Tools for Understanding Higher-Order Structure in Neural Data.” Journal of Computational Neuroscience 41: 1. Giusti, Lorenzo, Claudio Battiloro, Paolo Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. 2022. “Simplicial Attention Networks.” arXiv Preprint arXiv:2203.07485. Giusti, Lorenzo, Claudio Battiloro, Lucia Testa, Paolo Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. 2022. “Cell Attention Networks.” arXiv Preprint arXiv:2209.08179. Goes, Fernando de, Mathieu Desbrun, and Yiying Tong. 2016. “Vector Field Processing on Triangle Meshes.” In ACM SIGGRAPH 2016 Courses, 1–49. Association for Computing Machinery. Goh, Christopher Wei Jin, Cristian Bodnar, and Pietro Lio. 2022. “Simplicial Attention Networks.” In ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Gong, Xue, Desmond J. Higham, and Konstantinos Zygalakis. 2023. “Generative Hypergraph Models and Spectral Embedding.” Scientific Reports 13 (1): 540. Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep Learning. Vol. 1. MIT Press Cambridge. https://mitpress.mit.edu/9780262035613/deep-learning/. Goyal, Palash, and Emilio Ferrara. 2018. “Graph Embedding Techniques, Applications, and Performance: A Survey.” Knowledge-Based Systems 151: 78–94. Grady, Leo J., and Jonathan R. Polimeni. 2010. Discrete Calculus: Applied Analysis on Graphs for Computational Science. Vol. 3. Springer. Grattarola, Daniele, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. 2022. “Understanding Pooling in Graph Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems. Hagberg, Aric, Pieter Swart, and Daniel S Chult. 2008. “Exploring Network Structure, Dynamics, and Function Using NetworkX.” Los Alamos National Lab (LANL), Los Alamos, NM (United States). Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. “Cell Complex Neural Networks.” In NeurIPS 2020 Workshop TDA and Beyond. Hajij, Mustafa, Karthikeyan Natesan Ramamurthy, Aldo Saenz, and Ghada Zamzmi. 2022. “High Skip Networks: A Higher Order Generalization of Skip Connections.” In ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Hajij, Mustafa, and Paul Rosen. 2020. “An Efficient Data Retrieval Parallel Reeb Graph Algorithm.” Algorithms 13 (10): 258. Hajij, Mustafa, Bei Wang, and Paul Rosen. 2018. “MOG: Mapper on Graphs for Relationship Preserving Clustering.” arXiv Preprint arXiv:1804.11242. Hajij, Mustafa, Bei Wang, Carlos Scheidegger, and Paul Rosen. 2018. “Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology.” In 2018 IEEE Pacific Visualization Symposium (PacificVis), 125–34. IEEE. Hajij, Mustafa, Ghada Zamzmi, Theodore Papamarkou, Vasileios Maroulas, and Xuanting Cai. 2022. “Simplicial Complex Representation Learning.” In Machine Learning on Graphs (MLoG) Workshop at ACM International WSD Conference. Halaoui, Hatem F. 2010. “Smart Traffic Online System (STOS): Presenting Road Networks with Time-Weighted Graphs.” In 2010 International Conference on Information Society, 349–56. IEEE. Hamilton, William L., Rex Ying, and Jure Leskovec. 2017. “Representation Learning on Graphs: Methods and Applications.” IEEE Data Engineering Bulletin 40 (3): 52–74. Hanocka, Rana, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. 2019. “MeshCNN: A Network with an Edge.” ACM Transactions on Graphics 38 (4): 1–12. Hansen, Jakob, and Robert Ghrist. 2019. “Toward a Spectral Theory of Cellular Sheaves.” Journal of Applied and Computational Topology 3 (4): 315–58. Hatcher, Allen. 2005. Algebraic Topology. Cambridge University Press. Hayhoe, Mikhail, Hans Riess, Victor M Preciado, and Alejandro Ribeiro. 2022. “Stable and Transferable Hyper-Graph Neural Networks.” arXiv Preprint arXiv:2211.06513. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. https://doi.org/10.1109/CVPR.2016.90. Hensel, Felix, Michael Moor, and Bastian Rieck. 2021. “A Survey of Topological Machine Learning Methods.” Frontiers in Artificial Intelligence 4: 681108. Hofer, Christoph, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. 2020. “Graph Filtration Learning.” In International Conference on Machine Learning, 4314–23. PMLR. Hofer, Christoph, Roland Kwitt, Marc Niethammer, and Andreas Uhl. 2017. “Deep Learning with Topological Signatures.” In Neurips, 1634–44. Horak, Danijela, Slobodan Maletić, and Milan Rajković. 2009. “Persistent Homology of Complex Networks.” Journal of Statistical Mechanics: Theory and Experiment, P03034. Hu, Xiaoling, Fuxin Li, Dimitris Samaras, and Chao Chen. 2019. “Topology-Preserving Deep Image Segmentation.” In Advances in Neural Information Processing Systems. Vol. 32. Curran Associates, Inc. Hu, Xiaoling, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. 2022. “Trigger Hunting with a Topological Prior for Trojan Detection.” In International Conference on Learning Representations. Huang, Jiahui, Tolga Birdal, Zan Gojcic, Leonidas J Guibas, and Shi-Min Hu. 2022. “Multiway Non-Rigid Point Cloud Registration via Learned Functional Map Synchronization.” IEEE Transactions on Pattern Analysis and Machine Intelligence. Huang, Jingjia, Zhangheng Li, Nannan Li, Shan Liu, and Ge Li. 2019. “AttPool: Towards Hierarchical Feature Representation in Graph Convolutional Networks via Attention Mechanism.” In Iccv, 6480–89. Huang, Jing, and Jie Yang. 2021. “UniGNN: A Unified Framework for Graph and Hypergraph Neural Networks.” In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI. Itoh, Takeshi D., Takatomi Kubo, and Kazushi Ikeda. 2022. “Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities.” Neural Networks 145: 356–73. Jha, Kanchan, Sriparna Saha, and Hiteshi Singh. 2022. “Prediction of Protein–Protein Interaction Using Graph Neural Networks.” Scientific Reports 12 (1): 1–12. Jiang, Jianwen, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. 2019. “Dynamic Hypergraph Neural Networks.” In IJCAI, 2635–41. Jiang, Weiwei, and Jiayun Luo. 2022. “Graph Neural Network for Traffic Forecasting: A Survey.” Expert Systems with Applications, 117921. Jogl, Fabian. 2022. “Do We Need to Improve Message Passing? Improving Graph Neural Networks with Graph Transformations.” PhD thesis, Vienna University of Technology. Joslyn, Cliff A, Sinan G Aksoy, Tiffany J Callahan, Lawrence E Hunter, Brett Jefferson, Brenda Praggastis, Emilie Purvine, and Ignacio J Tripodi. 2021. “Hypernetwork Science: From Multidimensional Networks to Computational Topology.” In Unifying Themes in Complex Systems x: Proceedings of the Tenth International Conference on Complex Systems, 377–92. Springer. Keros, Alexandros D., Vidit Nanda, and Kartic Subr. 2022. “Dist2Cycle: A Simplicial Neural Network for Homology Localization.” Proceedings of the AAAI Conference on Artificial Intelligence 36 (7): 7133–42. https://doi.org/10.1609/aaai.v36i7.20673. Kim, Eun-Sol, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak Zhang. 2020. “Hypergraph Attention Networks for Multimodal Learning.” In Cvpr, 14581–90. Kim, Kwangho, Jisu Kim, Manzil Zaheer, Joon Kim, Frédéric Chazal, and Larry Wasserman. 2020. “Pllay: Efficient Topological Layer Based on Persistent Landscapes.” Advances in Neural Information Processing Systems 33: 15965–77. Kim, Vladimir G, Yaron Lipman, Xiaobai Chen, and Thomas Funkhouser. 2010. “Möbius Transformations for Global Intrinsic Symmetry Analysis.” Computer Graphics Forum 29 (5): 1689–1700. Kipf, Thomas N., and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907. Kivelä, Mikko, Alex Arenas, Marc Barthelemy, James P Gleeson, Yamir Moreno, and Mason A Porter. 2014. “Multilayer Networks.” Journal of Complex Networks 2 (3): 203–71. Klette, Reinhard. 2000. “Cell Complexes Through Time.” In Vision Geometry IX, 4117:134–45. SPIE. Knoke, David, and Song Yang. 2019. Social Network Analysis. SAGE publications. Kokkinos, Iasonas, Michael M Bronstein, Roee Litman, and Alex M Bronstein. 2012. “Intrinsic Shape Context Descriptors for Deformable Shapes.” In Cvpr, 159–66. IEEE. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2017. “Imagenet Classification with Deep Convolutional Neural Networks.” Communications of the ACM 60 (6): 84–90. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems. https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html. Kunegis, Jérôme, Stephan Schmidt, Andreas Lommatzsch, Jürgen Lerner, Ernesto W De Luca, and Sahin Albayrak. 2010. “Spectral Analysis of Signed Graphs for Clustering, Prediction and Visualization.” In Proceedings of the 2010 SIAM International Conference on Data Mining, 559–70. SIAM. Kusano, Genki, Yasuaki Hiraoka, and Kenji Fukumizu. 2016. “Persistence Weighted Gaussian Kernel for Topological Data Analysis.” In Icml, 2004–13. Kushnir, Dan, Meirav Galun, and Achi Brandt. 2006. “Fast Multiscale Clustering and Manifold Identification.” Pattern Recognition 39 (10): 1876–91. Kweon, In So, and Takeo Kanade. 1994. “Extracting Topographic Terrain Features from Elevation Maps.” CVGIP: Image Understanding 59 (2): 171–82. La Gatta, Valerio, Vincenzo Moscato, Mirko Pennone, Marco Postiglione, and Giancarlo Sperlı́. 2022. “Music Recommendation via Hypergraph Embedding.” IEEE Transactions on Neural Networks and Learning Systems. Lambiotte, Renaud, Martin Rosvall, and Ingo Scholtes. 2019. “From Networks to Optimal Higher-Order Models of Complex Systems.” Nature Physics 15 (4): 313–20. LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://ieeexplore.ieee.org/document/726791. Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Boong-Nyun Kim, and Dong Soo Lee. 2011a. “Computing the Shape of Brain Networks Using Graph Filtration and Gromov-Hausdorff Metric.” International Conference on Medical Image Computing and Computer Assisted Intervention, 302–9. Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Bung-Nyun Kim, and Dong Soo Lee. 2011b. “Discriminative Persistent Homology of Brain Networks.” IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 841–44. Lee, Hyekyoung, Hyejin Kang, Moo K. Chung, Bung-Nyun Kim, and Dong Soo Lee. 2012a. “Persistent Brain Network Homology from the Perspective of Dendrogram.” IEEE Transactions on Medical Imaging 31 (12): 2267–77. ———. 2012b. “Weighted Functional Brain Network Modeling via Network Filtration.” NIPS Workshop on Algebraic Topology and Machine Learning. Lee, John Boaz, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. 2019. “Attention Models in Graphs: A Survey.” ACM Transactions on Knowledge Discovery from Data 13 (6): 1–25. Lee, John Boaz, Ryan Rossi, and Xiangnan Kong. 2018. “Graph Classification Using Structural Attention.” In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. “Self-Attention Graph Pooling.” In Icml, 3734–43. PMLR. Leventhal, Samuel, Attila Gyulassy, Mark Heimann, and Valerio Pascucci. 2023. “Exploring Classification of Topological Priors with Machine Learning for Feature Extraction.” IEEE Transactions on Visualization and Computer Graphics. Li, Juanhui, Yao Ma, Yiqi Wang, Charu Aggarwal, Chang-Dong Wang, and Jiliang Tang. 2020. “Graph Pooling with Representativeness.” In 2020 IEEE International Conference on Data Mining (ICDM), 302–11. IEEE. Li, Zhifei, Hai Liu, Zhaoli Zhang, Tingting Liu, and Neal N Xiong. 2021. “Learning Knowledge Graph Embedding with Heterogeneous Relation Attention Networks.” IEEE Transactions on Neural Networks and Learning Systems. Lian, Z., A. Godil, B. Bustos, M Daoudi, J. Hermans, S. Kawamura, Y. Kurita, G. Lavoua, P. Dp Suetens, et al. 2011. “Shape Retrieval on Non-Rigid 3D Watertight Meshes.” In Eurographics Workshop on 3d Object Retrieval (3DOR). Citeseer. Lim, Lek-Heng. 2020. “Hodge Laplacians on Graphs.” SIAM Review 62 (3): 685–715. Linka, Kevin, Mathias Peirlinck, Francisco Sahli Costabal, and Ellen Kuhl. 2020. “Outbreak Dynamics of COVID-19 in Europe and the Effect of Travel Restrictions.” Computer Methods in Biomechanics and Biomedical Engineering 23 (11): 710–17. Lo, Derek, and Briton Park. 2016. “Modeling the Spread of the Zika Virus Using Topological Data Analysis.” arXiv Preprint arXiv:1612.03554. Loukas, Andreas. 2019. “What Graph Neural Networks Cannot Learn: Depth Vs Width.” arXiv Preprint arXiv:1907.03199. Love, Ephy R., Benjamin Filippenko, Vasileios Maroulas, and Gunnar Carlsson. 2023a. “Topological Convolutional Layers for Deep Learning.” Jmlr 24 (59): 1–35. ———. 2023b. “Topological Convolutional Layers for Deep Learning.” Jmlr 24 (59): 1–35. Lum, P. Y., G. Singh, A. Lehman, T. Ishkanov, Mikael Vejdemo-Johansson, M. Alagappan, J. Carlsson, and G. Carlsson. 2013. “Extracting Insights from the Shape of Complex Data Using Topology.” Scientific Reports 3: 1236. Ma, Yao, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019. “Graph Convolutional Networks with Eigenpooling.” In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 723–31. Majhi, Soumen, Matjaž Perc, and Dibakar Ghosh. 2022. “Dynamics on Higher-Order Networks: A Review.” Journal of the Royal Society Interface 19 (188): 20220043. Maletić, Slobodan, Yi Zhao, and Milan Rajković. 2016. “Persistent Topological Features of Dynamical Systems.” Chaos: An Interdisciplinary Journal of Nonlinear Science 26 (5): 053105. Manrı́quez, Ronald, Camilo Guerrero-Nancuante, and Carla Taramasco. 2021. “Protection Strategy Against an Epidemic Disease on Edge-Weighted Graphs Applied to a COVID-19 Case.” Biology 10 (7): 667. Maron, Haggai, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. 2019. “Provably Powerful Graph Networks.” arXiv Preprint arXiv:1905.11136. Maron, Haggai, Meirav Galun, Noam Aigerman, Miri Trope, Nadav Dym, Ersin Yumer, Vladimir G Kim, and Yaron Lipman. 2017. “Convolutional Neural Networks on Surfaces via Seamless Toric Covers.” ACM Transactions on Graphics 36 (4): 71–71. Masci, Jonathan, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. 2015. “Geodesic Convolutional Neural Networks on Riemannian Manifolds.” In Conference on Computer Vision and Pattern Recognition. Mejia, Daniel, Oscar Ruiz-Salguero, and Carlos A. Cadavid. 2017. “Spectral-Based Mesh Segmentation.” International Journal on Interactive Design and Manufacturing 11 (3): 503–14. Mendel, Jerry M. 1991. “Tutorial on Higher-Order Statistics (Spectra) in Signal Processing and System Theory: Theoretical Results and Some Applications.” Proceedings of the IEEE 79 (3): 278–305. Menichetti, Giulia, Luca Dall’Asta, and Ginestra Bianconi. 2016. “Control of Multilayer Networks.” Scientific Reports 6 (1): 1–8. Mesquita, Diego, Amauri Souza, and Samuel Kaski. 2020. “Rethinking Pooling in Graph Neural Networks.” Neurips 33: 2220–31. Milano, Francesco, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, and Luca Carlone. 2020. “Primal-Dual Mesh Convolutional Neural Networks.” Conference on Neural Information Processing Systems 33: 952–63. Mitchell, Edward C., Brittany Story, David Boothe, Piotr J. Franaszczuk, and Vasileios Maroulas. 2022. “A Topological Deep Learning Framework for Neural Spike Decoding.” arXiv Preprint arXiv:2212.05037. Mitchell, Tom M. 1980. “The Need for Biases in Learning Generalizations.” Mnih, Volodymyr, Nicolas Heess, Alex Graves, et al. 2014. “Recurrent Models of Visual Attention.” In Advances in Neural Information Processing Systems. Vol. 27. Monti, Federico, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M. Bronstein. 2017. “Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.” In Cvpr, 5115–24. Moor, Michael, Max Horn, Bastian Rieck, and Karsten Borgwardt. 2020. “Topological Autoencoders.” In International Conference on Machine Learning, 7045–54. PMLR. Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. “Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence. Munkres, James R. 1974. Topology; a First Course. Prentice-Hall. ———. 2018. Elements of Algebraic Topology. CRC press. Murgas, Kevin A., Emil Saucan, and Romeil Sandhu. 2022. “Hypergraph Geometry Reflects Higher-Order Dynamics in Protein Interaction Networks.” Scientific Reports 12 (1): 20879. Neyshabur, Behnam, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. 2019. “The Role of over-Parametrization in Generalization of Neural Networks.” In International Conference on Learning Representations. Nicolau, Monica, Arnold J. Levine, and Gunnar Carlsson. 2011. “Topology Based Data Analysis Identifies a Subgroup of Breast Cancers with a Unique Mutational Profile and Excellent Survival.” Proceedings of the National Academy of Sciences 108 (17): 7265–70. Oballe, Christopher, Alan Cherne, Dave Boothe, Scott Kerick, Piotr J Franaszczuk, and Vasileios Maroulas. 2021. “Bayesian Topological Signal Processing.” Discrete &amp; Continuous Dynamical Systems-S. Ortega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. “Graph Signal Processing: Overview, Challenges, and Applications.” Proceedings of the IEEE 106 (5): 808–28. Pang, Yunsheng, Yunxiang Zhao, and Dongsheng Li. 2021. “Graph Pooling via Coarsened Graph Infomax.” In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2177–81. Papillon, Mathilde, Sophia Sanborn, Mustafa Hajij, and Nina Miolane. 2023. “Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.” arXiv Preprint arXiv:2304.10031. Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. “Automatic Differentiation in PyTorch.” In NIPS Workshop. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Jmlr 12: 2825–30. Perea, Jose A., Anastasia Deckard, Steve B. Haase, and John Harer. 2015. “SW1PerS: Sliding Windows and 1-Persistence Scoring; Discovering Periodicity in Gene Expression Time Series Data.” BMC Bioinformatics 16 (1): 257. Petri, Giovanni, Martina Scolamiero, Irene Donato, and Francesco Vaccarino. 2013a. “Networks and Cycles: A Persistent Homology Approach to Complex Networks.” Proceedings European Conference on Complex Systems 2012, Springer Proceedings in Complexity, 93–99. ———. 2013b. “Topological Strata of Weighted Complex Networks.” PLoS ONE 8 (6). Piaggesi, Simone, André Panisson, and Giovanni Petri. 2022. “Effective Higher-Order Link Prediction and Reconstruction from Simplicial Complex Embeddings.” In Learning on Graphs Conference, 55–51. PMLR. Plizzari, Chiara, Marco Cannici, and Matteo Matteucci. 2021. “Spatial Temporal Transformer Network for Skeleton-Based Action Recognition.” In Icpr, 694–701. Springer. Pun, Chi Seng, Kelin Xia, and Si Xian Lee. 2018. “Persistent-Homology-Based Machine Learning and Its Applications–a Survey.” arXiv Preprint arXiv:1811.00252. Qi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” In Cvpr, 652–60. Reddy, Thummaluru Siddartha, Sundeep Prabhakar Chepuri, and Pierre Borgnat. 2023. “Clustering with Simplicial Complexes.” arXiv Preprint arXiv:2303.07646. Rempe, Davis, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar, and Leonidas J Guibas. 2020. “CASPR: Learning Canonical Spatiotemporal Point Cloud Representations.” Neurips 33: 13688–701. Rieck, Bastian, Christian Bock, and Karsten Borgwardt. 2019. “A Persistent Weisfeiler-Lehman Procedure for Graph Classification.” In International Conference on Machine Learning, 5448–58. PMLR. Rieck, Bastian, and Heike Leitte. 2015. “Persistent Homology for the Evaluation of Dimensionality Reduction Schemes.” Computer Graphics Forum 34 (3): 431–40. Rieck, Bastian, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy Wolf, Nick Turk-Browne, and Smita Krishnaswamy. 2020. “Uncovering the Topology of Time-Varying fMRI Data Using Cubical Persistence.” In Advances in Neural Information Processing Systems (NeurIPS), edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6900–6912. Curran Associates, Inc. Robinson, Michael. 2014. Topological Signal Processing. Vol. 81. Springer. Roddenberry, T Mitchell, and Santiago Segarra. 2019. “HodgeNet: Graph Neural Networks for Edge Data.” In 2019 53rd Asilomar Conference on Signals, Systems, and Computers, 220–24. IEEE. Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021. “Principled Simplicial Neural Networks for Trajectory Prediction.” In International Conference on Machine Learning. Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022. “Signal Processing on Cell Complexes.” In IEEE International Conference on Acoustics, Speech and Signal Processing. Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. “High-Resolution Image Synthesis with Latent Diffusion Models.” In Computer Vision and Pattern Recognition. https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600k0674/1H1iFsO7Zuw. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” In International Conference on Medical Image Computing and Computer-Assisted Intervention, 234–41. Springer. Rosen, Paul, Bei Wang, Anil Seth, Betsy Mills, Adam Ginsburg, Julia Kamenetzky, Jeff Kern, and Chris R Johnson. 2017. “Using Contour Trees in the Analysis and Visualization of Radio Astronomy Data Cubes.” arXiv Preprint arXiv:1704.04561, 1–7. Sanchez-Gonzalez, Alvaro, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. 2020. “Learning to Simulate Complex Physics with Graph Networks.” In International Conference on Machine Learning. Santoro, Adam, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. 2017. “A Simple Neural Network Module for Relational Reasoning.” In Advances in Neural Information Processing Systems. Santoro, Andrea, Federico Battiston, Giovanni Petri, and Enrico Amico. 2023. “Higher-Order Organization of Multivariate Time Series.” Nature Physics, 1–9. Sardellitti, Stefania, and Sergio Barbarossa. 2022. “Topological Signal Representation and Processing over Cell Complexes.” arXiv Preprint arXiv:2201.08993. Sardellitti, Stefania, Sergio Barbarossa, and Lucia Testa. 2021. “Topological Signal Processing over Cell Complexes.” Proceeding IEEE Asilomar Conference. Signals, Systems and Computers. Savoy, Maxime. 2021. “Combinatorial Cell Complexes: Duality, Reconstruction and Causal Cobordisms.” PhD thesis, École Polytechnique Fédérale de Lausanne. Scarselli, Franco, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 20 (1): 61–80. Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. 2020. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62 (2): 353–91. Schaub, Michael T., Jean-Baptiste Seby, Florian Frantzen, T. Mitchell Roddenberry, Yu Zhu, and Santiago Segarra. 2022. “Signal Processing on Simplicial Complexes.” In Higher-Order Systems, 301–28. Springer. Schaub, Michael T., and Santiago Segarra. 2018. “Flow Smoothing and Denoising: Graph Signal Processing in the Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39. Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 2021. “Signal Processing on Higher-Order Networks: Livin’on the Edge... And Beyond.” Signal Processing 187: 108149. Schiff, Yair, Vijil Chenthamarakshan, Karthikeyan Natesan Ramamurthy, and Payel Das. 2020. “Characterizing the Latent Space of Molecular Deep Generative Models with Persistent Homology Metrics.” arXiv Preprint arXiv:2010.08548. Schlichtkrull, Michael, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. “Modeling Relational Data with Graph Convolutional Networks.” In European Semantic Web Conference. Sharp, Nicholas, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. 2022. “DiffusionNet: Discretization Agnostic Learning on Surfaces.” Tog 41 (3): 1–16. Shi, Heyuan, Yubo Zhang, Zizhao Zhang, Nan Ma, Xibin Zhao, Yue Gao, and Jiaguang Sun. 2018. “Hypergraph-Induced Convolutional Networks for Visual Classification.” IEEE Transactions on Neural Networks and Learning Systems 30 (10): 2963–72. Shlomi, Jonathan, Peter Battaglia, and Jean-Roch Vlimant. 2020. “Graph Neural Networks in Particle Physics.” Machine Learning: Science and Technology 2 (2): 021001. Shuman, David I., Benjamin Ricaud, and Pierre Vandergheynst. 2016. “Vertex-Frequency Analysis on Graphs.” Applied and Computational Harmonic Analysis 40 (2): 260–91. Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv Preprint arXiv:1409.1556. https://arxiv.org/abs/1409.1556. Singh, Gurjeet, Facundo Mémoli, Gunnar E Carlsson, et al. 2007. “Topological Methods for the Analysis of High Dimensional Data Sets and 3d Object Recognition.” PBG@ Eurographics 2: 091–100. Skardal, Per Sebastian, Lluı́s Arola-Fernández, Dane Taylor, and Alex Arenas. 2021. “Higher-Order Interactions Improve Optimal Collective Dynamics on Networks.” arXiv Preprint arXiv:2108.08190. Smirnov, Dmitriy, and Justin Solomon. 2021. “HodgeNet: Learning Spectral Geometry on Triangle Meshes.” ACM Transactions on Graphics 40 (4): 1–11. Su, Zidong, Zehui Hu, and Yangding Li. 2021. “Hierarchical Graph Representation Learning with Local Capsule Pooling.” In ACM International Conference on Multimedia in Asia. Sun, Yizhou, Jiawei Han, Peixiang Zhao, Zhijun Yin, Hong Cheng, and Tianyi Wu. 2009. “RankClus: Integrating Clustering with Ranking for Heterogeneous Information Network Analysis.” In Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology. Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In Advances in Neural Information Processing Systems. https://arxiv.org/abs/1409.3215. Tabassum, Shazia, Fabiola SF Pereira, Sofia Fernandes, and João Gama. 2018. “Social Network Analysis: An Overview.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8 (5): e1256. Taylor, Dane, Florian Klimm, Heather A Harrington, Miroslav Kramár, Konstantin Mischaikow, Mason A Porter, and Peter J Mucha. 2015. “Topological Data Analysis of Contagion Maps for Examining Spreading Processes on Networks.” Nature Communications 6: 7723. Topaz, Chad M, Lori Ziegelmeier, and Tom Halverson. 2015. “Topological Data Analysis of Biological Aggregation Models.” PloS One 10 (5): e0126383. Torres, Leo, Ann S Blevins, Danielle Bassett, and Tina Eliassi-Rad. 2021. “The Why, How, and When of Representations for Complex Systems.” SIAM Review 63 (3): 435–85. Trask, Nathaniel, Andy Huang, and Xiaozhe Hu. 2022. “Enforcing Exact Physics in Scientific Machine Learning: A Data-Driven Exterior Calculus on Graphs.” Journal of Computational Physics. Turaev, Vladimir G. 2016. Quantum Invariants of Knots and 3-Manifolds. Vol. 18. Walter de Gruyter GmbH &amp; Co KG. Umeda, Yuhei. 2017. “Time Series Classification via Topological Data Analysis.” Information and Media Technologies 12: 228–39. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Veličković, Petar. 2022. “Message Passing All the Way Up.” ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. “Graph Attention Networks.” In International Conference on Learning Representations. Wachs, Michelle L. 2006. “Poset Topology: Tools and Applications.” arXiv Preprint Math/0602226. Waibel, Dominik J. E., Scott Atwell, Matthias Meier, Carsten Marr, and Bastian Rieck. 2022. “Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction.” In Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, edited by Linwei Wang, Qi Dou, P. Thomas Fletcher, Stefanie Speidel, and Shuo Li, 150–59. Lecture Notes in Computer Science. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-16440-8_15. Wang, Cheng, Nan Ma, Zhixuan Wu, Jin Zhang, and Yongqiang Yao. 2023. “Survey of Hypergraph Neural Networks and Its Application to Action Recognition.” In Artificial Intelligence: Second CAAI International Conference, CICAI 2022, Beijing, China, August 27–28, 2022, Revised Selected Papers, Part II, 387–98. Springer. Wang, Fan, Huidong Liu, Dimitris Samaras, and Chao Chen. 2020. “Topogan: A Topology-Aware Generative Adversarial Network.” In Eccv, 118–36. Springer. Wang, Yunhai, Shmulik Asafi, Oliver Van Kaick, Hao Zhang, Daniel Cohen-Or, and Baoquan Chen. 2012. “Active Co-Analysis of a Set of Shapes.” ACM Transactions on Graphics 31 (6): 1–10. Weinan, E., Luan Jianfeng, and Yao Yuan. 2013. “The Landscape of Complex Networks: Critical Nodes and a Hierarchical Decomposition.” Methods and Applications of Analysis 20: 383–404. Weisfeiler, Boris, and Andrei Leman. 1968. “The Reduction of a Graph to Canonical Form and the Algebra Which Appears Therein.” NTI, Series 2 (9): 12–16. Williams, Francis. 2022. “Point Cloud Utils.” Wu, Hanrui, and Michael K. Ng. 2022. “Hypergraph Convolution on Nodes-Hyperedges Network for Semi-Supervised Node Classification.” ACM Transactions on Knowledge Discovery from Data (TKDD) 16 (4): 1–19. Wu, Zhirong, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 2015. “3D ShapeNets: A Deep Representation for Volumetric Shapes.” In Cvpr, 1912–20. Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. “A Comprehensive Survey on Graph Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems 32 (1): 4–24. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2024. Bookdown: Authoring Books and Technical Documents with R Markdown. https://github.com/rstudio/bookdown. Xu, Chenxin, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. 2022. “GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning.” In Conference on Computer Vision and Pattern Recognition. Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. “How Powerful Are Graph Neural Networks?” arXiv Preprint arXiv:1810.00826. Yan, Sijie, Yuanjun Xiong, and Dahua Lin. 2018. “Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition.” In Thirty-Second AAAI Conference on Artificial Intelligence. Yang, Maosheng, and Elvin Isufi. 2023. “Convolutional Learning on Simplicial Complexes.” arXiv Preprint arXiv:2301.11163. Yang, Maosheng, Elvin Isufi, Michael T Schaub, and Geert Leus. 2021. “Finite Impulse Response Filters for Simplicial Complexes.” In 2021 29th European Signal Processing Conference (EUSIPCO), 2005–9. IEEE. Yin, Nan, Fuli Feng, Zhigang Luo, Xiang Zhang, Wenjie Wang, Xiao Luo, Chong Chen, and Xian-Sheng Hua. 2022. “Dynamic Hypergraph Convolutional Network.” In 2022 IEEE 38th International Conference on Data Engineering (ICDE), 1621–34. IEEE. Ying, Zhitao, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. “Hierarchical Graph Representation Learning with Differentiable Pooling.” Neurips 31. Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. “Deep Sets.” In Advances in Neural Information Processing Systems. Zeng, Sebastian, Florian Graf, Christoph Hofer, and Roland Kwitt. 2021. “Topological Attention for Time Series Forecasting.” In Advances in Neural Information Processing Systems, 34:24871–82. Curran Associates, Inc. Zhang, Qi, Qizhao Jin, Jianlong Chang, Shiming Xiang, and Chunhong Pan. 2018. “Kernel-Weighted Graph Convolutional Network: A Deep Learning Approach for Traffic Forecasting.” In 2018 24th International Conference on Pattern Recognition (ICPR), 1018–23. IEEE. Zhang, Shi-Xue, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, and Xu-Cheng Yin. 2020. “Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection.” In Cvpr, 9699–9708. Zhang, Weifeng, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. “Multiplex Graph Neural Networks for Multi-Behavior Recommendation.” In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, 2313–16. Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Zhao Li, Chengwei Yao, Huifen Dai, Zhi Yu, and Can Wang. 2021. “Hierarchical Multi-View Graph Pooling with Structure Learning.” IEEE Transactions on Knowledge and Data Engineering 35 (1): 545–59. Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. 2019. “Hierarchical Graph Pooling with Structure Learning.” arXiv Preprint arXiv:1911.05954. Zhao, Lingxiao, Wei Jin, Leman Akoglu, and Neil Shah. 2022. “From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness.” In International Conference on Learning Representations. Zhao, Yongheng, Guangchi Fang, Yulan Guo, Leonidas Guibas, Federico Tombari, and Tolga Birdal. 2022. “3DPointCaps++: Learning 3D Representations with Capsule Networks.” Ijcv 130 (9): 2321–36. Zhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. “Graph Neural Networks: A Review of Methods and Applications.” AI Open 1: 57–81. Zhu, Jianming, Junlei Zhu, Smita Ghosh, Weili Wu, and Jing Yuan. 2018. “Social Influence Maximization in Hypergraph in Social Networks.” IEEE Transactions on Network Science and Engineering 6 (4): 801–11. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
