<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>第 6 章 消息传递 | 拓扑深度学习：超越图数据</title>
  <meta name="description" content="一本关于拓扑深度学习的书。" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="第 6 章 消息传递 | 拓扑深度学习：超越图数据" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="一本关于拓扑深度学习的书。" />
  <meta name="github-repo" content="pyt-team/tdlbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 6 章 消息传递 | 拓扑深度学习：超越图数据" />
  
  <meta name="twitter:description" content="一本关于拓扑深度学习的书。" />
  

<meta name="author" content="Mustafa Hajij, Theodore Papamarkou, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Michael T. Schaub" />


<meta name="date" content="2024-09-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="combinatorial-complex-neural-networks.html"/>
<link rel="next" href="push-forward-pooling-and-unpooling.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/glossarybox.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">拓扑深度学习</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>贡献者</a></li>
<li class="chapter" data-level="" data-path="译者.html"><a href="译者.html"><i class="fa fa-check"></i>译者</a></li>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html"><i class="fa fa-check"></i>序言</a>
<ul>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html#编译"><i class="fa fa-check"></i>编译</a></li>
<li class="chapter" data-level="" data-path="序言.html"><a href="序言.html#致谢"><i class="fa fa-check"></i>致谢</a></li>
</ul></li>
<li class="part"><span><b>第一部分：基础知识</b></span></li>
<li class="chapter" data-level="1" data-path="引言.html"><a href="引言.html"><i class="fa fa-check"></i><b>1</b> 引言</a></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> 研究动机</a>
<ul>
<li class="chapter" data-level="2.1" data-path="motivation.html"><a href="motivation.html#从拓扑空间数据中建模和学习"><i class="fa fa-check"></i><b>2.1</b> 从拓扑空间数据中建模和学习</a></li>
<li class="chapter" data-level="2.2" data-path="motivation.html"><a href="motivation.html#the-utility-of-topology"><i class="fa fa-check"></i><b>2.2</b> 拓扑的有用性</a></li>
<li class="chapter" data-level="2.3" data-path="motivation.html"><a href="motivation.html#深度学习和结构化计算的统一视角"><i class="fa fa-check"></i><b>2.3</b> 深度学习和结构化计算的统一视角</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>3</b> 预备知识</a>
<ul>
<li class="chapter" data-level="3.1" data-path="preliminaries.html"><a href="preliminaries.html#邻域函数和拓扑空间"><i class="fa fa-check"></i><b>3.1</b> 邻域函数和拓扑空间</a></li>
<li class="chapter" data-level="3.2" data-path="preliminaries.html"><a href="preliminaries.html#bridging-the-gap-among-higher-order-networks"><i class="fa fa-check"></i><b>3.2</b> 填补与高阶网络间的代沟</a></li>
<li class="chapter" data-level="3.3" data-path="preliminaries.html"><a href="preliminaries.html#hierarchical-structure-and-set-type-relations"><i class="fa fa-check"></i><b>3.3</b> 层次化结构与集合型关系</a></li>
</ul></li>
<li class="part"><span><b>第二部分:组合复形</b></span></li>
<li class="chapter" data-level="4" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html"><i class="fa fa-check"></i><b>4</b> 组合复形</a>
<ul>
<li class="chapter" data-level="4.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc-definition"><i class="fa fa-check"></i><b>4.1</b> 组合复形定义</a></li>
<li class="chapter" data-level="4.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc-homomorphisms-and-sub-ccs"><i class="fa fa-check"></i><b>4.2</b> CC同态和子CCs</a></li>
<li class="chapter" data-level="4.3" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#motivation-for-ccs"><i class="fa fa-check"></i><b>4.3</b> 引入CCs的动机</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#pooling-operations-on-ccs"><i class="fa fa-check"></i><b>4.3.1</b> CCs上的池化操作</a></li>
<li class="chapter" data-level="4.3.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#structural-advantages-of-ccs"><i class="fa fa-check"></i><b>4.3.2</b> CCs的结构化优势</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#neighbourhood-functions-on-ccs"><i class="fa fa-check"></i><b>4.4</b> CCs上的邻域函数</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#incidence-in-a-cc"><i class="fa fa-check"></i><b>4.4.1</b> CC中的关联关系（Incidence）</a></li>
<li class="chapter" data-level="4.4.2" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#cc内的邻接关系adjacency"><i class="fa fa-check"></i><b>4.4.2</b> CC内的邻接关系（Adjacency）</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="combinatorial-complexes.html"><a href="combinatorial-complexes.html#data-on-ccs"><i class="fa fa-check"></i><b>4.5</b> CCs上的数据</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html"><i class="fa fa-check"></i><b>5</b> 组合复形神经网络（Combinatorial complex neural networks）</a>
<ul>
<li class="chapter" data-level="5.1" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#building-ccnns-tensor-diagrams"><i class="fa fa-check"></i><b>5.1</b> 构建 CCNN：张量图</a></li>
<li class="chapter" data-level="5.2" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#push-forward-operator-and-merge-node"><i class="fa fa-check"></i><b>5.2</b> 前推操作（Push-forward operator）和聚合节点</a></li>
<li class="chapter" data-level="5.3" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#the-main-three-tensor-operations"><i class="fa fa-check"></i><b>5.3</b> 三种主要的张量操作</a></li>
<li class="chapter" data-level="5.4" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#definition-of-combinatorial-complex-convolutional-networks"><i class="fa fa-check"></i><b>5.4</b> 组合复形卷积网络的定义（combinatorial complex convolutional networks）</a></li>
<li class="chapter" data-level="5.5" data-path="combinatorial-complex-neural-networks.html"><a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks"><i class="fa fa-check"></i><b>5.5</b> 组合复形注意力神经网络</a></li>
</ul></li>
<li class="part"><span><b>第三部分：高阶消息传递（Higher-order message passing）</b></span></li>
<li class="chapter" data-level="6" data-path="message-passing.html"><a href="message-passing.html"><i class="fa fa-check"></i><b>6</b> 消息传递</a>
<ul>
<li class="chapter" data-level="6.1" data-path="message-passing.html"><a href="message-passing.html#definition-of-higher-order-message-passing"><i class="fa fa-check"></i><b>6.1</b> 高阶消息传递的定义</a></li>
<li class="chapter" data-level="6.2" data-path="message-passing.html"><a href="message-passing.html#higher-order-message-passing-neural-networks-are-ccnns"><i class="fa fa-check"></i><b>6.2</b> 高阶消息传递神经网络就是CCNNs</a></li>
<li class="chapter" data-level="6.3" data-path="message-passing.html"><a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison"><i class="fa fa-check"></i><b>6.3</b> 聚合节点和高阶消息传递：量化比较</a></li>
<li class="chapter" data-level="6.4" data-path="message-passing.html"><a href="message-passing.html#attention-higher-order-message-passing-and-ccanns"><i class="fa fa-check"></i><b>6.4</b> 注意力高阶消息传递和CCANNs</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html"><i class="fa fa-check"></i><b>7</b> 前推、池化和反池化</a>
<ul>
<li class="chapter" data-level="7.1" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#cc-pooling-and-unpooling"><i class="fa fa-check"></i><b>7.1</b> CC池化和反池化</a></li>
<li class="chapter" data-level="7.2" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#formulating-common-pooling-operations-as-cc-pooling"><i class="fa fa-check"></i><b>7.2</b> 将常见的池化操作表述为 CC-pooling</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#graph-pooling-as-cc-pooling"><i class="fa fa-check"></i><b>7.2.1</b> 用CC-pooling表示图池化操作</a></li>
<li class="chapter" data-level="7.2.2" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#image-pooling-as-cc-pooling"><i class="fa fa-check"></i><b>7.2.2</b> 图像池化作为CC-pooing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#pooling-and-unpooling-ccnns"><i class="fa fa-check"></i><b>7.3</b> 池化与反池化CCNNs</a></li>
<li class="chapter" data-level="7.4" data-path="push-forward-pooling-and-unpooling.html"><a href="push-forward-pooling-and-unpooling.html#mapper-and-the-cc-pooling-operation"><i class="fa fa-check"></i><b>7.4</b> 映射器和CC池化操作</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html"><i class="fa fa-check"></i><b>8</b> CCNNs的Hasse图解释</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#hasse-graph-interpretation-of-ccnns-2"><i class="fa fa-check"></i><b>8.1</b> CCNNs的Hasse图解释</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#ccs-as-hasse-graphs"><i class="fa fa-check"></i><b>8.1.1</b> CCs作为Hasse图</a></li>
<li class="chapter" data-level="8.1.2" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#augmented-hasse-graphs"><i class="fa fa-check"></i><b>8.1.2</b> 增强的Hasse图</a></li>
<li class="chapter" data-level="8.1.3" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#reducibility-of-ccnns-to-graph-basedmodels"><i class="fa fa-check"></i><b>8.1.3</b> CCNN对图模型的归约能力</a></li>
<li class="chapter" data-level="8.1.4" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#augmented-hasse-graphs-and-cc-pooling"><i class="fa fa-check"></i><b>8.1.4</b> 增强Hasse图和CC-pooling</a></li>
<li class="chapter" data-level="8.1.5" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#augmented-hasse-diagrams-message-passing-and-mergenodes"><i class="fa fa-check"></i><b>8.1.5</b> 增强Hasse图消息传递和聚合节点</a></li>
<li class="chapter" data-level="8.1.6" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#higher-order-representation-learning"><i class="fa fa-check"></i><b>8.1.6</b> 高阶表征学习</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#on-the-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2</b> CCNNs的等变性</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#permutation-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2.1</b> CCNNs的置换等变</a></li>
<li class="chapter" data-level="8.2.2" data-path="hasse-graph-interpretation-of-ccnns-1.html"><a href="hasse-graph-interpretation-of-ccnns-1.html#orientation-equivariance-of-ccnns"><i class="fa fa-check"></i><b>8.2.2</b> CCNNs的方向等变</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>第四部分：应用，文献和结论</b></span></li>
<li class="chapter" data-level="9" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html"><i class="fa fa-check"></i><b>9</b> 实现与实验</a>
<ul>
<li class="chapter" data-level="9.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#software-toponetx-topoembedx-and-topomodelx"><i class="fa fa-check"></i><b>9.1</b> 软件：TopoNetX, TopoEmbedX, and TopoModelX</a></li>
<li class="chapter" data-level="9.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#datasets"><i class="fa fa-check"></i><b>9.2</b> 数据集</a></li>
<li class="chapter" data-level="9.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#shape-analysis-mesh-segmentation-and-classification"><i class="fa fa-check"></i><b>9.3</b> 形状分析：网格分割与分类</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-segmentation"><i class="fa fa-check"></i><b>9.3.1</b> 网格分割</a></li>
<li class="chapter" data-level="9.3.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-and-point-cloud-classification"><i class="fa fa-check"></i><b>9.3.2</b> Mesh and point cloud classification</a></li>
<li class="chapter" data-level="9.3.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#graph-classification"><i class="fa fa-check"></i><b>9.3.3</b> Graph classification</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#pooling-with-mapper-on-graphs-and-data-classification"><i class="fa fa-check"></i><b>9.4</b> Pooling with mapper on graphs and data classification</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-classification-cc-pooling-with-input-vertex-and-edge-features"><i class="fa fa-check"></i><b>9.4.1</b> Mesh classification: CC-pooling with input vertex and edge features</a></li>
<li class="chapter" data-level="9.4.2" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#mesh-classification-cc-pooling-with-input-vertex-features-only"><i class="fa fa-check"></i><b>9.4.2</b> Mesh classification: CC-pooling with input vertex features only</a></li>
<li class="chapter" data-level="9.4.3" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#point-cloud-classification-cc-pooling-with-input-vertex-features-only"><i class="fa fa-check"></i><b>9.4.3</b> Point cloud classification: CC-pooling with input vertex features only</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="implementation-and-numerical-results.html"><a href="implementation-and-numerical-results.html#ablation-studies"><i class="fa fa-check"></i><b>9.5</b> Ablation studies</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>10</b> Related work</a>
<ul>
<li class="chapter" data-level="10.1" data-path="related-work.html"><a href="related-work.html#graph-based-models"><i class="fa fa-check"></i><b>10.1</b> Graph-based models</a></li>
<li class="chapter" data-level="10.2" data-path="related-work.html"><a href="related-work.html#higher-order-deep-learning-models"><i class="fa fa-check"></i><b>10.2</b> Higher-order deep learning models</a></li>
<li class="chapter" data-level="10.3" data-path="related-work.html"><a href="related-work.html#attention-based-models"><i class="fa fa-check"></i><b>10.3</b> Attention-based models</a></li>
<li class="chapter" data-level="10.4" data-path="related-work.html"><a href="related-work.html#graph-based-pooling"><i class="fa fa-check"></i><b>10.4</b> Graph-based pooling</a></li>
<li class="chapter" data-level="10.5" data-path="related-work.html"><a href="related-work.html#applied-algebraic-topology"><i class="fa fa-check"></i><b>10.5</b> Applied algebraic topology</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>11</b> Conclusions</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>A</b> 术语</a></li>
<li class="chapter" data-level="B" data-path="lifting-maps.html"><a href="lifting-maps.html"><i class="fa fa-check"></i><b>B</b> Lifting maps</a>
<ul>
<li class="chapter" data-level="B.1" data-path="lifting-maps.html"><a href="lifting-maps.html#n-hop-cc-of-a-graph"><i class="fa fa-check"></i><b>B.1</b> n-hop CC of a graph</a></li>
<li class="chapter" data-level="B.2" data-path="lifting-maps.html"><a href="lifting-maps.html#path-based-and-subgraph-based-cc-of-a-graph"><i class="fa fa-check"></i><b>B.2</b> Path-based and subgraph-based CC of a graph</a></li>
<li class="chapter" data-level="B.3" data-path="lifting-maps.html"><a href="lifting-maps.html#loop-based-cc-of-a-graph"><i class="fa fa-check"></i><b>B.3</b> Loop-based CC of a graph</a></li>
<li class="chapter" data-level="B.4" data-path="lifting-maps.html"><a href="lifting-maps.html#coface-cc-of-a-simplicial-complex-or-of-a-cc"><i class="fa fa-check"></i><b>B.4</b> Coface CC of a simplicial complex or of a CC</a></li>
<li class="chapter" data-level="B.5" data-path="lifting-maps.html"><a href="lifting-maps.html#augmentation-of-ccs-by-higher-rank-cells"><i class="fa fa-check"></i><b>B.5</b> Augmentation of CCs by higher-rank cells</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="ccnn-architecture-search-and-topological-quantum-field-theories.html"><a href="ccnn-architecture-search-and-topological-quantum-field-theories.html"><i class="fa fa-check"></i><b>C</b> CCNN architecture search and topological quantum field theories</a></li>
<li class="chapter" data-level="D" data-path="learning-discrete-exterior-calculus-operators-with-ccanns.html"><a href="learning-discrete-exterior-calculus-operators-with-ccanns.html"><i class="fa fa-check"></i><b>D</b> Learning discrete exterior calculus operators with CCANNs</a></li>
<li class="chapter" data-level="E" data-path="a-mapper-induced-topology-preserving-cc-pooling-operation.html"><a href="a-mapper-induced-topology-preserving-cc-pooling-operation.html"><i class="fa fa-check"></i><b>E</b> A mapper-induced topology-preserving CC-pooling operation</a></li>
<li class="chapter" data-level="" data-path="参考文献.html"><a href="参考文献.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">拓扑深度学习：超越图数据</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="message-passing" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">第 6 章</span> 消息传递<a href="message-passing.html#message-passing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>在本节中，我们将解释在第<a href="combinatorial-complex-neural-networks.html#push-forward-operator-and-merge-node">5.2</a>节中引入的聚合节点与高阶消息传递之间的关系。 特别是，我们证明了 CC 上的高阶消息传递可以通过第 <a href="combinatorial-complex-neural-networks.html#the-main-three-tensor-operations">5.3</a>节中介绍的基本张量运算来实现。 此外，我们还证明了 CCANNs（见章节<a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks">5.5</a>）与高阶消息传递之间的联系，并介绍了高阶消息传递的注意力版本。我们首先定义了CC上的高阶消息传递，泛化了<span class="citation">(<a href="#ref-hajijcell">Hajij, Istvan, and Zamzmi 2020</a>)</span>中引入的概念。</p>
<p>我们要指出的是，这里讨论的许多构造都是最基本的形式，但还可以进一步扩展。这个方向的一个重要方面是构建与特定群的作用相关的不变量或等变量的信息传递协议。</p>
<div id="definition-of-higher-order-message-passing" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> 高阶消息传递的定义<a href="message-passing.html#definition-of-higher-order-message-passing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>高阶消息传递指的是一种计算框架，它涉及使用一组邻域函数在高阶域中的实体和胞腔之间交换消息。 在定义@ref(def: homp-definition)中，我们形式化了CCs的高阶消息传递概念,图<a href="message-passing.html#fig:homp">6.1</a>说明了定义<a href="message-passing.html#def:homp-definition">6.1</a>。</p>
<div class="definition">
<p><span id="def:homp-definition" class="definition"><strong>定义 6.1  (CC上的高阶消息传递(Higher-order message passing)) </strong></span>令 <span class="math inline">\(\mathcal{X}\)</span>是CC， <span class="math inline">\(\mathcal{N}=\{ \mathcal{N}_1,\ldots,\mathcal{N}_n\}\)</span>是定义在 <span class="math inline">\(\mathcal{X}\)</span>上的邻域函数几何，<span class="math inline">\(x\)</span>是胞腔， 并且对于某个<span class="math inline">\(\mathcal{N}_k \in \mathcal{N}\)</span> 有<span class="math inline">\(y\in \mathcal{N}_k(x)\)</span>。 胞腔<span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span>之间的<em>message</em> <span class="math inline">\(m_{x,y}\)</span>依赖于两个胞腔或胞腔上支持的数据。用<span class="math inline">\(\mathcal{N}(x)\)</span>表示多集合<span class="math inline">\(\{\!\!\{ \mathcal{N}_1(x) , \ldots , \mathcal{N}_n (x) \}\!\!\}\)</span>，用<span class="math inline">\(\mathbf{h}_x^{(l)}\)</span>表示在层<span class="math inline">\(l\)</span>的胞腔<span class="math inline">\(x\)</span>上支持的某些数据。由<span class="math inline">\(\mathcal{N}\)</span>诱导的<span class="math inline">\(\mathcal{X}\)</span>上的高阶消息传递按如下四点规则来定义。</p>
<p><span class="math display" id="eq:homp3" id="eq:homp2" id="eq:homp1" id="eq:homp0">\[\begin{align}
m_{x,y} &amp;= \alpha_{\mathcal{N}_k}(\mathbf{h}_x^{(l)},\mathbf{h}_y^{(l)}), \tag{6.1} \\
m_{x}^k &amp;=  \bigoplus_{y \in \mathcal{N}_k(x)}  m_{x,y}, \; 1\leq k \leq n, \tag{6.2} \\
m_{x} &amp;=  \bigotimes_{ \mathcal{N}_k \in \mathcal{N} } m_x^k, \tag{6.3} \\
\mathbf{h}_x^{(l+1)} &amp;= \beta (\mathbf{h}_x^{(l)}, m_x).
\tag{6.4}
\end{align}\]</span>
其中, <span class="math inline">\(\bigoplus\)</span>是置换不变性聚合函数（permutation-invariant aggregation function），称作 <span class="math inline">\(x\)</span>的<em>内领域（intra-neighborhood）</em>； <span class="math inline">\(\bigotimes\)</span>是<span class="math inline">\(x\)</span>的<em>外邻域（inter-neighborhood）</em> ，称作聚合函；<span class="math inline">\(\alpha_{\mathcal{N}_k},\beta\)</span> 等是可微函数。</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:homp"></span>
<img src="figures/homp.png" alt="高阶信息传递示例. 左侧：选择一系列邻域函数 $\mathcal{N}_1,\ldots,\mathcal{N}_k$， 选择通常取决于学习任务。 右侧：对于每个 $\mathcal{N}_k$ ，使用内部邻域函数 $\bigoplus$ 聚合消息。 然后，用邻域间函数 $\bigotimes$ 汇集从所有邻域获得的最终信息。"  />
<p class="caption">
图 6.1: 高阶信息传递示例. 左侧：选择一系列邻域函数 <span class="math inline">\(\mathcal{N}_1,\ldots,\mathcal{N}_k\)</span>， 选择通常取决于学习任务。 右侧：对于每个 <span class="math inline">\(\mathcal{N}_k\)</span> ，使用内部邻域函数 <span class="math inline">\(\bigoplus\)</span> 聚合消息。 然后，用邻域间函数 <span class="math inline">\(\bigotimes\)</span> 汇集从所有邻域获得的最终信息。
</p>
</div>
<p>关于定义 <a href="message-passing.html#def:homp-definition">6.1</a>的一些讨论如下。首先，方程 <a href="message-passing.html#eq:homp0">(6.1)</a>中的信息<span class="math inline">\(m_{x,y}\)</span>并不仅仅取决于胞腔 <span class="math inline">\(x、y\)</span> 上支持的数据<span class="math inline">\(\mathbf{h}_x^{(l)}\)</span>、<span class="math inline">\(\mathbf{h}_y^{(l)}\)</span>，还取决于胞腔本身。例如，如果<span class="math inline">\(\mathcal{X}\)</span>是一个胞腔复形，那么在计算信息<span class="math inline">\(m_{x,y}\)</span>时，<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>的<em>方向</em>都会被考虑在内。 另外，如果<span class="math inline">\(x\cup y\)</span> 或 <span class="math inline">\(x\cap y\)</span> 是 <span class="math inline">\(\mathcal{X}\)</span> 中的胞腔，那么在计算信息<span class="math inline">\(m_{x,y}\)</span>时包含它们的数据也可能是有用的。这种独特的特性只体现在高阶域中，而不会出现在基于图的消息传递框架中<span class="citation">(<a href="#ref-gilmer2017neural">Gilmer et al. 2017</a>; <a href="#ref-bronstein2021geometric">Bronstein et al. 2021</a>)</span><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>。其次，高阶消息传递依赖于邻域函数集<span class="math inline">\(\mathcal{N}\)</span>的选择。这也是只有在高阶领域中才会出现的独特特征，在高阶领域中，邻域函数必然是由一组邻域关系来描述的，而不是像基于图的消息传递那样由图邻接关系来描述。 第三，在公式 <a href="message-passing.html#eq:homp0">(6.1)</a>中，由于<span class="math inline">\(y\)</span>是根据邻域关系<span class="math inline">\(\mathcal{N}_k \in\mathcal{N}\)</span>来隐含定义的，所以函数<span class="math inline">\(\alpha_{\mathcal{N}_k}\)</span>和消息<span class="math inline">\(m_{x,y}\)</span>都取决于<span class="math inline">\(\mathcal{N}_k\)</span>。第四，邻域间 <span class="math inline">\(\bigotimes\)</span> 并不一定非得是一个置换不变的聚合函数。 例如，可以在多重集合 <span class="math inline">\(\mathcal{N}(x)\)</span> 上设置一个阶（order），并根据这个阶计算<span class="math inline">\(m_x\)</span>。最后，高阶消息传递依赖于两个聚合函数，即邻内聚合函数和邻间聚合函数，而基于图的消息传递依赖于一个聚合函数。正如第<a href="combinatorial-complexes.html#combinatorial-complexes">4</a>章节中所说明的，集合 <span class="math inline">\(\mathcal{N}\)</span>的选择，使得可以在高阶消息传递中使用各种邻域函数。</p>
<div class="remark">
<p><span id="unlabeled-div-6" class="remark"><em>备注</em>. </span>定义 <a href="combinatorial-complex-neural-networks.html#def:pushing-exact-definition">5.3</a>中给出的前推算子与公式 <a href="message-passing.html#eq:homp0">(6.1)</a>的更新规则有关。 一方面，公式<a href="message-passing.html#eq:homp0">(6.1)</a>需要两个共链<span class="math inline">\(\mathbf{X}_i= [\mathbf{h}_{x^i_1}^{(l)},\ldots,\mathbf{h}_{x^i_{|\mathcal{X}^i|}}^{(l)}]\)</span> 和 <span class="math inline">\(\mathbf{Y}_{j}^{(l)}=[\mathbf{h}_{y^{j}_1}^{(l)},\ldots,\mathbf{h}_{y^{j}_{|\mathcal{X}^{j}|}}^{(l)}]\)</span> 来计算 <span class="math inline">\(\mathbf{X}^{(l+1)}_i = [\mathbf{h}_{x^i_1}^{(l+1)},\ldots,\mathbf{h}_{x^i_{|\mathcal{X}^i|}}^{(l+1)}]\)</span>，因此<span class="math inline">\(\mathcal{C}^j\)</span>
和<span class="math inline">\(\mathcal{C}^i\)</span>上的信号必须存在，才能计算公式<a href="message-passing.html#eq:homp0">(6.1)</a>。 从这个角度来看，把这个操作看作更新规则是很自然和习惯的。另一方面，定义 <a href="combinatorial-complex-neural-networks.html#def:pushing-exact-definition">5.3</a>中的前推算子计算的是在给定共链 <span class="math inline">\(\mathbf{H}_i\in \mathcal{C}^i\)</span>时，<span class="math inline">\(\mathcal{C}^j\)</span>中的共链<span class="math inline">\(\mathbf{K}_{j}\)</span>。 由于只需要 <span class="math inline">\(\mathbf{H}_i\)</span> 一条共链就可以完成这个计算，所以我们可以很自然地把方程 <a href="combinatorial-complex-neural-networks.html#eq:functional">(5.2)</a>看作一个函数。更多详情，请参见第 <a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison">6.3</a>节。</p>
</div>
<p>定义<a href="message-passing.html#def:homp-definition">6.1</a>给出的高阶消息传递框架可用于构造CC上的新的神经网络架构，正如在图<a href="combinatorial-complex-neural-networks.html#fig:tdl">5.2</a>中隐含反应的那样。首先，对于给定的的CC <span class="math inline">\(\mathcal{X}\)</span>，给出<span class="math inline">\(\mathcal{X}\)</span>上支持的共链 <span class="math inline">\(\mathbf{H}_{i_1}\ldots, \mathbf{H}_{i_m}\)</span>。 其次，根据期望的学习任务选择合适的邻域函数集合。然后,用选定的邻域函数在输入共链<span class="math inline">\(\mathbf{H}_{i_1}\ldots, \mathbf{H}_{i_m}\)</span>上执行定义<a href="message-passing.html#def:homp-definition">6.1</a>中的更新规则。 重复第2、3步，直到获得最终结果。</p>
<div class="definition">
<p><span id="def:hmpsnn" class="definition"><strong>定义 6.2  (高阶消息传递神经网络，Higher-order message-passing neural network) </strong></span>凡是基于定义<a href="message-passing.html#def:homp-definition">6.1</a>构造的任何神经网络都可称作<em>高阶消息传递神经网络</em>.</p>
</div>
</div>
<div id="higher-order-message-passing-neural-networks-are-ccnns" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> 高阶消息传递神经网络就是CCNNs<a href="message-passing.html#higher-order-message-passing-neural-networks-are-ccnns" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>在本节，将表明高阶消息传递计算可以用聚合节点计算来实现，因此，高阶消息传递神经网络就可认为是CCNNs。结果，高阶消息传递就可以通过一致性更新规则集合来统一单纯复形、胞腔复形、超图上的消息传递，作为可选项，也可以通过张量图表示语言来实现统一。</p>
<div class="theorem">
<p><span id="thm:mn" class="theorem"><strong>定理 6.1  (聚合节点计算，Merge node computation) </strong></span>定义<a href="message-passing.html#def:homp-definition">6.1</a>中的高阶消息传递计算可以用聚合节点计算来实现。</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>证明</em>. </span>令 <span class="math inline">\(\mathcal{X}\)</span>是CC，<span class="math inline">\(\mathcal{N}=\{ \mathcal{N}_1,\ldots,\mathcal{N}_n\}\)</span>是定义<a href="message-passing.html#def:homp-definition">6.1</a>中规定的一组邻域函数，<span class="math inline">\(G_k\)</span>是邻域函数<span class="math inline">\(\mathcal{N}_k\)</span>诱导的矩阵。假定定义<a href="message-passing.html#def:homp-definition">6.1</a>指定的胞腔 <span class="math inline">\(x\)</span>是 <span class="math inline">\(j\)</span>-cell，<span class="math inline">\(y \in \mathcal{N}_k(x)\)</span>的邻居是 <span class="math inline">\(i_k\)</span>-cells。 这里将指出公式 <a href="message-passing.html#eq:homp0">(6.1)</a>–<a href="message-passing.html#eq:homp3">(6.4)</a>可以看作是聚合节点的应用实现。接下来，我们将定义的邻域函数是<span class="math inline">\(\mathcal{N}_{Id}(x)=\{x\}\)</span> for <span class="math inline">\(x\in \mathcal{X}\)</span>。 此外，我们将用<span class="math inline">\(Id\colon\mathcal{C}^j\to \mathcal{C}^j\)</span>来标记与<span class="math inline">\(\mathcal{N}_{Id}\)</span>关联的的邻居矩阵，因为它就是一个单位阵（identity matrix）。</p>
<blockquote>
<p>译者注：最后一句不知道翻的对不对，感觉句子不通顺，意思不明确。</p>
</blockquote>
<p>计算公式<a href="message-passing.html#eq:homp0">(6.1)</a>中的消息<span class="math inline">\(m_{x,y}\)</span>涉及到两个共链:
<span class="math display">\[\begin{equation*}
        \mathbf{X}_j^{(l)}=
        [\mathbf{h}_{x^j_1}^{(l)},\ldots,\mathbf{h}_{x^j_{|\mathcal{X}^j|}}^{(l)}],~
        \mathbf{Y}_{i_k}^{(l)}=
        [\mathbf{h}_{y^{i_k}_1}^{(l)},\ldots,\mathbf{h}_{y^{i_k}_{|\mathcal{X}^{i_k}|}}^{(l)}].
    \end{equation*}\]</span>
每个消息<span class="math inline">\(m_{x^{^j}_t, y^{i_k}_s }\)</span>都与矩阵<span class="math inline">\(G_k\)</span>中的一个项<span class="math inline">\([G_k]_{st}\)</span>相对应。换句话说，矩阵<span class="math inline">\(G_k\)</span>的每个非零项和消息<span class="math inline">\(m_{x^{^j}_t, y^{i_k}_s }\)</span>都是一一对应的。</p>
<p>从第<a href="combinatorial-complex-neural-networks.html#push-forward-operator-and-merge-node">5.2</a>节可以看出，计算 <span class="math inline">\(\{m_x^k\}_{k=1}^n\)</span> 相当于聚合节点 <span class="math inline">\(\mathcal{M}_{Id_j、 G_k}\colon \mathcal{C}^j\times \mathcal{C}^{i_k}\to \mathcal{C}^j\)</span> 执行通过 <span class="math inline">\(\alpha_k\)</span> 和 <span class="math inline">\(\bigoplus\)</span> 确定的计算，并且得出
<span class="math display">\[\begin{equation*}
        \mathbf{m}_j^k=[m_{x^j_1}^k,\ldots,m_{x^j_{|\mathcal{X}^j|}}^k]=
        \mathcal{M}_{Id_j,G_k}(\mathbf{X}_j^{(l)},\mathbf{Y}_{i_k}^{(l)}) \in \mathcal{C}^{j}.
    \end{equation*}\]</span>
在这个阶段， 有 <span class="math inline">\(n\)</span> <span class="math inline">\(j\)</span>-cochains <span class="math inline">\(\{\mathbf{m}_j^k\}_{k=1}^n\)</span>。公式<a href="message-passing.html#eq:homp2">(6.3)</a> 和 <a href="message-passing.html#eq:homp3">(6.4)</a> ，将这些共链与输入的<span class="math inline">\(j\)</span>-cochain <span class="math inline">\(\mathbf{X}_j^{(l)}\)</span>聚合。 尤其，计算公式 <a href="message-passing.html#eq:homp2">(6.3)</a>中的 <span class="math inline">\(m_x\)</span> 相当于在共链 <span class="math inline">\(\{\mathbf{m}_j^k\}_{k=1}^n\)</span>上应用<span class="math inline">\(n-1\)</span>次
$_{Id_k,Id_k}^j <span class="math inline">\(的聚合节点操作。 很明显，首先通过聚合\)</span>_j^1$和 <span class="math inline">\(\mathbf{m}_j^2\)</span> 来获得 <span class="math inline">\(\mathbf{n}_j^1=\mathcal{M}_{Id_j,Id_j}(\mathbf{m}_j^1,\mathbf{m}_j^2)\)</span>。然后，聚合<span class="math inline">\(j\)</span>-cochain <span class="math inline">\(\mathbf{n}_j^1\)</span> 和 <span class="math inline">\(j\)</span>-cochain <span class="math inline">\(\mathbf{m}_j^3\)</span>，等等。最后的聚合节点是执行聚合操作 <span class="math inline">\(\mathbf{n}_j^{n-1}=\mathcal{M}_{Id_j,Id_j}(\mathbf{n}_j^{n-2},\mathbf{m}_j^n)\)</span>， 也即 <span class="math inline">\(\mathbf{m}_j = [ m_{x_1^j},\ldots, m_{x_{|\mathcal{X}^j|}^j }]\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.
最后，计算由聚合节点<span class="math inline">\(\mathcal{M}_{(Id_j,Id_j)}(\mathbf{m}_j, \mathbf{X}_j^{(l)})\)</span>（该计算由公式<a href="message-passing.html#eq:homp3">(6.4)</a>中的函数<span class="math inline">\(\beta\)</span>确定）实现的 <span class="math inline">\(\mathbf{X}_j^{(l+1)}\)</span> 。</p>
</div>
<p>定理 <a href="message-passing.html#thm:mn">6.1</a>表明CCs上定义的高阶消息传递网络可以从基本张量操作来构造， 因此它们是 CCNN 的特例。我们在定理<a href="message-passing.html#thm:thm-unifying">6.2</a>中正式阐述了这一结果。</p>
<div class="theorem">
<p><span id="thm:thm-unifying" class="theorem"><strong>定理 6.2  (高阶消息传递和CCNs,Higher-order message passing and CCNNs) </strong></span>高阶信息传递神经网络就是 CCNN。</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>证明</em>. </span>从定义 <a href="message-passing.html#def:hmpsnn">6.2</a>和定理 <a href="message-passing.html#thm:mn">6.1</a>可以立即得出该结论。</p>
</div>
<p>由定理 <a href="message-passing.html#thm:thm-unifying">6.2</a>可知，定义在通用性不如 CCs 的高阶域（如单纯复形、胞腔复形和超图）上的高阶消息传递神经网络也是 CCNNs 的特例。因此，定义<a href="combinatorial-complex-neural-networks.html#def:tdd">5.2</a>中引入的张量图形成了一种通用的图解方法，用于表达定义在常用高阶域上的神经网络。</p>
<div class="theorem">
<p><span id="thm:unifying1" class="theorem"><strong>定理 6.3  (消息传递神经网络和张量图，Message-passing neural networks and tensor diagrams) </strong></span>Message-passing neural networks defined on simplicial complexes, cell complexes or hypergraphs can be expressed in terms of tensor diagrams and their computations can be realized in terms of the three elementary tensor operators.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>证明</em>. </span>从两点可得出这一结论，即：<a href="mailto:定理@ref" class="email">定理@ref</a>(thm:thm-unifying)，以及单纯复形、胞腔复形和超图可看作是 CCs 的特例实现。</p>
</div>
<p>定理 <a href="message-passing.html#thm:thm-unifying">6.2</a> 和 <a href="message-passing.html#thm:unifying1">6.3</a> 基于张量图提出了一个统一的TDL框架，从而为今后的发展提供了空间。 例如, 文献<span class="citation">(<a href="#ref-mathilde2023">Papillon et al. 2023</a>)</span> 已经利基于该框架，用张量图来表达了单纯复形、胞腔复形和超图等现有的 TDL 架构。</p>
</div>
<div id="merge-nodes-and-higher-order-message-passing-a-qualitative-comparison" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> 聚合节点和高阶消息传递：量化比较<a href="message-passing.html#merge-nodes-and-higher-order-message-passing-a-qualitative-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>定义 <a href="message-passing.html#def:homp-definition">6.1</a>中给出的高阶消息传递提供了一种更新规则，可以使用由 <span class="math inline">\(\mathcal{N}(x)\)</span> 确定的一组邻域向量 <span class="math inline">\(\mathbf{h}_y^{l}\)</span> 从向量 <span class="math inline">\(\mathbf{h}_x^{l}\)</span> 获得向量 <span class="math inline">\(\mathbf{h}_x^{l+1}\)</span>。显然，这个计算框架假定向量 <span class="math inline">\(\mathbf{h}_x^{(l)}\)</span> 和 <span class="math inline">\(\mathbf{h}_{y}^{(l)}\)</span> 是作为输入提供的。 换句话说，根据定义 <a href="message-passing.html#def:homp-definition">6.1</a>需要目标域中的 共链<span class="math inline">\(\mathbf{X}_j^{(l)} \in \mathcal{C}^{j}\)</span>，以及共链 <span class="math inline">\(\mathbf{Y}_{i_k}^{(l)} \in \mathcal{C}^{i_k}\)</span>，以便计算更新后的 <span class="math inline">\(j-\)</span>cochain <span class="math inline">\(\mathbf{X}_j^{(l+1)}\)</span> 。 另一方面，进行聚合节点计算需要一个共链向量 <span class="math inline">\((\mathbf{H}_{i_1},\mathbf{H}_{i_2})\)</span>，这可以从公式 <a href="combinatorial-complex-neural-networks.html#eq:sum">(5.1)</a>和定义 <a href="combinatorial-complex-neural-networks.html#def:exact-definition-merge-node">5.4</a>中看出。</p>
<p>这两种计算框架之间的差异可能看起来是符号性的，并且消息传递的视角可能看起来更直观，尤其是在处理基于图的模型时。然而，我们认为，在存在自定义高阶网络架构的情况下，聚合节点框架在计算上更自然、更灵活。为了说明这一点，我们考虑一下图 <a href="message-passing.html#fig:merge-example">6.2</a>中可视化的示例。</p>
<p>在图<a href="message-passing.html#fig:merge-example">6.2</a>中显示的神经网络有一个共链输入向量 <span class="math inline">\((\mathbf{H}_0,\mathbf{H}_2) \in  \mathcal{C}^0 \times \mathcal{C}^2\)</span>，第一层的神经网络负责计算共链<span class="math inline">\(\mathbf{H}_1 \in \mathcal{C}^1\)</span>，第二层的神经网络负责计算共链 <span class="math inline">\(\mathbf{H}_3\in \mathcal{C}^3\)</span>。为了在第一层获得共链<span class="math inline">\(\mathbf{H}_1\)</span>，需要考察由<span class="math inline">\(B_{0,1}^T\)</span> 和 <span class="math inline">\(B_{1,2}\)</span>诱导的邻域函数。 然而，如果使用公式 <a href="message-passing.html#eq:homp0">(6.1)</a> 和 <a href="message-passing.html#eq:homp1">(6.2)</a>执行在图<a href="message-passing.html#fig:merge-example">6.2</a>中张量图第一层确定的计算时，那么应该注意到在<span class="math inline">\(\mathcal{C}^1\)</span>上并没有提供共链来作为输入的一部分。因此，当使用公式<a href="message-passing.html#eq:homp0">(6.1)</a>
和<a href="message-passing.html#eq:homp1">(6.2)</a>时，需要特殊处理，因为向量 <span class="math inline">\(\mathbf{h}_{x^1_j}\)</span> 还没有计算出来。 请注意，GNN 中不存在这种人工制品，因为它们经常更新节点特征，而节点特征通常是作为输入的一部分提供的。具体来说，在 GNN 中，公式 <a href="message-passing.html#eq:homp0">(6.1)</a>更新规则中的前两个参数是在底层图的 0-cells上支持的共链。</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:merge-example"></span>
<img src="figures/merge_example_scaled.png" alt="图中描述的神经网络可以实现为两个聚合节点的组合。更具体的说，输入是共链向量 $(\mathbf{H}_0,\mathbf{H}_2)$，第一个聚合节点负责计算1-chain $\mathbf{H}_1 = \mathcal{M}_{B_{0,1}^T,B_{1,2}} (\mathbf{H}_0,\mathbf{H}_2)$；第二个聚合节点 $\mathcal{M}_{B_{1,3}^T, B_{2,3}} \colon\mathcal{C}^1 \times \mathcal{C}^2 \to \mathcal{C}^3$负责计算3-cochain $\mathbf{H}_3=\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\mathbf{H}_1,\mathbf{H}_2)$。从聚合节点的角度来看，可以计算出 1-cells 胞腔和 3-cells 胞腔支持的共链，而无需这些胞腔的初始共链。另一方面，高阶消息传递框架有两个主要限制：一是假定输入域所有维度所有胞腔上都有支持的初始共链，二是假定每次迭代都能更新输入域所有维度所有胞腔上所支持的所有共链。"  />
<p class="caption">
图 6.2: 图中描述的神经网络可以实现为两个聚合节点的组合。更具体的说，输入是共链向量 <span class="math inline">\((\mathbf{H}_0,\mathbf{H}_2)\)</span>，第一个聚合节点负责计算1-chain <span class="math inline">\(\mathbf{H}_1 = \mathcal{M}_{B_{0,1}^T,B_{1,2}} (\mathbf{H}_0,\mathbf{H}_2)\)</span>；第二个聚合节点 <span class="math inline">\(\mathcal{M}_{B_{1,3}^T, B_{2,3}} \colon\mathcal{C}^1 \times \mathcal{C}^2 \to \mathcal{C}^3\)</span>负责计算3-cochain <span class="math inline">\(\mathbf{H}_3=\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\mathbf{H}_1,\mathbf{H}_2)\)</span>。从聚合节点的角度来看，可以计算出 1-cells 胞腔和 3-cells 胞腔支持的共链，而无需这些胞腔的初始共链。另一方面，高阶消息传递框架有两个主要限制：一是假定输入域所有维度所有胞腔上都有支持的初始共链，二是假定每次迭代都能更新输入域所有维度所有胞腔上所支持的所有共链。
</p>
</div>
<p>类似的，为了计算图<a href="message-passing.html#fig:merge-example">6.2</a>中第二层的共链<span class="math inline">\(\mathbf{H}_3 \in  \mathcal{C}^3\)</span>，就必须考虑由<span class="math inline">\(B_{1,3}^T\)</span> and <span class="math inline">\(B_{2,3}\)</span>诱导的邻域函数，而且必须使用共链向量<span class="math inline">\((\mathbf{H}_1,\mathbf{H}_2)\)</span>。
这意味着图<a href="message-passing.html#fig:merge-example">6.2</a>中给出的神经网络计算得出的 共链<span class="math inline">\(\mathbf{H}_1\)</span> 和 <span class="math inline">\(\mathbf{H}_3\)</span> 不能通过迭代过程得到。 此外，输入向量 <span class="math inline">\(\mathbf{H}_0\)</span> 和<span class="math inline">\(\mathbf{H}_2\)</span> 在迭代过程的任何一步都不会被更新，结果就导致共链 <span class="math inline">\(\mathbf{H}_1\)</span> 和 <span class="math inline">\(\mathbf{H}_3\)</span> 也从不会被更新。从更新规则（如高阶消息传递框架中出现的更新规则）的角度来看（定义<a href="message-passing.html#def:homp-definition">6.1</a>），这种设置是不自然的，因为它假定所有维度的<em>所有胞腔上的初始共链</em>都可作为输入，并在每次迭代时都要对输入域的的复形上<em>所有胞腔所支持的所有共链进行更新</em>。</p>
<p>事实上，如果在使用高阶消息传递框架时遇到这样的困难，可以使用一些临时的解决办法，比如通过开启或关闭某些共链的迭代，或通过引入辅助共链来解决，聚合节点就是为了克服这些限制而设计的。 尤其, 从聚合节点的角度来看，我们可以把图<a href="message-passing.html#fig:merge-example">6.2</a>的第一层看作是函数<span class="math inline">\(\mathcal{M}_{B_{0,1}^T,B_{1,2}}\colon \mathcal{C}^0 \times \mathcal{C}^1 \to \mathcal{C}^1\)</span>，参见公式<a href="combinatorial-complex-neural-networks.html#eq:functional">(5.2)</a>。函数 <span class="math inline">\(\mathcal{M}_{B_{0,1}^T,B_{1,2}}\)</span> 把共链向量 <span class="math inline">\((\mathbf{H}_0,\mathbf{H}_2)\)</span>作为输入，然后来计算1-chain <span class="math inline">\(\mathbf{H}_1 = \mathcal{M}_{B_{0,1}^T,B_{1,2}} (\mathbf{H}_0,\mathbf{H}_2)\)</span>。 类似的，可以用聚合节点<span class="math inline">\(\mathcal{M}_{B_{1,3}^T, B_{2,3}} \colon
\mathcal{C}^1 \times \mathcal{C}^2 \to \mathcal{C}^3\)</span>来计算3-cochain <span class="math inline">\(\mathbf{H}_3=\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\mathbf{H}_1,\mathbf{H}_2)\)</span>。</p>
</div>
<div id="attention-higher-order-message-passing-and-ccanns" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> 注意力高阶消息传递和CCANNs<a href="message-passing.html#attention-higher-order-message-passing-and-ccanns" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>在这里，我们展示了高阶消息传递（定义 <a href="message-passing.html#def:homp-definition">6.1</a>）与 CCANNs（章节<a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks">5.5</a>）之间的联系。我们将首先介绍一个注意力版本的定义 <a href="message-passing.html#def:homp-definition">6.1</a>。</p>
<div class="definition">
<p><span id="def:attention-homp" class="definition"><strong>定义 6.3  (CC上的注意力高阶消息传递(Attention higher-order message passing on a CC)) </strong></span>令 <span class="math inline">\(\mathcal{X}\)</span>是CC，<span class="math inline">\(\mathcal{N}=\{ \mathcal{N}_1,\ldots,\mathcal{N}_n\}\)</span>是定义在<span class="math inline">\(\mathcal{X}\)</span>上的邻域函数<span class="math inline">\(\mathcal{X}\)</span>，<span class="math inline">\(x\)</span>是胞腔，并且对于某个<span class="math inline">\(\mathcal{N}_k \in \mathcal{N}\)</span>，存在<span class="math inline">\(y\in \mathcal{N}_k(x)\)</span>。所谓胞腔<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>之间的<em>消息</em><span class="math inline">\(m_{x,y}\)</span>就是要完成某种计算，它依赖于两个胞腔或他们所支持的数据。用<span class="math inline">\(\mathcal{N}(x)\)</span>表示多集 <span class="math inline">\(\{\!\!\{ \mathcal{N}_1(x) , \ldots ,  \mathcal{N}_n (x) \}\!\!\}\)</span>，<span class="math inline">\(\mathbf{h}_x^{(l)}\)</span>表示层<span class="math inline">\(l\)</span>的胞腔<span class="math inline">\(x\)</span>支持的数据。<span class="math inline">\(\mathcal{N}\)</span>诱导的<span class="math inline">\(\mathcal{X}\)</span>上的<em>注意力高阶消息传递</em>可沟通过如下四条规则来定义：
<span class="math display" id="eq:ahomp3" id="eq:ahomp2" id="eq:ahomp1" id="eq:ahomp0">\[\begin{align}
m_{x,y} &amp;= \alpha_{\mathcal{N}_k}(\mathbf{h}_x^{(l)},\mathbf{h}_y^{(l)}), \tag{6.5} \\
m_{x}^k &amp;=  \bigoplus_{y \in \mathcal{N}_k(x)} a^k(x,y)  m_{x,y}, \; 1\leq k \leq n , \tag{6.6} \\
m_{x} &amp;=  \bigotimes_{ \mathcal{N}_k \in \mathcal{N} } b^k m_x^k , \tag{6.7} \\
\mathbf{h}_x^{(l+1)} &amp;= \beta (\mathbf{h}_x^{(l)}, m_x) . \tag{6.8}
\end{align}\]</span>
其中, <span class="math inline">\(a^k \colon \{x\} \times \mathcal{N}_k(x)\to [0,1]\)</span> 是高阶注意力函数(参见定义<a href="combinatorial-complex-neural-networks.html#def:hoa">5.9</a>),
<span class="math inline">\(b^k\)</span> 是满足<span class="math inline">\(\sum_{k=1}^n b^k=1\)</span>的可训练的注意力权重,
<span class="math inline">\(\bigoplus\)</span> 是置换不变性聚合函数，<span class="math inline">\(\bigotimes\)</span> 是一般的聚合函数， <span class="math inline">\(\alpha_{\mathcal{N}_k}\)</span> 和 <span class="math inline">\(\beta\)</span> 是可微函数。</p>
</div>
<p>定义<a href="message-passing.html#def:attention-homp">6.3</a> 区别了两类，第一种由函数 <span class="math inline">\(a^k\)</span>确定，公式<a href="message-passing.html#eq:ahomp1">(6.6)</a>的权重 <span class="math inline">\(a^k(x,y)\)</span>依赖于邻域函数 <span class="math inline">\(\mathcal{N}_k\)</span>和胞腔 <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>。更进一步，<span class="math inline">\(a^k(x,y)\)</span>确定了 确定了由邻域函数 <span class="math inline">\(\mathcal{N}_k\)</span> 决定的胞腔 <span class="math inline">\(x\)</span> 对其周围邻域 <span class="math inline">\(y\in\mathcal{N}_k\)</span> 的关注度。在章节<a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks">5.5</a>提到的CC-attention前推操作由这些权重来特定的参数化实现。另一方面，公式<a href="message-passing.html#eq:ahomp2">(6.7)</a>的权重 <span class="math inline">\(b^k\)</span> 仅是邻域<span class="math inline">\(\mathcal{N}_k\)</span>的函数，
并由此决定了胞腔 <span class="math inline">\(x\)</span> 对从每个邻域函数 <span class="math inline">\(\mathcal{N}_k\)</span> 中获得的消息的关注程度。在章节<a href="combinatorial-complex-neural-networks.html#combinatorial-complex-attention-neural-networks">5.5</a>给出的CC-attention前推操作中，我们设置<span class="math inline">\(b^k\)</span>等于1。然而，聚合节点的概念（参见定义 <a href="combinatorial-complex-neural-networks.html#def:exact-definition-merge-node">5.4</a>）可以很容易地扩展为到引入相应的<em>注意力聚合节点</em>的概念，这反过来又可以用来在实践中实现公式 <a href="message-passing.html#eq:ahomp2">(6.7)</a>。 请注意，由权重 <span class="math inline">\(b^k\)</span> 决定的注意力是高阶领域所独有的，在基于图的注意力模型中不会出现。</p>

</div>
</div>
<h3>参考文献<a href="参考文献.html#参考文献" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bronstein2021geometric" class="csl-entry">
Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. 2021. <span>“Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.”</span> <em>arXiv Preprint arXiv:2104.13478</em>.
</div>
<div id="ref-gilmer2017neural" class="csl-entry">
Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. <span>“Neural Message Passing for Quantum Chemistry.”</span> In <em>International Conference on Machine Learning</em>.
</div>
<div id="ref-hajijcell" class="csl-entry">
Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. <span>“Cell Complex Neural Networks.”</span> In <em>NeurIPS 2020 Workshop TDA and Beyond</em>.
</div>
<div id="ref-mathilde2023" class="csl-entry">
Papillon, Mathilde, Sophia Sanborn, Mustafa Hajij, and Nina Miolane. 2023. <span>“Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.”</span> <em>arXiv Preprint arXiv:2304.10031</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p><span class="math inline">\(m_{x,y}\)</span>中的消息 “方向”是从<span class="math inline">\(y\)</span>到<span class="math inline">\(x\)</span>。一般来说，<span class="math inline">\(m_{x,y}\)</span> 和 <span class="math inline">\(m_{y,x}\)</span>是不相等的<a href="message-passing.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>请注意，虽然我们对所有聚合节点都使用了相同的符号 <span class="math inline">\(\mathcal{M}_{Id_k,Id_k}\)</span>，但这些节点一般都有不同的参数。<a href="message-passing.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="combinatorial-complex-neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="push-forward-pooling-and-unpooling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pyt-team/tdlbook/edit/main/rmd/06-message-passing.rmd",
"text": "编辑"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
