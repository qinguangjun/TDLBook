% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
]{krantz}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={拓扑深度学习：超越图数据},
  pdfauthor={Mustafa Hajij, Theodore Papamarkou, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Michael T. Schaub},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{拓扑深度学习：超越图数据}
\author{Mustafa Hajij, Theodore Papamarkou, Ghada Zamzmi, Karthikeyan
Natesan Ramamurthy, Tolga Birdal, Michael T. Schaub}
\date{2024-08-31}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables
\section*{贡献者}\label{ux8d21ux732eux8005}
\addcontentsline{toc}{section}{贡献者}

除了共同作者之外，以下贡献者对本书的撰写做出了很多贡献: \emph{Nina
Miolane, Aldo Guzmán-Sáenz, Tamal K. Dey, Soham Mukherjee, Shreyas N.
Samaga, Neal Livesay, Robin Walters, Paul Rosen}.

\section*{译者}\label{ux8bd1ux8005}
\addcontentsline{toc}{section}{译者}

\textbf{秦广军}

北京联合大学 智慧城市学院

2024年08月06日开始，xxxx年xx月xx日结束

北京

\begin{quote}
几何深度学习，这真的是一个很酷的想法，作为其一个分支，拓扑深度学习更是直接将代数拓扑应用到了神经网络中。读过作者的相关文章，本书是作者写的第一本关于拓扑深度学习的书，于是决定把本书翻译了。说它是书，其实更像个小册子，或者蓝皮书，是学习几何深度学习和拓扑深度学习的重要入门材料。
\end{quote}

\begin{quote}
对于深度学习，我不是太了解的那么深入，只是在科研上逐渐开始使用，对于代数拓扑这种十分抽象的专业知识更是初次接触，不确定相关术语和表述翻译是否准确，请参考原文阅读（\href{https://tdlbook.org/}{TDLBook}）。
\end{quote}

\begin{figure}

{\centering \includegraphics{figures/face} 

}

\caption{来自这个网站}\label{fig:thissite}
\end{figure}

\begin{quote}
\textbf{问题1.} 生成html文件正常，但是生成epub文件里的公式不能正常显示
\end{quote}

\begin{quote}
\textbf{问题2.}
如果生成的HTML中公式显示不正常，重启电脑就好了，不指导为什么
\end{quote}

\begin{quote}
\textbf{问题3.} 标签``Proposition
''如何替换为中文，应该在\_bookdown.yml中设置，不知道怎么设置
\end{quote}

\section*{序言}\label{ux5e8fux8a00}
\addcontentsline{toc}{section}{序言}

拓扑深度学习是一个快速发展的领域，用于为拓扑域(如单纯复形、胞腔复形和超图等多种科学计算中常用的数据域)上的数据研发深度学习模型。在本文中，我们提出了一个统一的深度学习框架，该框架涵盖丰富的数据结构，其中包括广泛采用的拓扑域。

具体来说，我们首先介绍\emph{组合复形(combinatorial
complexes)}--一种新型拓扑域。组合复形可以被看作保持某些理想性质的图的一般化，与超图类似，组合复形对关系集不施加任何约束。此外，组合复形可以用于构建分层的高阶关系，类似于单纯复形和胞腔复形中的关系。因此，组合复形涵盖并结合了超图和胞腔复形的有用特征（他们是两种有前途的抽象概念），这有助于将图神经网络推广到拓扑空间。

其次，在组合复形及其丰富的组合和代数结构基础上，我们构造了一类通用的消息传递组合复形神经网络\emph{（combinatorial
complex neural networks,CCNNs）}，主要侧重于基于注意力的
CCNNs。我们描述了 CCNNs 的置换和方向等变关系，并详细讨论了 CCNNs
中的池化和反池化操作。

最后，我们评估了 CCNN
在网格形状分析和图形学习任务中的表现。实验表明，与专为相同任务定制的最先进的深度学习模型相比，CCNN
的性能更具竞争力。

\subsection*{编译}\label{ux7f16ux8bd1}
\addcontentsline{toc}{subsection}{编译}

本书用\emph{knitr包}(\citeproc{ref-xie2015}{Xie
2015})和\emph{bookdown包}(\citeproc{ref-xie2024}{Xie
2024})编辑，书中源代码见\href{https://github.com/pyt-team/tdlbook}{TDLBook的github仓库}

\subsection*{致谢}\label{ux81f4ux8c22}
\addcontentsline{toc}{subsection}{致谢}

M. H. 感谢美国国家科学基金会 DMS-2134231 奖项的支持。 G.
Z.目前隶属于美国国立卫生研究院（NIH），但本研究的核心内容是在南佛罗里达大学（USF）工作期间完成的。本文仅代表作者个人观点，不代表美国国立卫生研究院或美国政府的观点。
N. M. 感谢美国国家科学基金会（National Science Foundation）颁发的
DMS-2134241 奖项。 T. B. 感谢工程与物理科学研究委员会 {[}grant
EP/X011364/1{]} 的支持。 T. K. D. 感谢美国国家科学基金会 CCF 2049010
奖项的资助。 N. L. 感谢 Roux 研究所和 Harold Alfond 基金会的支持。 R. W.
感谢美国国家科学基金会 DMS-2134178 奖项的资助。 P. R.
感谢美国国家科学基金会 IIS-2316496 奖项的资助。 M. T. S.
感谢德国北莱茵-威斯特法伦州文化与科学部（MKW）（NRW
Rückkehrprogramm）和欧盟（ERC, HIGH-HOPeS,
101039827）的资助。然而，所表达的观点和意见仅代表 M. T. S.
的观点和意见，并不一定反映欧盟或欧洲研究理事会执行局的观点和意见；欧盟或拨款机构均不对这些观点和意见负责。

作者感谢玛蒂尔德-帕皮隆和索菲亚-桑伯恩帮助改进图@ref(fig:hon)，并就张量图的发展进行了深入讨论。

\BeginKnitrBlock{flushright}

本书作者 \EndKnitrBlock{flushright}

\mainmatter

\section*{(PART*)
第一部分：基础知识}\label{part-ux7b2cux4e00ux90e8ux5206ux57faux7840ux77e5ux8bc6}
\addcontentsline{toc}{section}{(PART*) 第一部分：基础知识}

\section{引言}\label{ux5f15ux8a00}

最近几年，可用于计算分析的数据量呈指数级增长，包括科学数据以及常见的数据类型，如文本、图像和音频。丰富的数据使物理、化学、计算社会科学和生物学等各个领域都能利用机器学习技术（主要是深度神经网络）取得重大进展，由于深度神经网络可以有效地从大型数据集中总结和提取模式，因此适用于许多复杂的任务。深度神经网络的开发是为了从规则（欧式）域支持的数据中学习，如图像中的网格、文本序列和时间序列，此类模型包括卷积神经网络（CNNs）(\citeproc{ref-lecun1998}{LeCun
et al. 1998}; \citeproc{ref-krizhevsky2012}{Krizhevsky, Sutskever, and
Hinton 2012}; \citeproc{ref-simonyan2014}{Simonyan and Zisserman
2014})、递归神经网络（RNNs）(\citeproc{ref-bahdanau2014}{Bahdanau, Cho,
and Bengio 2014}; \citeproc{ref-sutskever2014}{Sutskever, Vinyals, and
Le 2014})、注意力Transformers(\citeproc{ref-vaswani2017}{Vaswani et al.
2017})等，这些模型已被证明在处理欧式域上的数据时非常有效(\citeproc{ref-goodfellow2016}{Goodfellow
et al.
2016})，在各类应用中表现出了前所未有的性能，最近的应用是聊天机器人，例如ChatGPT
((\citeproc{ref-adesso2023}{Adesso 2023})，以及和文本控制图像合成
(\citeproc{ref-rombach2022}{Rombach et al. 2022}) 。

然而，各领域的科学数据往往结构不同，不被常规的欧式域支持。因此，用深度神经网络处理这类数据一直是个挑战。在此背景下，几何深度学习（geometric
deep learning，GDL）(\citeproc{ref-wu2020comprehensive}{Zonghan Wu et
al. 2020}; \citeproc{ref-zhou2020graph}{Zhou et al. 2020};
\citeproc{ref-bronstein2021geometric}{Bronstein et al.
2021})作为深度学习模型向非欧式域的扩展而出现。 为了做到这种扩展，GDL
将计算限制在对称性、不变性、等变性等几何规则上。当然，在处理任意数据域时，包括集合(\citeproc{ref-qi2017pointnet}{Qi
et al. 2017}; \citeproc{ref-rempe2020caspr}{Rempe et al. 2020};
\citeproc{ref-deng2018ppfnet}{H. Deng, Birdal, and Ilic 2018};
\citeproc{ref-zhao20223dpointcaps}{Y. Zhao et al. 2022};
\citeproc{ref-huang2022multiway}{Jiahui Huang et al. 2022})、网格
(\citeproc{ref-boscaini2015learning}{Boscaini et al. 2015},
\citeproc{ref-boscaini2016learning}{2016};
\citeproc{ref-masci2015geodesic}{Masci et al. 2015};
\citeproc{ref-kokkinos2012intrinsic}{Kokkinos et al. 2012};
\citeproc{ref-shuman2016vertex}{Shuman, Ricaud, and Vandergheynst 2016};
\citeproc{ref-wu20153d}{Zhirong Wu et al. 2015};
\citeproc{ref-monti2017geometric}{Monti et al. 2017})、
流形(\citeproc{ref-boscaini2015learning}{Boscaini et al. 2015},
\citeproc{ref-boscaini2016learning}{2016};
\citeproc{ref-masci2015geodesic}{Masci et al. 2015};
\citeproc{ref-kokkinos2012intrinsic}{Kokkinos et al. 2012};
\citeproc{ref-shuman2016vertex}{Shuman, Ricaud, and Vandergheynst 2016};
\citeproc{ref-wu20153d}{Zhirong Wu et al. 2015};
\citeproc{ref-monti2017geometric}{Monti et al. 2017})、图
(\citeproc{ref-scarselli2008graph}{Scarselli et al. 2008};
\citeproc{ref-gallicchio2010graph}{Gallicchio and Micheli 2010};
\citeproc{ref-zhou2020graph}{Zhou et al. 2020};
\citeproc{ref-wu2020comprehensive}{Zonghan Wu et al. 2020};
\citeproc{ref-boscaini2016learning}{Boscaini et al. 2016};
\citeproc{ref-monti2017geometric}{Monti et al. 2017};
\citeproc{ref-bronstein2017geometric}{Bronstein et al. 2017};
\citeproc{ref-kipf2016semi}{Kipf and Welling
2016})，也允许施加适当的归纳偏差。尤其是图，由于其在众多科学研究中的适用性及其对传统网格的泛化能力，引起了人们的广泛兴趣。因此，图神经网络（GNNs）
(\citeproc{ref-bronstein2017geometric}{Bronstein et al. 2017};
\citeproc{ref-kipf2016semi}{Kipf and Welling
2016})的发展极大地增强了人们对几类自然存在的图类数据进行建模和分析的能力。

尽管 GDL 和 GNN
取得了成功，但从纯粹的几何视角来看图形，只能产生局部抽象，无法捕捉数据中的非局部属性和依赖关系。拓扑数据（\emph{Topological
data}）包括边（在图中）、三角形（在网格中）或小团（cliques）等的相互作用，在复杂物理系统的一系列新应用中自然存在
(\citeproc{ref-battiston2021physics}{Battiston et al. 2021};
\citeproc{ref-lambiotte2019networks}{Lambiotte, Rosvall, and Scholtes
2019})，诸如交通流预测(\citeproc{ref-jiang2022graph}{W. Jiang and Luo
2022})、社会影响力 (\citeproc{ref-zhu2018social}{Zhu et al.
2018})、蛋白质相互作用 (\citeproc{ref-murgas2022hypergraph}{Murgas,
Saucan, and Sandhu 2022})、分子涉及
(\citeproc{ref-schiff2020characterizing}{Schiff et al. 2020})、视觉增强
(\citeproc{ref-efthymiou2021graph}{Efthymiou et al. 2021})、推荐系统
(\citeproc{ref-la2022music}{La Gatta et al. 2022})，以及流行病学
(\citeproc{ref-deng2020cola}{S. Deng et al. 2020})等。
为了对这些数据进行原生而有效的建模，我们必须超越图形，并且考虑在某些几何变换下保持不变的量化空间属性。换句话说，我们需要考虑\emph{数据的拓扑结构}
(\citeproc{ref-carlsson2009topology}{G. Carlsson 2009})
来制定能够从复杂数据中提取语义信息的神经网络架构。

从数据中抽取更多全局信息的方法是超越基于图的抽象，去考虑图的扩展，例如单纯复形（simplicial
complexes）、胞腔复形（ cell
complexes）、超图（hypergraphs），甚至推广到科学计算中会遇到的更多的数据域(\citeproc{ref-bick2021higher}{Bick
et al. 2021}; \citeproc{ref-battiston2020networks}{Battiston et al.
2020}; \citeproc{ref-benson2021higher}{Benson, Gleich, and Higham 2021};
\citeproc{ref-torres2021and}{Torres et al.
2021})。继续发展机器学习模型，以便从这些拓扑域的支持的数据中学习，这是正在快速发展的新领域，我们在下文中将其称为\emph{拓扑深度学习（topological
deep learning，TDL）}。TDL
将多个研究领域交织在一起，包括拓扑数据分析（topological data
analysis，TDA)(\citeproc{ref-edelsbrunner2010computational}{Edelsbrunner
and Harer 2010}; \citeproc{ref-carlsson2009topology}{G. Carlsson 2009};
\citeproc{ref-dey22}{Dey and Wang 2022a};
\citeproc{ref-love2023topological}{Love et al. 2023a};
\citeproc{ref-ghrist2014elementary}{Ghrist
2014})、拓扑信号处理（topological signal processing）
(\citeproc{ref-schaub2018denoising}{Schaub and Segarra 2018};
\citeproc{ref-yang2021finite}{Yang et al. 2021};
\citeproc{ref-schaub2022signal}{Schaub et al. 2022};
\citeproc{ref-roddenberry2021signal}{T. Mitchell Roddenberry, Schaub,
and Hajij 2022}; \citeproc{ref-barbarossa2020topological}{Barbarossa and
Sardellitti 2020a}; \citeproc{ref-robinson2014topological}{Robinson
2014}; \citeproc{ref-sardellitti2022topological}{Sardellitti and
Barbarossa 2022})、网络科学（network science）
(\citeproc{ref-skardal2021higher}{Skardal et al. 2021};
\citeproc{ref-lambiotte2019networks}{Lambiotte, Rosvall, and Scholtes
2019}; \citeproc{ref-barabasi2013network}{Barabási 2013};
\citeproc{ref-battiston2020networks}{Battiston et al. 2020};
\citeproc{ref-bick2021higher}{Bick et al. 2021};
\citeproc{ref-bianconi2021higher}{Bianconi 2021};
\citeproc{ref-benson2016higher}{Benson, Gleich, and Leskovec 2016};
\citeproc{ref-de2016physics}{De Domenico et al. 2016};
\citeproc{ref-bao2022impact}{Bao et al. 2022};
\citeproc{ref-oballe2021bayesian}{Oballe et al.
2021})、几何深度学习（and geometric deep
learning）(\citeproc{ref-zhang2020deep}{S.-X. Zhang et al. 2020};
\citeproc{ref-cao2020comprehensive}{Cao et al. 2020};
\citeproc{ref-fey2019fast}{Fey and Lenssen 2019};
\citeproc{ref-loukas2019graph}{Loukas 2019};
\citeproc{ref-battaglia2018relational}{P. W. Battaglia et al. 2018};
\citeproc{ref-morris2019weisfeiler}{Morris et al. 2019};
\citeproc{ref-battaglia2016interaction}{P. Battaglia et al. 2016})。

尽管人们对 TDL
的兴趣与日俱增，但迄今为止还没有建立对这些思想的基本原理的广泛综合。我们认为，这是阻碍
TDL
取得进展的一个缺陷，因为它使得在不同概念之间建立联系变得具有挑战性，阻碍了比较，并使其他领域的研究人员难以找到
进入TDL领域 的切入点。因此，在本文中，我们旨在对 TDL
的基本原理进行基础性概述，不仅为近年来文献中出现的许多令人兴奋的观点提供一个统一的框架，而且作为一个概念起点，促进对新观点的探索。最终，我们希望这项工作能促进
TDL
的加速发展，我们相信这将是把深度学习的成功经验应用到更多应用场景的关键因素。

通过从代数拓扑学的传统拓扑概念(\citeproc{ref-ghrist2014elementary}{Ghrist
2014}; \citeproc{ref-hatcher2005algebraic}{Hatcher
2005})和高阶网络的最新进展
(\citeproc{ref-battiston2020networks}{Battiston et al. 2020},
\citeproc{ref-battiston2021physics}{2021};
\citeproc{ref-torres2021and}{Torres et al. 2021};
\citeproc{ref-bick2021higher}{Bick et al.
2021})中汲取灵感，我们首先引入组合复形\emph{(combinatorial complexes
，CCs)}作为我们TDL框架的主要构建模块。
CCs构建了一个新的拓扑域，将图、单纯复形、胞腔复形、超图等作为特例统一了起来，如图@ref(fig:main-figure)所示\footnote{本文中的所有图表都应使用彩色，因为不同的颜色传达不同的信息。}。与超图类似，CCs可以编码抽象实体集间的类集合关系。此外，CCs
还可以构建层次化的高阶关系，类似于单纯复形和胞腔复形中的关系。因此，CCs
概括并结合了超图和胞腔复形的理想特性。

\begin{figure}

{\centering \includegraphics{figures/main_figure} 

}

\caption{该图解直观展示了我们的主要贡献。(a): 不同的数学结构可用来表示抽象实体之间的不同关系。集合由无连接的实体组成，图编码了顶点间的二元关系，单纯复形和胞腔复形模型化了分层的高阶关系，超图则表示了无层次的任意集合型关系，我们采用组合复形（CCs）来覆盖图、单纯复形、胞腔复形和超图，CCs 不仅具有集合型关系，也能表示关系间的层次结构。(b): 通过利用 CCs 的层次和拓扑结构，我们引入了前推操作（push-forward），它是高阶消息传递协议和 CCs上非池化/池化操作的基本构件。前推操作可用来够构建组合复形神经网络（combinatorial complex neural networks，CCNN），为高阶域上的拓扑深度学习提供通用的概念框架。}\label{fig:main-figure}
\end{figure}

此外，此外，我们还引入了构建深度神经网络所需的算子，用于学习锚定在
CCs上的输入特征和抽象摘要。这些算子提供了卷积、注意机制、消息传递，以及包含不变性、等变性或其他几何规律的方法。具体来说，前推操作（\emph{push-forward
operation}）允许在不同维度之间推送数据，从而形成了一个基本构件，用于在CCs上定义高阶消息传递协议（\emph{higher-order
message-passing protocols}）、非池化/池化（\emph{(un)pooling
operations}）等操作。由此产生的学习模型，我们称之为组合复杂神经网络\emph{（combinatorial
complex neural
networks，CCNNs）}，它能够学习抽象的高阶数据结构，这在我们的实验评估中得到了清晰的验证。

我们希望我们的贡献能成为一个平台，鼓励研究人员和从业人员扩展我们的
CCNNs，并邀请社区在我们工作的基础上扩展高阶领域的
TDL。图@ref(fig:main-figure)直观地概括了我们的贡献，具体如下：

\begin{itemize}
\item
  首先，我们为TDL引入了CCs域，我们描述了 CCs
  的特征及其属性，并解释了它们如何用来推广到现有的数据域，如图、超图、单纯复形和胞腔复形。因此，CCs
  可以作为一个统一的起点，帮助学习拓扑数据的表达式表示。
\item
  其次，使用CCs作为数据域，我们构造了CCNNs，一个高阶消息传递神经网络的抽象类，可为基于超图、胞腔复形的TDL模型提供统一的蓝图。

  \begin{itemize}
  \item
    基于CCs上的push-forward运算，我们为CCNNs引入了卷积、注意力、非池化和池化操作。

    \begin{itemize}
    \tightlist
    \item
      我们形式化并研究了 CCNNs
      的变换(\emph{permutation})和方向等变(\emph{orientation
      equivariance})，为今后 CCNNs 的几何化工作铺平了道路。
    \end{itemize}
  \item
    我们展示了如何通过图形符号直观地构建 CCNNs。
  \end{itemize}
\item
  最后，我们在实践场景中评估了我们的想法

  \begin{itemize}
  \item
    我们以python库的形式发布了我们框架的源代码：\emph{TopoNetX,
    TopoEmbedX，TopoModelX}
  \item
    我们的研究表明，在形状分析和图学习等各种应用中，CCNNs
    的预测性能可与最先进的特定任务神经网络相媲美。
  \item
    我们将我们的工作与 TDA 中的经典构造建立了联系，如 \emph{mapper}
    (\citeproc{ref-singh2007topological}{Singh et al.
    2007})。尤其，我们用 TDL 框架实现了mapper，并演示了如何将其用于 CCs
    的高阶非池化和池化操作。

    \begin{itemize}
    \tightlist
    \item
      我们也展示了任何 CC 都可以还原为一种叫做Hasse 图(\emph{Hasse
      graph})的特殊图。这使得我们能够用基于图的模型来描述 CCNNs
      的某些方面，从而将高阶表示学习归约为图表示学习（使用放大的计算图）。
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{术语}

在深入探讨更多细节之前，我们先介绍一下本文中使用的既定概念的基本术语。其中一些术语将在第@ref(preliminaries)章中重新正式讨论。附录@ref(glossary)为本文提出的新观点提供了符号和术语表。

\begin{glossarybox}
\textbf{胞腔复形}(\href{https://app.vectary.com/p/3EBiRiJcYjFNvkbbWszQ0Z}{\textbf{Cell
complex}})：一类拓扑空间，是拓扑圆盘（胞腔）的不相交并，其中每个胞腔都与欧式球的内部同构，这些胞腔通过附加映射以局部合理的方式连接在一起。

\textbf{域(Domain)}：通常指支持数据的底层空间。

\textbf{实体（Entity）}或\textbf{顶点（vertex）}：一个抽象的点，也可以认为是集合的元素

\textbf{图（Grapg）}或网络：一组由边集联系起来的实体或顶点集合，其中，边集表示了顶点间的二元关系。

\textbf{拓扑域上的层次化结构(Hierarchical
structure)}：通过一个整数值函数为域中的每个关系都分配一个正整数（秩，rank），使得高阶关系可以被分配更高的秩值。例如，一个单纯复形具有由其单纯形的基数（cardinality）推导出的层次化结构。

\textbf{高阶网络/拓扑域（Higher-order network/topological
domain）}：图的推广，反应实体间的二元或高阶关系，单纯复形、胞腔复形、超图式此类高阶网络的示例。

\textbf{超图（Hypergraph）}：一组由超边集联系的实体（顶点）集合，超边表示顶点间的二元或高阶关系。

\textbf{消息传递（Message
passing）}：一种在域上领域实体间传递数据、`消息'的计算框架，根据从领域收到的消息来更新每个实体的表示

\textbf{关系（Relation）}或\textbf{胞腔（cell）}：实体（顶点）集的子集，如果关系的基数等于2，则说它是\textbf{二元关系}；如果基数大于2，则说它是\textbf{高阶关系}。
\textbf{集合型关系（Set-type
relation）}：如果高阶网络中的某一关系存在且不蕴含于网络中的另一关系，则称该网络具有集合类型关系。例如，超图就包含集合类型关系。

\textbf{单纯形（Simplex）}：三角形或四面体在任意维度上的推广，例如，维度为0、1、2、3的单纯形分别是点、线段、三角形、四面体。

\textbf{单纯复形}
\href{https://app.vectary.com/p/4HZRioKH7lZ2jWESIBrjhf}{\textbf{Simplicial
complex}}:单纯形的集合，集合中每个单纯形的每个面也都在集合中，集合中任意两个单纯形的交集要么是空的，要么是两个单纯形的一个面。

\textbf{拓扑数据（Topological data）}：支持拓扑域中关系的特征向量

\textbf{拓扑深度学习（Topological deep
learning）}：用深度学习技术研究拓扑域，用拓扑域在深度学习中表示数据

\textbf{拓扑神经网络（Topological neural
network）}：在拓扑域上处理数据的深度学习模型

\end{glossarybox}

\section{研究动机}\label{motivation}

本文介绍的组合复形神经网络（CCNNs）通过拓扑结构推广了图神经网络（GNNs）及其高阶类似物。在本节中，我们将从三个角度介绍在机器学习，特别是深度学习中使用拓扑结构的动机。第一，\emph{数据建模(}
data
modeling)\emph{：拓扑抽象如何帮助我们推理和计算拓扑空间上支持的各种数据类型；第二，}实用性(\emph{utility)}：怎了利用拓扑结构来提高性能。第三，\emph{统一性（unification）}：拓扑如何用于综合和抽象不同的概念。

\subsection{从拓扑空间数据中建模和学习}\label{ux4eceux62d3ux6251ux7a7aux95f4ux6570ux636eux4e2dux5efaux6a21ux548cux5b66ux4e60}

在机器学习上下文中，域上支持的典型数据类型是线性或者说向量空间。这种底层向量空间为许多计算提供了便利，并且经常被隐含地假定。然而，正如几何深度学习所认识到的那样，考虑不同领域支持的数据往往至关重要。事实上，出于各种原因，明确考虑域所支持的数据并对其进行建模至关重要。

首先，不同的域可能具有不同的特征和属性，这就需要不同类型的深度学习模型来有效处理和分析数据。例如，图域可能需要图卷积网络
(\citeproc{ref-kipf2016semi}{Kipf and Welling 2016})
来考虑非欧结构，而点云域则可能需要类似 PointNet 的架构
(\citeproc{ref-qi2017pointnet}{Qi et al. 2017};
\citeproc{ref-zaheer2017deep}{Zaheer et al. 2017})来处理无序的点集。

其次，
每个域中的具体数据在大小、复杂性和噪声方面都会有很大差异。通过了解特定域内的特定数据属性，研究人员可以开发出针对这些特定属性的模型。例如，针对高噪声点云设计的模型可能会采用离群点去除或局部特征聚合等技术。

第三，准确区分领域和数据对于开发能够很好地概括新的、未见过的数据的模型非常重要。通过识别域的底层结构，研究人员可以开发出既能适应数据变化，又能捕捉相关几何特征的模型。总之，通过同时考虑域和数据，研究人员可以开发出更适合处理不同几何学习任务的特定挑战和复杂性的模型。

\textbf{数据的不同视角：域（domains）和关系（relations）}。文献中提到拓扑数据或拓扑数据分析时，对于实际数据包括什么以及数据应该建模什么存在不同观点。在此，我们遵循(\citeproc{ref-bick2021higher}{Bick
et al.
2021})的观点，区分不同类型的数据和我们学习过程的目标，尽管这些不同类型数据之间的区别有时可能比这里介绍的更加模糊。

\emph{关系数据（Relational
data）}，用于描述不同实体或对象间的关系，这一基本概念有多种表现形式，例如社交网络中用户之间的联系，朋友关系或追随者关系，就是这种关系的例子。传统上，人们通过图来理解这些关系，图中的顶点和边分别代表实体和实体之间的成对连接（\emph{pairwise
connections}）。换句话说，图中的边被视为度量单位，每条关系数据都涉及两个顶点。

然而，现实世界中的许多现象自然涉及复杂的多方互动，涉及不止两个实体以错综复杂的方式相互影响。例如，社会中的个人群体、合著者集合、相互影响的基因或蛋白质都是这种高阶关系的例子。这种依赖关系超越了成对的相互作用，可以用高阶关系（\emph{higher-order
relations}）来建模，并恰当地抽象为超图、胞腔复形或
CCs。显然，拓扑学思想可以在理解这些数据方面发挥重要作用，关键是能让我们从模拟成对关系的图转向更丰富的表征。

\emph{数据作为域的实例}，与TDA
采用的相关概念类似，即我们的观测数据应该代表什么。也就是说，所有观测到的数据都应该对应于拓扑对象本身的噪声实例。例如，我们的目标是学习数据的
``拓扑形状''。请注意，在这种情况下，我们通常不会直接观察相关数据，而是从观察到的数据中构建关系，然后利用这些关系对观察到的数据集进行分类，对点云数据进行的持久同源性分析就是这种观点的一个主流例子。

\emph{拓扑域上支持的数据类型}，当在拓扑域（例如图）上考察数据时，拓扑化思想起着重要的作用。它可能会是某种特定的动态变化，例如流行病的传播，也可能是顶点上支持的任何类型的其他数据或信号(图上的边缘信号（edage-signals）的情况不经常被考虑，尽管存在例外情况(\citeproc{ref-schaub2018denoising}{Schaub
and Segarra 2018}; \citeproc{ref-schaub2021signal}{Schaub et al.
2021}))。在图信号处理（graph signal
processing）的语境下，在图域上可以定义多种函数或信号，并且称他们是\emph{被该域支持}。重要的是，图本身可以任意复杂，典型情况下看作是固定的，观察到的数据不是关系的，而是图的顶点上支持所支持的。

在超越图结构方面更一般的拓扑结构，例如CCs，可支持的数据不仅在顶点和边上，也在其他的高阶实体上，正如图@ref(fig:support)(a-b)上所展示的。例如，计算机图形学中定义在网格上的向量场（vector
fields）就是边和面上通常支持的数据形式，可以很方便地建模为高阶域上支持的数据形式
(\citeproc{ref-de2016vector}{Goes, Desbrun, and Tong
2016})。类似的，类别标签化的数据也可以在给定网格的边和面上提供支持，看图
@ref(fig:support)(c)中的例子。为了处理这样的数据，就需要再去考虑底层拓扑域上的结构。

\begin{figure}

{\centering \includegraphics{figures/two_tori_horse} 

}

\caption{数据可以在高阶关系上很自然的得到支持. (a): 基于边的向量场（edge-based vector field）. (b): 基于面的向量场（face-based vector field）.（a）和（b）上的向量场都定义在胞腔复形torus结构上^[此URL提供（a）和（b）的交互式可视化[戳这里](https://app.vectary.com/p/5uLAflZj6U2kvACv2kk2tN).]。（c）：类别标签化的拓扑数据可以很自然的在高阶关系上被支持。例如，二维面（2-Face）上的网格分割标签（mesh segmentation labels）可以用不同的颜色来描述（蓝色，绿色，绿松石色，粉色，棕色），以表示马的不同部分（头部，颈部，身体，腿部，尾部）}\label{fig:support}
\end{figure}

\textbf{建模和处理图之外的数据：示例。}图非常适合建模表现出成对相互作用的系统，例如，如图@ref(fig:cloth)(a)所示，基于粒子的流体模拟可以用图有效地表示，消息传递用于更新流体分子的物理状态。采用这种方法，每个分子都表示为包含流体分子物理状态的顶点，边则将他们连接起来，以进行交互作用的计算(\citeproc{ref-sanchez2020learning}{Sanchez-Gonzalez
et al. 2020}; \citeproc{ref-shlomi2020graph}{Shlomi, Battaglia, and
Vlimant 2020})。

\begin{figure}

{\centering \includegraphics{figures/cloth} 

}

\caption{在图或高阶网络上处理数据的例子。 (a): 图可用于建模流体动力学中的分子相互作用，其中，顶点表示粒子， 粒子与粒子之间的相互作用通过顶点之间的信息传递来模拟  [@sanchez2020learning; @shlomi2020graph]. (b): 在对弹性（spring）和自碰撞(self-collision)建模时，自然要使用边而不是顶点，这是 这是因为布料的拉伸行为是由沿边缘作用的拉力和压力决定的，而不仅仅是由单个颗粒的位置决定的。 为了模拟多条边之间的相互作用，可以使用多边形面来表示布的局部几何形状,多边形面可提供计算边缘间高阶信息传递的方法}\label{fig:cloth}
\end{figure}

图不适用于布料仿真等更复杂的系统建模，因为状态变量是和边或三角面等关联的，而非顶点，在这种情况下，就需要高阶消息传递来计算和更新物理状态，如图@ref(fig:cloth)(b)所示。在自然语言处理中也有类似的挑战，语言表现出了多层次的语法、语义和语境。虽然，GNN可以捕获词汇间基本的语法和语义关系，但是否定、讽刺或挖苦等更复杂的关系可能难以表述
(\citeproc{ref-girault2017towards}{Girault, Narayanan, and Ortega
2017})。将高阶关系和层次关系纳入其中，可以为这些关系提供更好的模型，并能更细致、更准确地理解语言。

\subsection{拓扑的有用性}\label{the-utility-of-topology}

除了为复杂系统建模提供多功能框架外，TDL
模型还具有广泛的用途：它们可以为模型学习推导出有意义的归纳偏差，促进底层域中更大范围内的高效信息传播，增强现有基于图的模型的表达能力，并有可能揭示深度网络本身的关键特征，下文将对此进行介绍。

\textbf{建立数据的分层表示}，
虽然利用高阶信息对学习表征很重要，但保留实体间复杂的高阶关系对实现强大而多变的表征也很关键。例如，人们可以通过分层建立关系之间的关系来形成抽象和类比。然而，常用于在各种实体间建立关系推理的基于图的模型(\citeproc{ref-santoro2017simple}{Adam
Santoro et al. 2017}; \citeproc{ref-zhang2020deep}{S.-X. Zhang et al.
2020}; \citeproc{ref-chen2019graph}{Yunpeng Chen et al. 2019};
\citeproc{ref-schlichtkrull2018modeling}{Schlichtkrull et al.
2018})在建立实体间高阶关系的能力方面却是是有限的。

分层关系推理的需求(\citeproc{ref-xu2022groupnet}{C. Xu et al.
2022})要求所用方法能够捕捉关系之间更深层次的抽象关系，而不仅仅是模拟原始实体之间的关系。高阶网络模型为应对高阶关系推理的挑战提供了一个很有前景的解决方案，如图所示@ref(fig:hs)。

\begin{figure}

{\centering \includegraphics{figures/hs} 

}

\caption{用高阶网络表示分层数据的示例。黑色箭头表示通过高阶关系扩增图，橙色箭头表示粗略的图提取。(a): 表示抽象实体（黄色顶点）间二元关系（粉色边）的图编码。 (b): 蓝色胞腔表示的高阶关系可看作是原始图中顶点或边之间的关系。 (c): 提取原始图的粗略版本。在粗略图中，顶点代表原始图的高阶关系（蓝色胞腔），边代表这些蓝色胞腔的交集。 (d-e): 重复同样的过程，可以得到原始图的更粗略版本。整个过程对应于分层的高阶关系，即关系之间的关系，从而提取意义和内容 (包括数据形状),这是拓扑数据分析的常见任务[@carlsson2009topology; @dey22].}\label{fig:hs}
\end{figure}

\textbf{通过归纳偏置提高性能}，
拓扑深度学习模型通过提供一个定义明确的过程，以将图形提升到指定的高阶网络，从而提高基于图形的学习任务的预测性能。通过这种方法纳入多节顶点关系可被视为一种归纳偏差，可用于提高预测性能。归纳偏置允许学习算法根据观察到的数据之外的因素，优先选择一种解决方案，而不是另一种解决方案(\citeproc{ref-mitchell1980need}{T.
M. Mitchell 1980})。图 @ref(fig:inductivebias)
展示了使用高阶胞腔对图进行增强的两种形式。

\begin{figure}

{\centering \includegraphics{figures/inductive_bias} 

}

\caption{中间的图可以用高阶关系来增强，以改进学习任务.在(a)中,在缺失的面上添加了胞腔；[@bodnar2021weisfeiler]考虑了类似的归纳偏置，以改进分子数据的分类。在（b）中，图中的一些顶点添加了单跳邻域；[@feng2019hypergraph]，并使用一种基于将图提升到其相应单跳邻域超图的归纳偏置，以提高基于顶点任务的性能}\label{fig:inductivebias}
\end{figure}

\textbf{构建基于图的高效远程消息传递系统}，TDL
网络的分层结构有助于高效地构建远距离交互。利用这种结构，定义在域顶点上的信号可以在通过长边路径连接的顶点之间有效传播远距离依赖关系。图
@ref(fig:ft)说明了通过增加拓扑关系对图进行扩充，并将信号反池化（unpooling）回顶点，从而提升定义在图顶点上的信号的过程。(\citeproc{ref-hajij2022high}{Hajij,
Ramamurthy, et al. 2022})
利用这种池化和反池化操作来构建能够捕捉远距离依赖关系的模型，从而更有效、更自适应地学习底层域上的局部和全局结构。关于基于图的池化的相关工作，我们推荐读者参阅(\citeproc{ref-su2021hierarchical}{Su,
Hu, and Li 2021}; \citeproc{ref-itoh2022multi}{Itoh, Kubo, and Ikeda
2022})。

\begin{figure}

{\centering \includegraphics{figures/factors_thru} 

}

\caption{二元或高阶胞腔之间的消息传递示意图。(a): 使用基于图的消息传递方案时，一些从顶点$x$ 开始的信息需要经过很长的距离，即长边路径，才能到达顶点 $y$。(b): 使用蓝色胞腔表示的高阶胞腔结构，消息可以从顶点提升到高阶胞腔，从而以更少的步骤在顶点 $x$ 和 $y$ 之间来回传播。这种快捷方式可以在属于蓝色胞腔的所有顶点之间，以更少的计算步骤高效地共享信息[@hajij2022high]。}\label{fig:ft}
\end{figure}

\textbf{提高表达能力}，TDL
可以捕捉传统基于图的模型无法捕捉的复杂而微妙的依赖关系。通过允许更丰富的高阶交互，高阶网络可以提高表达能力，并在许多任务中带来卓越的性能(\citeproc{ref-morris2019weisfeiler}{Morris
et al. 2019}; \citeproc{ref-bodnar2021weisfeiler}{Bodnar et al. 2021})。

\subsection{深度学习和结构化计算的统一视角}\label{ux6df1ux5ea6ux5b66ux4e60ux548cux7ed3ux6784ux5316ux8ba1ux7b97ux7684ux7edfux4e00ux89c6ux89d2}

拓扑学提供了一个框架来泛化科学计算中通常会遇到的众多离散域。这些领域的例子包括图、单纯复形和胞腔复形；见图@ref(fig:unifying)。在这些域上支持的计算模型可以被视为各种深度学习架构的泛化；也就是说，可以为比较和设计
TDL 架构提供一个统一的框架。此外，通过统一理论了解不同类型 TDL
之间的联系，可以为复杂系统的底层结构和行为提供有价值的见解，因为在复杂系统中自然会出现高阶关系(\citeproc{ref-hansen2019toward}{Hansen
and Ghrist 2019}; \citeproc{ref-hajijcell}{Hajij, Istvan, and Zamzmi
2020}; \citeproc{ref-bick2021higher}{Bick et al. 2021};
\citeproc{ref-majhi2022dynamics}{Majhi, Perc, and Ghosh 2022})。

\begin{figure}

{\centering \includegraphics{figures/unifying} 

}

\caption{我们的工作引入了组合复形，它是一种高阶网络，可泛化科学计算中通常遇到的大多数离散域。 包括 (a) 序列和图像, (b) 图, (c) 3D形状和单纯复形, (d) 立方体（cubical）和胞腔复形（cellular complexes）, (e) 离散流形（discrete manifolds）, (f) 超图.}\label{fig:unifying}
\end{figure}

\textbf{了解深度网络的工作原理}，当代深度神经网络的一个显著特点是，即使拥有大量参数，它们也能很好地泛化未见数据。这与我们以往对统计学习理论的理解相矛盾，促使研究人员寻求新的方法来理解深度神经网络的底层工作原理(\citeproc{ref-neyshabur2019role}{Neyshabur
et al.
2019})。最近的研究发现，深度神经网络推导出的图拓扑或其相应的训练轨迹（建模为马尔科夫链）与泛化性能表现出很强的相关性
(\citeproc{ref-birdal2021intrinsic}{Birdal et al. 2021};
\citeproc{ref-dupuis2023generalization}{Dupuis, Deligiannidis, and
Şimşekli
2023})。一个悬而未决的问题是，在支持高阶域的学习架构中，拓扑结构、计算函数和泛化特性之间能在多大程度上存在这种关联。我们相信，通过使用我们提出的拓扑结构（如
CCs 和 CCNNs）来研究这些问题，不仅能深入了解
TDL，还能更广泛地了解深度学习。

\section{预备知识}\label{preliminaries}

在各种机器学习应用中，集合\(S\)中实体间的邻近性概念具有重要意义，因为它有助于理解\(S\)中实体间的关系。例如，聚类算法的目的是将相互接近的点进行分组。在推荐系统中，目标是推荐与用户已表示感兴趣的项具有相似性的项。然而，问题是我们如何精确量化近似性的概念？

考虑一个由抽象实体集合组成的集合\(S\)，如图@ref(fig:proximity)(a)所示。考虑同一图中的红色实体（节点）\(x\)。我们希望找出\(S\)中与\(x\)``密切相关''或
``非常接近''的实体（节点）。然而，集合本身并没有实体之间的接近或关系的概念。

\begin{figure}

{\centering \includegraphics{figures/proximity} 

}

\caption{集合$S$实体间邻近性概念的说明. (a): 抽象实体的有限集合$S$. (b): $S$中实体$x$（红色）的邻域（黄色），邻域的定义是$S$中通过边与$x$相邻的实体集合。 (c): $x$的邻域，由$S$中与红色实体$x$距离最远为2的所有黄色实体组成. (d): 由$S$中所有与红色实体$x$形成三角形（蓝色）的黄色实体组成的$x$邻域. (e): 由 $S$ 中所有与红色实体 $x$ 形成梯形（蓝色）的黄色实体组成的 $x$ 邻域。}\label{fig:proximity}
\end{figure}

在集合 \(S\)
的实体间定义二元关系（边）是引入邻近性概念的一种方法，其结果是一个顶点集为
\(S\) 的图，如图所示@ref(fig:proximity)(b)。 利用这种定义在集合 \(S\)
上的边的 ``辅助结构''，我们就可以声明 \(x\) 的 ``局部邻域''（用
\(\mathcal{N}(x)\) 表示），它是 \(S\) 的子集，由通过一条边与 \(x\)
相邻的所有实体组成。在图@ref(fig:proximity)(b)中，\(S\)中顶点\(x\)（红色）的邻域由所有黄色顶点组成。

在图@ref(fig:proximity)(b) 中，邻域 \(\mathcal{N}(x)\)
的选择可以是任意的。例如，通过定义\(mathcal{N}(x)\)包含所有与红色顶点\(x\)的距离最多为2的顶点，可以给出另一个可选的有效邻域概念，即图@ref(fig:proximity)(c)中所有黄色顶点。在图@ref(fig:proximity)(d)中，\(mathcal{N}(x)\)邻域是由所有与红色顶点\(x\)形成三角形的黄色顶点组成的。在图@ref(fig:proximity)(e)中，\(mathcal{N}(x)\)邻域是由所有与红色顶点\(x\)形成梯形的黄色顶点组成的。从图（d）和（e）中可以清楚地看出，三角形和梯形等其他辅助结构也可以用来定义邻域概念。实际上，邻域的选择通常取决于应用。最近的研究探索了使用图形几何来获得更丰富的邻域概念，如
(\citeproc{ref-morris2019weisfeiler}{Morris et al. 2019};
\citeproc{ref-hajijcell}{Hajij, Istvan, and Zamzmi 2020};
\citeproc{ref-zhao2022from}{L. Zhao et al. 2022})
所示。不过，基本概念仍然相同：首先引入定义在顶点集上的辅助结构，然后利用辅助结构推导出定义明确的邻近性概念。

将图推广到高阶网络后，自然也要将图的邻域概念推广到高阶网络。拓扑学(\citeproc{ref-munkres1974}{Munkres
1974})对集合 \(S\) 实体间邻域或邻近性的精确概念已经进行了研究，定义在
\(S\) 上的拓扑允许我们有意义地描述 \(S\)
中元素之间的邻近性。本节将介绍拓扑概念和定义，目的是将图推广到高阶网络。

\subsection{邻域函数和拓扑空间}\label{ux90bbux57dfux51fdux6570ux548cux62d3ux6251ux7a7aux95f4}

拓扑空间有几种等价的定义方法。例如，拓扑空间通常用 ``开集''或
``闭集''来定义，参见 (\citeproc{ref-munkres1974}{Munkres
1974})。在本文中，我们选择用
``邻域''来定义拓扑空间。这一定义更符合通常定义在图上的消息传递范式(\citeproc{ref-gilmer2017neural}{Gilmer
et al.
2017})，而且可以推广到高阶网络。关于为什么以邻域为单位的定义等同于以开集为单位的定义，我们请读者参阅(\citeproc{ref-brown2006}{Brown
2006})。

\phantomsection\label{ns}
令\(S\)
是非空集。\(S\)上的\emph{邻域函数}是函数\(\mathcal{N}\colon S\to\mathcal{P}(\mathcal{P}(S))\)，该函数给\(S\)中的每个点\(x\)分配一个\(S\)非空子集\(\mathcal{N}(x)\)。\(\mathcal{N}(x)\)中的元素被称为\(x\)相对于\(\mathcal{N}\)的\emph{邻域}。

\phantomsection\label{nt}
令\(\mathcal{N}\)是集合\(S\)上的邻域函数，如果\(\mathcal{N}\)满足如下规则，则称\(\mathcal{N}\)为\(S\)上的领域拓扑:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  如果\(N\)是\(x\)的邻域, 则\(x\in N\).
\item
  如果\(N\)包含\(x\)邻域的\(S\)的子集,则\(N\)是\(x\)的邻域.
\item
  \(S\)内顶点\(x\)的两个邻域的交集也是\(x\)的邻域.
\item
  \(S\)内顶点\(x\)存在一个邻域\(M\)，使得\(x\)的任何邻域\(N\)也是\(M\)内每个顶点的邻域。
\end{enumerate}

\phantomsection\label{topospace}
由非空集\(S\)和\(S\)上的邻域拓扑\(\mathcal{N}\)组成的\((S,\mathcal{N})\)称为\emph{拓扑空间}。

因此，拓扑空间就是定义了邻域函数\(\mathcal{N}\)，且满足定义@ref(def:nt)中属性的集合\(S\)。在章节@ref(neighbourhood-functions-on-ccs)，我们将引入高阶网络环境下关于近邻的类似概念。进一步来讲，邻域函数\(\mathcal{N}\)的选择是构建高阶域支持的深度学习模型的第一步，也是最基本的一步(参见@ref(combinatorial-complex-neural-networks)章节)。

\subsection{填补与高阶网络间的代沟}\label{bridging-the-gap-among-higher-order-networks}

给定抽象实体组成的有限集\(S\)，\(S\)上的邻域函数\(\mathcal{N}\)可以通过给\(S\)配置辅助结构来推导，例如边，如图@ref(fig:proximity)(b)所示。边提供了一种定义\(S\)内实体间关系的方法\footnote{\(S\)上的关系是\(S\)的非空子集}，特别说明的是，每条边都定义了\(S\)上的一种二元关系（例如，两个实体间的关系）。在许多应用中，我们希望允许包含两个以上实体间的关系，这种涉及两个以上实体间关系的思想就是高阶网络的核心。这种高阶关系使得可以在\(S\)上定义更广泛的邻域函数，以捕获\(S\)内实体间的多路交互作用。

为了描述更广泛的多路相互作用，就需要使用更复杂的邻域函数和拓扑。章节
@ref(combinatorial-complexes)着眼于定义通用的高阶网络（正如章节@ref(motivation)所谈到的研究动机），本节则综述通常研究的高阶网络的定义、优势和劣势，包括（抽象的）单纯复形、正则胞腔复形（regular
cell
complexes）、超图等。在章节@ref(combinatorial-complexes)，我们将引入组合复形，该定义更广义且能填补通常研究的高阶网络间的代沟。

单纯复形是带有期望属性的最简单的高阶域，它将图的相应属性进行了扩展。例如，\textbf{Hodge理论}是在单纯复形上很自然的定义，它扩展了图的类似属性(\citeproc{ref-barbarossa2020topological}{Barbarossa
and Sardellitti 2020a}; \citeproc{ref-schaub2020random}{Schaub et al.
2020}, \citeproc{ref-schaub2021signal}{2021})。

\phantomsection\label{scmain}
非空集\(S\)上的\href{https://app.vectary.com/p/4HZRioKH7lZ2jWESIBrjhf}{\emph{抽象单纯复形（abstract
simplicial complex）}}被表示为\((S,\mathcal{X})\)，其中，\(\mathcal{X}\)
是\(\mathcal{P}(S) \setminus \{\emptyset\}\)的子集，且满足\(x \in \mathcal{X} \bigcup y \subseteq x \rightarrow y \in \mathcal{X}\)，
\(\mathcal{X}\)中的元素被称为\emph{单纯形（simplices）}

\begin{glossarybox}

\begin{quote}
\textbf{译者注}：原文中上述定义中的\(x \in \mathcal{X} \bigcup y \subseteq x \rightarrow y \in \mathcal{X}\)是这么写的\(x \in \mathcal{X}\)
and \(y \subseteq x\) imply \(y \in \mathcal{X}\)
\end{quote}

\end{glossarybox}

图@ref(fig:unifying)(c)给出了三角形网格（triangular
meshes）的示例，这是计算机图形学中许多应用中常见的单纯复形的特例。读者可参考(\citeproc{ref-schaub2021signal}{Schaub
et al. 2021}; \citeproc{ref-crane2013digital}{Crane et al.
2013})获得关于单纯复形的相关介绍。从定义
@ref(def:scmain)中可看出，\(S\)上的每个关系\(x\)必须包含所有具有\(y\subseteq x\)的关系\(y\)。因此，一个单纯复形可能会编码了相当大量的数据，这将占用大量内存(\citeproc{ref-roddenberry2021signal}{T.
Mitchell Roddenberry, Schaub, and Hajij
2022})。此外，现实世界中的高阶数据（如矩形街道网络上的交通流量）可能无法采用有意义的单纯复形结构，这是因为底层数据空间本身缺乏可用的单纯形。为了解决这一局限，可以采用胞腔复形，胞腔复形(\citeproc{ref-hatcher2005algebraic}{Hatcher
2005}; \citeproc{ref-hansen2019toward}{Hansen and Ghrist
2019})可以泛化单纯复形，并克服它们的许多缺点。

\begin{glossarybox}

\begin{quote}
\textbf{译者注：} 图@ref(fig:unifying)(c)是三角剖分
\end{quote}

\end{glossarybox}

\phantomsection\label{rccmain}
\href{https://app.vectary.com/p/3EBiRiJcYjFNvkbbWszQ0Z}{\emph{正则胞腔复形}}是一个可划分为多个子空间（\emph{胞腔}）的拓扑空间
\(S\)，其子空间为 \(\{x_\alpha\}_{\alpha \in P_{S} }\)，其中 \(P_{S}\)
是索引集。正则胞腔复形满足以下条件：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(S= \cup_{\alpha \in P_{S}} \mbox{int}(x_{\alpha})\),
  其中，\(\mbox{int}(x)\) 胞腔\(x\)内部。
\item
  对每个\(\alpha \in P_S\),
  都存在\(x_\alpha\)的一个同胚（homeomorphism）映射\(\psi_{\alpha}\)（称作\emph{粘合映射，attaching
  map）},
  将\(x_\alpha\)映射到\(\mathbb{R}^{n_\alpha}\)，\(n_\alpha\in \mathbb{N}\)，\(n_\alpha\)称作胞腔\(x_\alpha\)的\emph{维度（dimension）}。
\item
  对每个胞腔\(x_\alpha\)，边界(boundary)\(\partial x_{\alpha}\)是有限多个胞腔的并集，且每个这样的胞腔的维度都小于
  \(x_\alpha\)的维度。
\end{enumerate}

\begin{glossarybox}

\begin{quote}
\textbf{译者注：}
\end{quote}

\begin{quote}
粘合映射，attaching
map，描述如何将一个更高维的胞腔粘合到现有的胞腔复形上的过程。在这个过程中，粘合映射定义了新胞腔的边界如何与现有复形的低维胞腔相连。在构造拓扑空间时，如何将两个或多个空间粘合在一起形成一个新的空间。粘合映射是定义在边界上的一个连续映射，它将一个空间的边界映射到另一个空间的边界上，从而实现空间的粘合。
\end{quote}

\begin{quote}
同态,
homomorphism，描述的是两个代数结构之间的映射关系，这种映射保持了两个结构之间的运算关系。
\end{quote}

\begin{quote}
同胚，homeomorphism，描述的是两个空间之间可以通过连续的变形从一个变成另一个，双方连续的一一映射，从拓扑学的角度来说就认为两个图是没有区别的。
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  条件2说明\(x_\alpha\)可被编码为实数空间中的向量编码？
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  条件3说明\(x_\alpha\)的边界是比\(x_\alpha\)低阶的结构？
\end{enumerate}
\end{quote}

\end{glossarybox}

为简洁起见，我们下文将 ``正则胞腔复形''称为
``胞腔复形''。胞腔复形包含多种高阶网络，许多高阶网络可以看作胞腔复形的实例。例如，胞腔复形是图、单纯复形和立方复形的自然推广(\citeproc{ref-hajijcell}{Hajij,
Istvan, and Zamzmi 2020})。图@ref(fig:unifying)(d)
给出了胞腔复形的一些示例。直观地说，一个胞腔复形是多个胞腔的不相交并，其中每个胞腔都与某个
\(k\) 的
\(k\)维欧式球的内部同构。这些胞腔通过粘合映射以局部适合的方式连接在一起。正则胞腔复形的粘合映射信息可以组合形式存储在矩阵序列中，该矩阵称作\emph{关联矩阵（incidence
matrices）}(\citeproc{ref-hatcher2005algebraic}{Hatcher
2005})，在章节@ref(incidence-in-a-cc)对这些矩阵有详细的描述。

定义@ref(def:rccmain)中的条件3被称作正则胞腔复形的\emph{正则条件（regularity
condition）}。正则性条件意味着，当且仅当
\(x_{\alpha} \subseteq \overline{x_{\beta}}\) 时，可以通过在索引集
\(P_{S}\) 中配备一个由\(\alpha\leq\beta\)给出的偏序结构(Poset
structure)来组合实现胞腔复形的拓扑信息，其中 \(\overline{x}\) 表示胞腔
\(x\) 的闭包（closure）。该偏序结构通常称为\emph{面偏序（face
poset）}(\citeproc{ref-hansen2019toward}{Hansen and Ghrist
2019})，它表明胞腔复形的拓扑信息编码完全由面偏序结构确定(\citeproc{ref-hansen2019toward}{Hansen
and Ghrist
2019})，这使得组合复形事实上可以通过偏序结构来组合表示(\citeproc{ref-aschbacher1996combinatorial}{Aschbacher
1996}; \citeproc{ref-klette2000cell}{Klette 2000};
\citeproc{ref-basak2010combinatorial}{Basak 2010};
\citeproc{ref-savoy2022combinatorial}{Savoy 2021})。

\begin{glossarybox}

\begin{quote}
\textbf{译者注}
\end{quote}

\begin{quote}
poset，偏序，Partial order set的简写
\end{quote}

\begin{quote}
closure，闭包
\end{quote}

\end{glossarybox}

定义@ref(def:rccmain)意味着胞腔复形内每个胞腔的边界胞腔也是胞腔复形中的胞腔，因此，可以把胞腔复形看作是不同维度的胞腔的集合，这些胞腔通过它们的边界相互关联。就关系而言，这意味着胞腔复形中胞腔的边界也必须是胞腔复形中的胞腔。虽然胞腔复形构成了高阶网络的一般类别，但这一属性对胞腔复形的关系设置了约束。在某些应用中，如果数据不满足这种约束，那么这种约束可能并不可取。为了消除对集合中实体间关系的所有限制，通常会考虑超图。

\phantomsection\label{hgmain}
非空集\(S\)上的\emph{超图}是\((S,\mathcal{X})\)，其中，\(\mathcal{X}\)是\(\mathcal{P}(S)\setminus\{\emptyset\}\)的子集，\(\mathcal{X}\)的元素被称作\emph{超图}。

基数为 2（cardinality two）
的超边称为\emph{边}。超图可以看作是单纯复形和胞腔复形的一般化。然而，超图并不直接包含胞腔（或关系）维度的概念，而胞腔复形的定义中明确包含了维度的概念，并且在单纯复形中关系的基数来表明维度的概念。正如我们在第@ref(motivation-for-ccs)节中所演示的，单纯复形和胞腔复形中胞腔和关系的维度可以用来赋予这些复形以层次结构，而层次结构可以用来在这些结构上进行（非）池化计算。

\subsection{层次化结构与集合型关系}\label{hierarchical-structure-and-set-type-relations}

正如第
@ref(bridging-the-gap-among-higher-order-networks)节所概述的那样，单纯复形、胞腔复形和超图的性质产生了高阶域上关系的两个主要特征，即关系的层次和集合型关系。在本小节中，我们将形式化这两个特征。

\phantomsection\label{rank}
高阶域\(\mathcal{X}\)上的\emph{秩函数}是一个保序函数（order-preserving
function）\(\mbox{rk}\colon \mathcal{X}\to \mathbb{Z}_{\ge 0}\)，例如，\(\forall x,y\in\mathcal{X}, x\subseteq y\rightarrow \mbox{rk}(x) \leq \mbox{rk}(y)\)
。

\begin{glossarybox}

\begin{quote}
\textbf{译者注} 原书中写作：\(x\subseteq  y\) implies
\(\mbox{rk}(x) \leq \mbox{rk}(y)\) for all \(x,y\in\mathcal{X}\)
\end{quote}

\end{glossarybox}

直观地说，在高阶域\(\mathcal{X}\)上的秩函数\(\mbox{rk}\)给\(\mathcal{X}\)中的每一个关系附加一个用非负整数值表示的秩，使得\(\mathcal{X}\)中的集合胞腔通过\(\mbox{rk}\)得以保全。实际上，秩函数在\(\mathcal{X}\)上诱导了一个\emph{层次结构}。胞腔和单纯复形是具有秩函数的高阶域的常见例子，因此具有层次化关系。。

\phantomsection\label{strelations}
如果高阶域中的一个关系的存在并不隐含于域中的另一个关系，那么这种关系就被称为\emph{集合型关系}。

超图可算作具有集合类型关系的高阶域的例子。鉴于单纯复形、胞腔复形和超图在建模上的局限性，我们在章节
@ref(combinatorial-complexes)
中提出了组合复形，这是一个同时具有层次关系和集合类型关系的高阶域。

\section*{(PART*)
第二部分:组合复形}\label{part-ux7b2cux4e8cux90e8ux5206ux7ec4ux5408ux590dux5f62}
\addcontentsline{toc}{section}{(PART*) 第二部分:组合复形}

\section{组合复形}\label{combinatorial-complexes}

在这一节，我们引入了组合复形（combinatorial complexes
，CCs），一类新的高阶域，它一般化了图、单纯复形、胞腔复形、超图等结构。图@ref(fig:hon)展示了CCs在这些域上一般化的第一个例子。表@ref(tab:complexsummary)列举了高阶域和图相关的关系特征,并概括了CCs对这种关系实现一般化的能力。

\begin{figure}

{\centering \includegraphics{figures/hon} 

}

\caption{演示CCs怎样一般化不同的域. (a):集合$S$由实体（顶点）抽象组成，但不包含关系(b): 图建模了顶点间的二元关系(例如，$S$的元素). (c): 单纯复形建模了高阶关系的层次化结构 (例如，关系间的关系) ，但对其关系的“形状”有严格的约束。 (d): 与单纯复形类似，胞腔复形建模了高阶关系的层次化结构，但是在关系的形状的表现上更灵活(例如，胞腔)。 (f):超图建模了$S$中元素间的任意集合型关系,但是这些关系不能反应层次化结构。(e): CC组合了胞腔复形（关系间的层次化结构）和超图（任何集合型关系）的特征,将两种域进行了一般化。}\label{fig:hon}
\end{figure}

\begin{longtable}[]{@{}lcccc@{}}
\caption{以表格形式总结拓扑域和图的关系的相关特征.
回想一下，关系是域的一个元素，域通过它的关系和这些关系相互关联的方式来指定的。与关系相关的理想特征在第一列中标出，关系的\emph{层次结构}意味着高阶域的关系可以有不同的等级，\emph{集合型关系}不受关系之间或关系长度的限制，\emph{多关系耦合}意味着每个关系都可以通过定义在高阶域上的多个邻域函数来拥有其他邻近关系，\emph{等级而非基数}表示在高阶域的给定层次中具有相同等级的关系不需要具有相同的基数。}\tabularnewline
\toprule\noalign{}
关系相关的特征 & CC & 超图 & 胞腔复形 & 单纯复形 \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
关系相关的特征 & CC & 超图 & 胞腔复形 & 单纯复形 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
层次化关系(Hierarcy of relations) & √ & & √ & √ \\
集合型关系(Set-type relations) & √ & √ & & \\
多关系耦合(Multi-relation coupling) & √ & & √ & √ \\
秩而非基数(Rank not cardinality) & √ & √ & √ & \\
\end{longtable}

章节@ref(cc-definition)
引入了组合复形的定义并提供了若干CCs的示例，章节@ref(cc-homomorphisms-and-sub-ccs)定义了组合复形同态（CC-homomorphism）的概念并给出了了相关示例，章节@ref(motivation-for-ccs)从实用角度介绍
CC
结构背后的动，章节@ref(neighbourhood-functions-on-ccs)展示了邻域矩阵上邻域函数的计算方法，章节@ref(data-on-ccs)引入了组合复形共链（CC-cochain）的概念。

\subsection{组合复形定义}\label{cc-definition}

我们希望定义一种结构，该结构能够填补单纯/胞腔复形和超图间的代沟，正如章节@ref(hierarchical-structure-and-set-type-relations)所描述的那样。为此，本章引入了组合复形（combinatorial
complex，CC），一种高阶域，可以从三个角度来看，作为一个单纯复形，其胞腔和单纯子可以缺失；作为一个具有松弛结构的广义胞腔复形；或者作为一个通过加入秩函数而丰富的超图。

\phantomsection\label{cc}
\emph{组合复形（combinatorial
complex，CC)}是一个三元组\((S,\mathcal{X},\mbox{rk})\),
\(S\)是集合，\(\mathcal{X}\)是\(\mathcal{P}(S)\setminus\{\emptyset\}\)的子集，\(\mbox{rk} \colon \mathcal{X}\to \mathbb{Z}_{\ge 0}\)是秩函数且满足如下属性:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\forall s\in S, \{s\}\in\mathcal{X}\);
\item
  函数\(\mbox{rk}\)是保序的（order-preserving），即：如果\(x,y\in \mathcal{X}\)满足\(x\subseteq  y\)，那么\(\mbox{rk}(x) \leq \mbox{rk}(y)\).
\end{enumerate}

\(S\)的元素称作\emph{实体（entities）}或\emph{顶点（vertices）},
\(\mathcal{X}\)的元素称作\emph{关系（relations）}或\emph{胞腔（cells）},
\(\mbox{rk}\)称作CC的\emph{秩函数（rank function）}。

简单起见,
\(\mathcal{X}\)被用作CC\((S,\mathcal{X},\mbox{rk})\)的速记符号。定义@ref(def:cc)建立了构建高阶网络的框架，在此基础上，我们可以定义通用的高阶深度学习架构。请注意，CC既表现出了层次关系，也表现出了集合类型关系。特别是，CC
的秩函数 \(\mbox{rk}\)给CC引入了关系的层次结构。此外，由于定义
@ref(def:cc)中没有关系约束，CCs也可以包含集合型关系。因此，CCs包含了胞腔复形和超图，因为它们结合了两者与关系相关的特征。表
@ref(tab:complexsummary)提供了 CCs
与常见的高阶网络和图之间关系相关特征的比较总结。

我们通常要求CC中的每个单一胞腔\(\{s\}\)的\(\mbox{rk}(\{s\})=0\)，这种约定使
CCs 自然地与单纯复形和胞腔复形（cellular complexes）相一致。

胞腔\(x\in\mathcal{X}\)的秩是\(x\)上秩函数\(\mbox{rk}\)的值\(\mbox{rk}(x)\)，CC\(\mathcal{X}\)的\emph{维度}\(\mbox{dim}(\mathcal{X})\)是
\(\mathcal{X}\)
中胞腔的最大秩。秩为\(k\)的胞腔称作\emph{\(k\)-cell}，记作\(x^k\)；CC
\(\mathcal{X}\)的\emph{\(k\)-骨架（skeleton）}记作\(\mathcal{X}^{(k)}\)，表示\(\mathcal{X}\)内秩最大为\(k\)的胞腔的集合；秩为\(k\)的胞腔集合记作\(\mathcal{X}^k\)，该集合对应于\(\mathcal{X}^k=\mbox{rk}^{-1}(\{k\})\)。\(1\)-cells称作\(\mathcal{X}\)的\emph{边界（edges）}。一般来讲，CC的边界可以包含不止两个节点，而边仅有两个节点的CCs称作\emph{基于图（graph-based）}的CCs。本文中，主要关注基于图的CCs。

\phantomsection\label{unnamed-chunk-3}
图@ref(fig:ccexother)给出了四个CCs示例。例如，图@ref(fig:ccexother)(a)是顶点集\(S=\{s_0,s_1,s_2\}\)上的2维CC，由\(0\)-cells
\(\{s_0\}\), \(\{s_1\}\)，\(\{s_2\}\)组成 (用橙色表示)，\(1\)-cell
\(\{s_0, s_1\}\) (紫色)，\(2\)-cell \(\{s_0, s_1, s_2\} = S\) (蓝色).

\begin{figure}

{\centering \includegraphics{figures/cc_examples} 

}

\caption{CCs示例，橙色圆圈代表顶点，粉色、蓝色和绿色分别代表秩为1、2和3的胞腔，(a)、(b) 和 (d) 中的每个 CC 的维度都等于 2，而 (c) 中的 CC 的维度等于 3。}\label{fig:ccexother}
\end{figure}

\subsection{CC同态和子CCs}\label{cc-homomorphisms-and-sub-ccs}

CC 同态是将 CC 互相联系起来的映射，CC 同态在描述将图或其他高阶域提升为
CC
的过程中发挥着重要作用。直观地说，提升映射是一个定义明确的过程，它将特定类型的域（如图）转换为另一种类型的域（如
CC）。提升映射的用处在于，它能将定义在 CC
上的深度学习模型的应用实现在更常见的域上，如图、胞腔复形或单纯复形。我们将在附录
@ref(lifting-maps)中详细研究提升映射，并提供相关示例。定义
@ref(def:maps)正式提出了CC同态的概念。

\phantomsection\label{maps}
从CC
\((S_1, \mathcal{X}_1, \mbox{rk}_1)\)到另一个CC\((S_2, \mathcal{X}_2, \mbox{rk}_2)\)的同态，也称作\emph{CC-同态}，是一个映射函数
\(f \colon \mathcal{X}_1 \to \mathcal{X}_2\)，该函数需满足如下属性：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  如果 \(x,y\in\mathcal{X}_1\) 满足 \(x\subseteq y\), 那么
  \(f(x) \subseteq f(y)\).
\item
  如果
  \(x\in\mathcal{X}_1\)，那么\(\mbox{rk}_1(x)\geq \mbox{rk}_2(f(x))\).
\end{enumerate}

定义@ref(def:maps)的条件2确保CC同态仅映射\(\mathcal{X}_1\)中的\(k\)-cell到\(\mathcal{X}_2\)中秩不大于\(k\)的胞腔。如果\(\forall x \in \mathcal{X}_1, \mbox{rk}_1(x) = \mbox{rk}_2(f(x))\)
并且
\(f\)是单射（injective），那么称同态映射\(f\)是\emph{CC-嵌入（CC-embedding）}。CC
嵌入在实践中非常有用，因为它们可以通过用高阶胞腔来增强域（例如图结构），从而将该域
``提升（lifting）''为 CC。示例 @ref(exm:augmentation)
给出了三种CC嵌入，而示例@ref(exm:homomorphism)给出的CC同态不是CC嵌入。

\phantomsection\label{augmentation}
图@ref(fig:non-embedd-hom)(a) 和
(b)展示了两种CC嵌入。图@ref(fig:non-embedd-hom)(a)中，左侧图形中的每个胞腔都会被发送到右侧
CC
中的相应胞腔。同样，在图@ref(fig:non-embedd-hom)(b)中，左边胞腔复形中的每个胞腔都被传送到右边
CC 中的对应胞腔。很容易验证这两个映射都是 CC 嵌入。

\begin{figure}

{\centering \includegraphics{figures/cc_map} 

}

\caption{CC同态示例. 粉色、蓝色和绿色分别代表秩为1、2、3胞腔。(a): 将 1 维 CC 嵌入 2 维 CC。(b)：一个 2 维的 CC 嵌入一个 3 维的 CC。(c) CC 同态的一个例子。左边CC $\mathcal{X}_1$，右边是$\mathcal{X}_2$，同态映射$f$用黑箭头表示。直观地说，CC同态映射 $f$ 可以类似看作是 $S_1 =\{1, 2, 3、 4\}$ 和 $S_2 =\{a, b, c\}$ 之间连续函数的组合（combinatorial analogue），该连续函数*塌缩（collapses）*胞腔$\{1,2,3\}$到胞腔$\{a,b\}$，胞腔$\{1,2,3,4\}$到胞腔$\{a,b,c\}$。从这个角度观察，CC 同态推广了单纯映射[@munkres2018elements]。}\label{fig:non-embedd-hom}
\end{figure}

\begin{glossarybox}

\begin{quote}
\textbf{译者注} 单纯映射(simplicial
map)是联系复形的多面体之间的一类重要映射,它是从复形K的多面体\textbar K\textbar 到复形L的多面体\textbar L\textbar 的连续映射.
\end{quote}

\end{glossarybox}

\phantomsection\label{homomorphism}
这里给出一个非CC嵌入的CC同态例子。考虑集合\(S_1 = \{1, 2, 3,4\}\)和\(S_2 = \{a, b, c\}\)，令\(\mathcal{X}_1\)表示由与\(S_1\)中元素对应的1个3-cell\(\{1,2,3,4\}\)、1个2-cell\(\{1,2,3\}\)和4个0-cells组成的\(S_1\)上的CC。同样，令\(\mathcal{X}_2\)表示由与\(S_2\)中元素对应的1个3-cell\(\{a,b,c\}\)、1个2-cell
\(\{a,b\}\)和3个0-cells。图@ref(fig:non-embedd-hom)(c)可视化了CCs\(\mathcal{X}_1\)
和
\(\mathcal{X}_2\)。设有函数\(f \colon S_1 \to S_2\)定义为\(f(1) = f(2) = a,~f(3) = b\)和\(f(4) = c\)，很容易验证\(f\)诱导了从\(\mathcal{X}_1\)
到 \(\mathcal{X}_2\)的CC同态。

\phantomsection\label{sub-cc}
令\((S,\mathcal{X}, \mbox{rk})\)是一个CC，CC
\((S,\mathcal{X}, \mbox{rk})\)的\emph{子组合复形（sub-combinatorial
complex，sub-CC）} 是 CC \((A,\mathcal{Y},\mbox{rk}^{\prime})\)，使得
\(A\subseteq S\), \(\mathcal{Y}\subseteq\mathcal{X}\) 且
\(\mbox{rk}^{\prime} = \mbox{rk}|_{\mathcal{Y}}\)是\(\mbox{rk}\)在\(\mathcal{Y}\)上的约束（restriction）.

简单起见，本文把子CC
\((A,\mathcal{Y},\mbox{rk}^{\prime})\)记作\(\mathcal{Y}\)，任何\(A \subseteq S\)的子集都能用于按下述方式诱导子CC。考虑带有约束
\(\mbox{rk}|_{\mathcal{X}_A}\)的集合\(\mathcal{X}_A = \{x \in \mathcal{X}  \mid x \subseteq A\}\)
，很容易看到三角\((A,\mathcal{X}_A,\mbox{rk}|_{\mathcal{X}_A})\)
构成了一个CC，称之为由\(A\)诱导的\(\mathcal{X}\)的子CC。注意， 集合
\(\mathcal{X}\) 中的任何一个胞腔都会诱导出一个子 CC，这个子 CC
是通过考虑包含在其中的所有胞腔而得到的。最后，对于任意
\(k\)，我们不难发现，CC \(\mathcal{X}^{k}\) 的骨架 \(\mathcal{X}\)
是一个子 CC。

\phantomsection\label{sub-cc-ex}
回忆一下图 @ref(fig:ccexother)(a)中给出的CC
\(\mathcal{X}= \{\{s_0\}, \{s_1\}, \{s_2\}, \{s_0, s_1\}, \{s_0, s_1, s_2\}\}\)，集合\(A = \{s_0, s_1\}\)诱导了\(\mathcal{X}\)的子CC\(\mathcal{X}_A = \{\{s_0\}, \{s_1\}, \{s_0, s_1\}\}\)

\subsection{引入CCs的动机}\label{motivation-for-ccs}

CCs的定义@ref(def:cc)旨在实现高阶建模的所有目标，正如章节@ref(motivation)所概述的那样。为了进一步说明定义
@ref(def:cc)的动机，我们考虑了 CCs 的池化操作以及
CCs在结构上的一些优势。

\subsubsection{CCs上的池化操作}\label{pooling-operations-on-ccs}

我们首先考虑图上池化的一般特征，然后演示如何通过 CCs
以统一的方式实现基于图的池化。图 \(\mathcal{G}\) 上的通用池化函数是
\(\mathcal{POOL} \colon \mathcal{G} \to \mathcal{G}^{\prime }\)，其中
\(\mathcal{G}^{\prime}\) 是代表 \(\mathcal{G}\)的粗粒度版本的池化图。
\(\mathcal{G}^{\prime}\)的顶点对应于原图\(\mathcal{G}\)中的顶点簇（超级顶点），而\(\mathcal{G}^{\prime}\)中的边表示这些顶点簇之间是否存在连接。参见图
@ref(fig:pooling-motivation)。

\begin{figure}

{\centering \includegraphics{figures/pooling_motivation} 

}

\caption{通过 CCs 实现的基于图的池化操作。(a): 在这里，图的池化操作用池化函数来表示，该函数将图及其上定义的数据映射到该图的粗粒度版本上。右侧池化图中的超级顶点（super-vertices）与左侧原图中的顶点簇相对应，而右侧图中的边则表示这些簇之间是否存在连接关系。 (b-d): 右侧池化图中的超级顶点可以通过从原图中得到的 CC 中的高阶增强胞腔（蓝色胞腔）来实现，右侧池化图中的边可以通过顶点与高阶增强胞腔之间的 CC的 关联矩阵来实现。}\label{fig:pooling-motivation}
\end{figure}

基于CCs提出的一些形式化描述将把基于图的池化操作作为特例来描述。尤其，在\(\mathcal{G}^{\prime}\)的顶点上定义的超级顶点（\(\mathcal{G}\)上的顶点簇），通过将超级顶点表示为胞腔来增强\(\mathcal{G}\)，从而可以实现为CC上的高秩胞腔（higher-order
ranked
cells）。这种层次化结构由原图\(\mathcal{G}\)，还有带高秩胞腔增强的CC一起组成。这种以基于CC的高秩胞腔为基础实现的图上的池化操作是层次化的，因为新的胞腔可以以递归的方式组合在一起，从而获得底层空间的粗粒度版本。有了这种对
CCs 进行高阶池化操作的概念，我们在 @ref(graph-pooling-as-cc-pooling)和
@ref(image-pooling-as-cc-pooling)两节中分别演示了如何表达图/网格上的池化操作和基于图像的池化操作。
从这个角度来看，CCs提供了在图、图像等高阶网络上定义池化操作的通用框架。

此外，还通过高阶池化操作探讨了 CCs
的两个主要特点。首先，CCs模型化集合型胞腔的能力为在顶点簇的形状上定义池化操作提供了灵活性，这种灵活性是实现自定义池化操作所必需的，在这种操作中，簇的形状可能是特定于任务的。其次，CC上的高秩胞腔（higher-order
ranked，
cells）对应于底层空间的粗粒度版本，这里提出的高阶池化可用于构建更粗粒度的表征，参见图@ref(fig:pooling-motivation)。需要注意的是，超图和胞腔复形等通用性较低的结构无法同时灵活地进行簇整形（
cluster shaping）和生成更粗粒度的底层空间表示。

\subsubsection{CCs的结构化优势}\label{structural-advantages-of-ccs}

除了实现基于图的池化操作，CCs还提供了多种结构优势。尤其，具体来说，CCs统一了众多常用的高阶网络，使得可以对拓扑特性进行细粒度分析，促进深度学习中拓扑特征的消息传递，并能灵活建模关系之间的关系。

\textbf{灵活的高阶结构和细粒度消息传递}。消息传递图模型在图的顶点之间传递消息，以学习图的表示，消息是根据顶点和边的特征计算出来的。这些消息更新顶点和边的特征，并从图的局部邻域收集消息。与其他高阶网络和图相比，CCs
的秩函数在两个方面更具通用性。首先，秩函数使得CCs在高阶结构表示方面更具灵活性；其次，在深度学习场景下，秩函数为CC提供了更细粒度的信息传递能力。例如，超图中的每个超边都被视为一个集合，没有秩的概念，因此所有超边都被统一处理，没有任何区别。更多详情请参见章节@ref(combinatorial-complex-neural-networks)。

\textbf{在关系间灵活的建模关系}，在用拓扑数据填充拓扑域的过程中，由于域中所有胞腔都缺乏可自然支持的数据，因此构建有意义的关系可能具有挑战性，在处理单纯复形或胞腔复形时尤其如此。例如，一个单纯复形的
\(k\) 个实体之间的任何关系都必须从 \(k-1\)
个实体的所有相应子集的关系中建立。现实世界的数据可能包含这些关系的一个子集，而不是全部。虽然胞腔复形在关系建模方面提供了更大的灵活性，但胞腔复形必须满足的边界条件限制了允许的关系类型。为了消除关系之间的所有限制，超图可以提供帮助，因为超图允许任意的集合类型关系。不过，超图不提供层次特征，这在需要同时考虑局部和全局特征的应用中可能会处于劣势。

\subsection{CCs上的邻域函数}\label{neighbourhood-functions-on-ccs}

我们在 CC 上引入了 CC
邻域函数的概念作为一种利用CC中拓扑信息的机制。在实践中，精心设计邻域函数通常是学习任务的一部分。出于我们的目的，我们只讨论两类广义邻域函数，即指定邻接和关联的函数(adjacency
and incidence)。从深度学习的角度来看，
从深度学习的角度看，CC-邻域函数奠定了扩展深度学习模型的一般性消息传递方案的基础，从而涵盖了几种最先进的
GNNs (\citeproc{ref-battaglia2016interaction}{P. Battaglia et al. 2016};
\citeproc{ref-kipf2016semi}{Kipf and Welling 2016};
\citeproc{ref-battaglia2018relational}{P. W. Battaglia et al. 2018};
\citeproc{ref-fey2019fast}{Fey and Lenssen 2019};
\citeproc{ref-loukas2019graph}{Loukas 2019};
\citeproc{ref-morris2019weisfeiler}{Morris et al. 2019}).

给定一个 CC，我们的目标是描述该 CC 的子 CC
的局部邻近的胞腔。为此，我们定义了 CC 邻近函数，该函数与CC
s上下文环境中的定义 @ref(def:ns)类似。

\phantomsection\label{cc-neighborhood-function}
CC
\((S,\mathcal{X}, \mbox{rk})\)上的\emph{CC邻域函数}记作\(\mathcal{N}\)，该函数可为CC的每个子CC
\((A,\mathcal{Y},\mbox{rk}^{\prime})\)
分配一个\(S\)子集的非空集合\(\mathcal{N}(\mathcal{Y})\)。

不失一般性，我们假定邻域\(\mathcal{N}(\mathcal{Y})\)的元素是\(\mathcal{X}\)的胞腔或子CCs。直观上来讲，sub-CC
\(\mathcal{Y}\)的邻域\(\mathcal{N}(\mathcal{Y})\)是\(S\)子集的集合，他们位于\(\mathcal{Y}\)的局部邻近地区（local
vicinity）。术语``局部邻近地区''是一般性描述，通常由具体情况而定。

定义
@ref(def:cc-neighborhood-function)是定义@ref(def:ns)的离散化版本，两个定义的相对应可参见图@ref(fig:cc-neighborhood)。在本文的其余部分，CC
邻域函数被简地称为邻域函数。实际上，CC
邻域函数中编码的信息是用矩阵来表示的，如下所述。

\begin{figure}

{\centering \includegraphics{figures/cts_discrete_nbhd} 

}

\caption{连续域邻域函数与 CC 邻域函数的可视化比较. (a): 具有连续域 $S$ 的邻域函数为 $x\in S$ 分配了一组$S$ 的子集 $\mathcal{N}(x)$ ，这些子集位于 $x$ 的局部邻近地区。 (b): 类似的，CC $(S,\mathcal{X}, \mbox{rk})$上的CC邻域函数分配给$x\in S$一组$\mathcal{X}$的子集$\mathcal{N}(x)$，它们也在 $x$的局部邻近地区。}\label{fig:cc-neighborhood}
\end{figure}

\textbf{邻域函数诱导的邻域矩阵}。
为了便于计算，可以用矩阵来表示邻域函数。关联矩阵、邻接矩阵和共邻接矩阵是编码各类邻域函数的常见矩阵形式。在定义
@ref(def:neighborhood-matrix)中，我们引入了这些矩阵的广义化，称为
``邻域矩阵''。在这个定义中以及以后，我们用 \(|S|\) 表示集合 \(S\)
的基数（cardinality）。

\phantomsection\label{neighborhood-matrix}
令\(\mathcal{N}\)是定义在 CC
\(\mathcal{X}\)上的邻域函数，令\(\mathcal{Y}=\{y_1,\ldots,y_n\}\) 和
\(\mathcal{Z}=\{z_1,\ldots,z_m\}\) 是
\(\mathcal{X}\)上两个胞腔的集合，使得\(\forall 1\leq i \leq n，\mathcal{N}(y_{i}) \subseteq \mathcal{Z}\)。则，\emph{\(\mathcal{N}\)上关于\(\mathcal{Y}\)和\(\mathcal{Z}\)的邻域矩阵}是\(|\mathcal{Z}| \times|\mathcal{Y}|\)二值矩阵(binary
matrix)\(G\)，如果\(z_i\in \mathcal{N}(y_j)\)，则其第\((i,j)\)个项\([G]_{ij}\)的值为\(1\)，否则为\(0\)。

在定义@ref(def:neighborhood-matrix)中，\(\mathcal{N}(y_j)\)存储在邻域矩阵\(G\)的第\(j-th\)列。因此，当我们使用邻域矩阵
\(G\) 时，我们用 \(\mathcal{N}_{G}(j)\) 表示胞腔 \(y_j\) 的邻域函数。

在 CC
上定义有用的邻接函数有很多种方法。在这项工作中，我们仅限于最直接的邻域函数：关联邻域函数和邻接邻域函数。

\subsubsection{CC中的关联关系（Incidence）}\label{incidence-in-a-cc}

我们定义了关联的三种术语，以捕捉 CC
中胞腔关联结构的不同方面。首先，在定义@ref(def:nstruct)种，我们引入了下关联（down-incidence）和上关联（up-incidence）邻域函数来描述，通过任意秩的胞腔来描述胞腔的关联结构。其次，在定义@ref(def:filtered-incidence)，引入了\(k-down\)下关联和\(k-up\)上关联邻域函数，通过特定秩\(k\)的胞腔来描述胞腔的关联结构。然后，在定义@ref(def:inc-mat)，引入\((r, k)\)-incidence关联矩阵，以描述特定秩\(r\)和\(k\)的胞腔的关联结构。在下文中，我们假定
CC\((S,\mathcal{X}, \mbox{rk})\)的\(\mathcal{X}\)
集合中的胞腔有固定的顺序。

\phantomsection\label{nstruct}
令\((S,\mathcal{X}, \mbox{rk})\)是一个CC，CC的两个胞腔\(x, y\in\mathcal{X}\)被称作关联的（
\emph{incident}），如果\(x \subsetneq y\)或\(y \subsetneq x\)。尤其，胞腔\(x\in\mathcal{X}\)的\emph{下关联邻域函数（down-incidence
neighborhood function）} \(\mathcal{N}_{\searrow}(x)\)
被定义为集合\(\{ y\in \mathcal{X} \mid y \subsetneq x\}\)，而\(x\)的\emph{上关联邻域函数（up-incidence
neighborhood function）} \(\mathcal{N}_{\nearrow}(x)\) 被定义为集合
\(\{ y\in \mathcal{X} \mid x \subsetneq y\}\).

定义@ref(def:filtered-incidence)

与定义 @ref(def:nstruct)相比，定义
@ref(def:filtered-incidence)提供了更精细的关联结构规定。具体来说，定义
@ref(def:filtered-incidence) 和 @ref(def:nstruct)
分别描述了一个胞腔相对于特定秩或任意秩胞腔的关联结构。

\phantomsection\label{filtered-incidence}
令\((S,\mathcal{X}, \mbox{rk})\) 是一个CC，对于任何\(k\in\mathbb{N}\),
胞腔\(x\)的\emph{\(k-down\)下关联邻域函数}
\(\mathcal{N}_{\searrow,k}(x)\)被定义为集合\(\{ y\in \mathcal{X} \mid y \subsetneq x, \mbox{rk}(y)=\mbox{rk}(x)-k \}\)。胞腔\(x\)的\emph{\(k-up\)上关联邻域函数}
\(\mathcal{N}_{\nearrow,k}(x)\)
被定义为集合\(\{ y\in \mathcal{X} \mid y \subsetneq x,\mbox{rk}(y)=\mbox{rk}(x)+k \}\)。

显然，\(\mathcal{N}_{\searrow}(x)= \bigcup_{k\in \mathbb{N}} \mathcal{N}_{\searrow,k}(x)\)，并且\(\mathcal{N}_{\nearrow}(x)= \bigcup_{k\in\mathbb{N}} \mathcal{N}_{\nearrow,k}(x)\)。直接关联（immediate
incidence）尤其重要，为此，胞腔\(x \in \mathcal{X}\)的\emph{面（faces）}集合定义为\(\mathcal{N}_{\searrow,1} (x)\)，并且\emph{共面（cofaces）}集合定义为\(\mathcal{N}_{\nearrow,1} (x)\)。参见图@ref(fig:structure-inc)，给出了\(k\)-down和\(k\)-up关联邻域函数的描述。

\phantomsection\label{inc-mat}
令\((S,\mathcal{X}, \mbox{rk})\)是CC，对于任何\(0\leq r<k \leq \dim(\mathcal{X})\)下的\(r,k \in \mathbb{Z}{\ge 0}\)，
\(\mathcal{X}^{r}\) 和 \(\mathcal{X}^{k}\)之间的\emph{\((r,k)\)关联矩阵}
\(B_{r,k}\)被定义为
\(|\mathcal{X}^r| \times |\mathcal{X}^k|\)二值矩阵，其中，如果\(x^r_i\)关联于\(x^k_j\)，那么第\((i, j)\)-th项\([B_{r,k}]_{ij}\)等于1，否则等于0。

\begin{figure}

{\centering \includegraphics{figures/structure_inc} 

}

\caption{维度为3的CC上的$k$-down 和 $k$-up 关联邻域函数。 (a): $k$-down 关联邻域函数， 目标橙色胞腔 $x$ 的秩为3。从左到右，红色胞腔标记了$\mathcal{N}_{\searrow,1}(x)$, $\mathcal{N}_{\searrow,2}(x)$ 和 $\mathcal{N}_{\searrow,3}(x)$. (b): $k$-up 关联邻域函数，目标橙色胞腔$x$秩为0，从左到右，红色胞腔标记了$\mathcal{N}_{\nearrow,1}(x)$, $\mathcal{N}_{\nearrow,2}(x)$ and $\mathcal{N}_{\nearrow,3}(x)$.}\label{fig:structure-inc}
\end{figure}

定义@ref(def:inc-mat)种的关联矩阵\(B_{r,k}\)规定了CC的邻域函数，关联矩阵诱导的邻域函数可用于构造CCs上的高阶消息传递实现方案，正如章节@ref(combinatorial-complex-neural-networks)所描述的。

\subsubsection{CC内的邻接关系（Adjacency）}\label{ccux5185ux7684ux90bbux63a5ux5173ux7cfbadjacency}

CCs 既有关联函数，也有其他邻域函数。例如，对于简化为图的
CC，更自然的邻域函数是基于邻接关系的概念。关联关系定义了不同秩的胞腔之间的关系，而邻接关系定义了相同秩的胞腔之间的关系。定义
@ref(def:coa-nf)、 @ref(def:k-coa-nf)和
@ref(def:adj-mat)/@ref(def:coadj-mat)引入了各自与关联关系相关的定义
@ref(def:nstruct)、 @ref(def:filtered-incidence)和
@ref(def:inc-mat)类似的（共）邻接关系。

\phantomsection\label{coa-nf}
令\((S,\mathcal{X}, \mbox{rk})\)是CC，胞腔\(x\in \mathcal{X}\)的\emph{邻接邻域函数（adjacency
neighborhood function）} \(\mathcal{N}_{a}(x)\)定义为集合
\begin{equation*}
\{  y \in \mathcal{X} \mid \mbox{rk}(y)=\mbox{rk}(x), \exists z \in \mathcal{X} \text{ with } \mbox{rk}(z)>\mbox{rk}(x) \text{ such that } x,y\subsetneq z\}.
\end{equation*} \(x\)的\emph{共邻接邻域函数（coadjacency neighborhood
function）} \(\mathcal{N}_{co}(x)\) 定义为集合 \begin{equation*}
\{ y \in \mathcal{X} \mid \mbox{rk}(y)=\mbox{rk}(x), \exists z \in \mathcal{X}
\text{ with } \mbox{rk}(z)<\mbox{rk}(x) \text{ such that } z\subsetneq y\text{ and }z\subsetneq x \}.
\end{equation*} 满足条件\(\mathcal{N}_{a}(x)\) 或
\(\mathcal{N}_{co}(x)\)的胞腔\(z\)称为\emph{桥接胞腔（bridge cell）}.

\phantomsection\label{k-coa-nf}
令\((S, \mathcal{X}, \mbox{rk})\)是CC，对于任何\(k\in\mathbb{N}\),
胞腔\(x \in \mathcal{X}\)的\emph{\(k\)-邻接邻域函数}
\(\mathcal{N}_{a,k}(x)\)定义为集合

\begin{equation*}
\{ y \in \mathcal{X} \mid \mbox{rk}(y)=\mbox{rk}(x),
\exists z \in \mathcal{X}
\text{ with } \mbox{rk}(z)=\mbox{rk}(x)+k \text{ such that } x,y\subsetneq z \}.
\end{equation*}

\(x\)的\emph{\(k\)-共邻邻域函数} \(\mathcal{N}_{co,k}(x)\)定义为集合

\begin{equation*}
\{ y \in \mathcal{X} \mid \mbox{rk}(y)=\mbox{rk}(x),
\exists z \in \mathcal{X}
\text{ with } \mbox{rk}(z)=\mbox{rk}(x)-k
\text{ such that } z\subsetneq y\text{ and }z\subsetneq x \}.
\end{equation*}

\phantomsection\label{adj-mat}
对于满足\(0\leq r<r+k \leq \dim(\mathcal{X})\)的任何\(r\in\mathbb{Z}_{\ge 0}\)
and \(k\in \mathbb{Z}_{>0}\) ， \(\mathcal{X}^{r}\)关于
\(\mathcal{X}^{k}\)的胞腔之间的 \emph{\((r,k)\)-邻接矩阵}
\(A_{r,k}\)被定义为\(|\mathcal{X}^r| \times |\mathcal{X}^r|\)
二值矩阵（binary matrix），该矩阵的第\((i, j)\)-th项，如果\(x^r_i\)
\(k\)-邻接于 \(x^r_j\)，那么\([A_{r,k}]_{ij}\)等于1，否则等于0。

\phantomsection\label{coadj-mat}
对于满足\(0\leq r-k<r \leq \dim(\mathcal{X})\)的任何\(r\in \mathbb{Z}_{\ge 0}\)
and \(k\in\mathbb{N}\) ，
\(\mathcal{X}^{r}\)关于\(\mathcal{X}^{k}\)的胞腔之间的\emph{\((r,k)\)-共邻接矩阵}
\(coA_{r,k}\) among the cells of \(\mathcal{X}^{r}\) 定义为
\(|\mathcal{X}^r| \times |\mathcal{X}^r|\)二值矩阵，如果\(x^r_i\)
\(k\)-共邻于\(x^r_j\)
，那么该矩阵的\((i, j)\)-th项\([coA_{r,k}]_{ij}\)等于1，否则等于0。

显然,
\(\mathcal{N}_{a}(x)= \cup_{k\in\mathbb{N}} \mathcal{N}_{a,k}(x)\)，并且
\(\mathcal{N}_{co}(x)= \cup_{k\in\mathbb{N}} \mathcal{N}_{co,k}(x)\)。图@ref(fig:structure-adj)给出了\(k\)-邻接和\(k\)-共邻邻域函数的描述。

\begin{figure}

{\centering \includegraphics{figures/structure_adj} 

}

\caption{维度为3的域CC的$k$-(共)邻接邻域函数。(a): $k$-邻接邻域函数。目标橙色胞腔$x$ 秩为0，从左到右，红色胞腔标记了$\mathcal{N}_{a,1}(x)$, $\mathcal{N}_{a,2}(x)$ 和 $\mathcal{N}_{a,3}(x)$。 (b): $k$-共邻邻域函数，目标橙色胞腔$x$秩为2，从左到右，红色胞腔标记了$\mathcal{N}_{co,1}(x)$, $\mathcal{N}_{co,2}(x)$和$\mathcal{N}_{co,3}(x)$.}\label{fig:structure-adj}
\end{figure}

\subsection{CCs上的数据}\label{data-on-ccs}

由于我们对处理定义在 CC \((S,\mathcal{X},\mbox{rk})\)
上的数据感兴趣，因此我们引入了 \(k\)-共链空间（\(k\)-cochain
spaces）、\(k\)-共链（\(k\)-cochains）和共链映射(cochain maps)等概念。

\phantomsection\label{k-cochain-spaces}
令\(\mathcal{C}^k(\mathcal{X},\mathbb{R}^d )\)是秩为\(k \in \mathbb{Z}_{\ge 0}\)且维度为\(d\)的函数\(\mathbf{H}_k\colon\mathcal{X}^k\to \mathbb{R}^d\)的\(\mathbb{R}\)-向量空间（vector
space）。\(d\) 称作\emph{数据维度（data
dimension）}，\(\mathcal{C}^k(\mathcal{X},\mathbb{R}^d)\)称作\emph{\(k\)-共链空间（cochain
space）}，\(\mathcal{C}^k(\mathcal{X},\mathbb{R}^d)\)中的元素\(\mathbf{H}_k\)称作\emph{\(k\)-共链（cochains）}
或 \emph{\(k\)-信号（signals）}。

当底层CC很明确的时候，可以直接使用\(\mathcal{C}^k(\mathcal{X})\) 或
\(\mathcal{C}^k\)来简略表示。此外，还可以说\(k\)-共链空间\(\mathcal{C}^k(\mathcal{X})\)被定义在\(\mathcal{X}\)上。直观来讲，\(k\)-cochain可被解释为定义在\(\mathcal{X}\)的\(k\)-cells上的信号(\citeproc{ref-grady2010discrete}{Grady
and Polimeni
2010})。图@ref(fig:cc-cochain)(a)给出了单纯复形的0、1、2秩上支持的共链。

\begin{figure}

{\centering \includegraphics{figures/mapping_cochain} 

}

\caption{CC在维度4上支持的$k$-cochains (左图) 和共链映射（ cochain maps，右图) 。左图： $k$-cochain可以被解释为定义在$k$-cells上的信号或特征向量（feature vector）。图中，维度3的共链附在顶点上，维度2的共链附在1-cells上，维度4的共链附在2-cells上。右图：$coA_{r,k}$ 和 $A_{r,k}$ 定义了等维共链空间之间的共链映射（cochain map），$B_{r,k}$定义了不同维度空间之间的共链映射。}\label{fig:cc-cochain}
\end{figure}

当\(\mathcal{X}\)是图时，\(0\)-cochains对应于图上的信号(\citeproc{ref-ortega2018graph}{Ortega
et al.
2018})。通过对\(\mathcal{X}^k\)中的胞腔排序，我们可以用欧式向量空间\(\mathbb{R}^{ |\mathcal{X}^k| \times d}\)
规范地识别出\(\mathcal{C}^k(\mathcal{X},\mathbb{R}^d )\)
，并且直接将\(\mathbf{H}_k\)写作向量\([ \mathbf{h}_{x^k_1},\ldots,\mathbf{h}_{x^k_{|\mathcal{X}^k|} }]\)。其中，\(\mathbf{h}_{x^k_j} \in \mathbb{R}^d\)是与胞腔\(x^k_j\)相关的特征向量。符号\(\mathbf{H}_{k,j}\)指特征向量\(\mathbf{h}_{x^k_j}\)
，以避免明确引用胞腔\(x^k_j\)。我们也可以处理共链空间之间的映射，他们被称为共链映射。

\phantomsection\label{cochain-maps}
对于\(r< k\), 关联矩阵\(B_{r,k}\) 产生映射 \begin{align*}
B_{r,k}\colon \mathcal{C}^k(\mathcal{X}) &\to   \mathcal{C}^r(\mathcal{X}),\\
\mathbf{H}_k &\to  B_{r,k}(\mathbf{H}_k),
\end{align*} 其中，\(B_{r,k}(\mathbf{H}_k)\)
表示矩阵\(B_{r,k}\)和向量\(\mathbf{H}_k\)的通常意义上的乘积\(B_{r,k}\mathbf{H}_k\)。类似的，\((r,k)\)-邻接矩阵\(A_{r,k}\)产生映射
\begin{align*}
A_{r,k}\colon \mathcal{C}^r(\mathcal{X}) &\to   \mathcal{C}^r(\mathcal{X}),\\
\mathbf{H}_r &\to  A_{r,k}(\mathbf{H}_r).
\end{align*} 共链空间之间的这两类映射都称为 \emph{共链映射（cochain
maps）}。

共链映射可以作为算子来用，例如在底层CC上的``变换（shuffle）''、``再分布（redistribute）''数据等操作。在TDL场景下，共链映射是定义高阶消息传递(Section
@ref(definition-of-higher-order-message-passing))和反池化/池化操作(Section
@ref(cc-pooling-and-unpooling))的主要工具。每个邻接矩阵\(A_{r,k}\)或共邻接矩阵\(coA_{r,k}\)都定义了等维共链空间之间的共链映射，而每个关联矩阵\(B_{r,k}\)都定义了不同维度空间之间的共链映射。参见图@ref(fig:cc-cochain)(b)
给出的维度4下的CC的共链映射示例。

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{组合复形神经网络（Combinatorial complex neural
networks）}\label{combinatorial-complex-neural-networks}

CCs建模的灵活性使得可以研究和分析多种基于CC构建的神经网络架构，基于 CC
的神经网络可以利用所有邻域矩阵或其中的一个子集，从而考察 CC
中各个胞腔之间的多向交互作用，以解决学习任务。在本节中，我们将通过制定基于
CC 的 TDL 模型的一般原则来介绍 TDL 的蓝图。我们将利用我们的 TDL
蓝图框架来研究当前的方法，并为设计新型模型提供指导。

TDL中的学习任务可以粗略分成三类：胞腔分类、复形分类、胞腔预测，参见图@ref(fig:tdl-tasks)。我们在章节@ref(implementation-and-numerical-results)的实验提供了关于胞腔和复分类的示例，三类学习任务的具体细节如下：

\begin{itemize}
\tightlist
\item
  \emph{胞腔分类（Cell
  classification）}：预测胞腔复形中每个胞腔。为了做到这一点，我们可以利用
  TDL分类器，将目标胞腔的拓扑邻域及其相关特征考虑在内。胞腔分类的一个例子是三角形网格分割（triangular
  mesh segmentation），其中的任务是预测给定网格中每个面或边的类别。
\item
  \emph{复形分类（Complex classification）}:
  预测整个复形。为了实现这一目标，我们可以使用高阶胞腔将复形的拓扑结构还原成一个常见的表示形式，例如池化方法，然后在得到的平面向量（flat
  vector）上学习 TDL
  分类器。复形分类的一个例子是对每个输入网格进行类别预测。
\item
  \emph{胞腔预测（Cell prediction）}:
  预测胞腔复形中胞腔-胞腔相互作用的特征，在某些情况下，预测胞腔复形中是否存在某个胞腔。这可以通过利用胞腔的拓扑结构和相关特征来实现。一个相关的例子是预测超图的超边中实体之间的联系。
\end{itemize}

\begin{figure}

{\centering \includegraphics{figures/tasks} 

}

\caption{拓扑空间上的学习可以粗略地分为三类任务 (1) *胞腔分类（Cell classification）*: 预测胞腔复形中单个胞腔。 这种任务的一个例子是网格分割，拓扑神经网络会为输入网格中的每个面输出一个分割标签。 (2) *复形分类（Complex classification）*: 预测整个复形，涉及到将拓扑结构还原为一种通用的表示方法。输入网格的类别预测就是这一任务的一个例子。 (3) *胞腔预测（Cell prediction）*: 通过利用底层胞腔复形的拓扑结构和相关特征，预测胞腔相互作用的属性，有时包括预测胞腔的存在。这种任务的一个例子是预测超图超桥中的联系。}\label{fig:tdl-tasks}
\end{figure}

图@ref(fig:tdl)概述了TDL的一般设置。首先，在集合 \(S\) 上构建一个由 CC
表示的高阶域。然后选择一组定义在该域上的邻域函数。邻域函数通常根据当前的学习问题来选择，并用于构建拓扑神经网络。为了开发我们的通用
TDL 框架，我们引入了\emph{组合复形神经网络（combinatorial complex neural
networks，CCNN）}，这是一类由 CC
支持的抽象神经网络，它有效地捕捉了图@ref(fig:tdl)中的流水线。CCNN
可被视为一种\emph{模板}，它概括了许多流行的架构，如卷积神经网络和注意力神经网络,CCNN
的抽象化具有很多优势。首先，任何适用于 CCNN 的结果都立即适用于 CCNN
架构的任何特定实例。事实上，只要符合 CCNN
定义，本文的理论分析和结果都适用于任何基于 CC
的神经网络。其次，如果神经网络的架构复杂，使用特定参数可能会很麻烦。在第
@ref(building-ccnns-tensor-diagrams)节中，我们将详细介绍参数化 TDL
模型的复杂架构。CCNN
更抽象的高层表示简化了学习过程的符号和一般目的，从而使 TDL 建模更直观。

\begin{figure}

{\centering \includegraphics{figures/tdl_blue_print} 

}

\caption{TDL蓝图 (a): 一组抽象实体. (b): 定义在$S$上的CC $(S, \mathcal{X}, \mbox{rk})$ . (c): 对于元素 $x \in \mathcal{X}$, 选择定义在CC上的邻域函数集合 (d): 用(c)中选择的领域函数建立神经网络. 神经网络利用在 (c) 中选择的邻域函数来更新 $x$ 上支持的数据.}\label{fig:tdl}
\end{figure}

\phantomsection\label{hoans-definition}
令 \(\mathcal{X}\) 是 CC，
令\(\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times  \mathcal{C}^{i_m}\)
和
\(\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times  \mathcal{C}^{j_n}\)
是\(m\) 和 \(\mathcal{X}\)上共链空间（cochain spaces）\(n\) 的笛卡尔积。
那么， \emph{组合复形神经网络（combinatorial complex neural
network，CCNN)} 就是如下形式的函数 \begin{equation*}
\mbox{CCNN}: \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times  \mathcal{C}^{i_m} \longrightarrow \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times \mathcal{C}^{j_n}.
\end{equation*}

直观上，CCNN以共链向量
\((\mathbf{H}_{i_1},\ldots, \mathbf{H}_{i_m})\)作为输入，返回共链向量\((\mathbf{K}_{j_1},\ldots, \mathbf{K}_{j_n})\)作为输出。在章节@ref(building-ccnns-tensor-diagrams)，我们将展示邻域函数如何在构建一般
CCNN 时发挥核心作用。定义 @ref(def:hoans-definition)并没有说明 CCNN
在一般情况下是如何计算的，章节 @ref(message-passing) 和
@ref(push-forward-pooling-and-unpooling)将 形式化将 CCNN
的计算工作流程。

\subsection{构建 CCNN：张量图}\label{building-ccnns-tensor-diagrams}

与涉及顶点或边缘信号的图不同，高阶网络需要更多的信号（见图
@ref(fig:cc-cochain)）。因此，构建 CCNN
需要构建大量相互作用的子神经网络。由于通过 CCNN
处理的共链数量可能很大，我们引入了\emph{张量图（tensor
diagrams）}，这是一种图解符号，用于描述拓扑域上支持的通用计算模型，并描述该域上支持和处理的各种信号的流向。

图解符号在几何拓扑文献中很常见Turaev
(\citeproc{ref-turaev2016quantum}{2016})，通常用于构建由更简单的构件构成的函数。进一步讨论见附录
@ref(ccnn-architecture-search-and-topological-quantum-field-theories)。关于单纯形神经网络的相关构造，另请参阅(\citeproc{ref-roddenberry2021principled}{T.
Mitchell Roddenberry, Glaze, and Segarra 2021})。

\phantomsection\label{tdd}
张量图可把CCNN表示成有相同图，张量图上的信号从\emph{源节点}流向\emph{目标节点}。源节点和目标节点分别对应
CCNN 的域和共域.

图@ref(fig:td)张量图的示例。左边显示的是三维的 CC，考虑0-cochain
\(\mathcal{C}^0\)，1-cochain \(\mathcal{C}^1\)和2-cochain
\(\mathcal{C}^2\)；中间的图给出了CCNN，它把\(\mathcal{C}^0 \times \mathcal{C}^1\times \mathcal{C}^2\)的共链向量映射到\(\mathcal{C}^0\times\mathcal{C}^1 \times \mathcal{C}^2\)中的共链向量；右图给出的是CCNN的张量图。用共链映射或其矩阵表示来标注张量图上的每条边，张量图@ref(fig:td)上的边标签是\(A_{0,1}, B_{0,1}^{T}, A_{1,1}, B_{1,2}\)
和 \(coA_{2,1}\)。因此，该张量图指明了 CC 上的共链流向。

\begin{figure}

{\centering \includegraphics{figures/tensor_diagram} 

}

\caption{张量图是 CCNN 的图解表示法，可捕捉 CCNN 上的信号流。}\label{fig:td}
\end{figure}

张量图箭头上的标签构成了一个\(\mathbf{G}= (G_i)_{i=1}^l\)
的共链映射序列，该序列定义在底层 CC
上，例如在@ref(fig:td)中的\(\mathbf{G}=(G_i)_{i=1}^5 = (A_{0,1}, B_{0,1}^{T}, A_{1,1}, B_{1,2}, coA_{2,1})\)。当使用张量图表示
CCNN 时，就可使用符号 \(\mbox{CCNN}_{\mathbf{G}}\) 表示张量图及其对应的
CCNN。共链映射\((G_i)_{i=1}^l\)反应了CC的结构，也确定了CC上的信号流。在第
@ref(neighbourhood-functions-on-ccs)节中提到的任何邻域矩阵都可以用作共链图，共链映射的选择取决于学习任务。

图@ref(fig:tensor)可视化了张量图的其它示例。张量图的\emph{高度（heigh）}是指从源节点到目标节点的最长路径上的边的数量。例如，图
@ref(fig:tensor) (a) 和 @ref(fig:tensor) (d) 中张量图的高度分别为 1 和
2。两个张量图的垂直连接表示其对应的 CCNN 的组成。例如，图
@ref(fig:tensor)(d)中的张量图就是图
@ref(fig:tensor)(c)和(b)中张量图的垂直连接。

\begin{figure}

{\centering \includegraphics{figures/hon_example} 

}

\caption{张量图示例 (a): 张量图 $\mbox{CCNN}_{coA_{1,1}}\colon \mathcal{C}^1 \to \mathcal{C}^1$. (b): 张量图 $\mbox{CCNN}_{ \{B_{1,2}, B_{1,2}^T\}} \colon \mathcal{C}^1 \times \mathcal{C}^2 \to \mathcal{C}^1 \times \mathcal{C}^2$. (c):合并节点，可合并三条共链. (d): 由 (c) 和 (b) 中的张量图垂直连接生成的张量图，边标签 $Id$ 表示同一矩阵。}\label{fig:tensor}
\end{figure}

如果张量图上的节点可以收到一个或多个信号，就称为\emph{聚合节点（merge
node）}。在数学上来解释，聚合节点就是函数\(\mathcal{M}_{G_1,\ldots ,G_m}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times \mathcal{C}^{i_m} \to \mathcal{C}^{j}\)，该函数由下式给出：

\begin{equation}
    (\mathbf{H}_{i_1},\ldots,\mathbf{H}_{i_m}) \xrightarrow[]{\mathcal{M}} \mathbf{K}_{j}=
    \mathcal{M}_{G_1,\ldots,G_m}(\mathbf{H}_{i_1},\ldots,\mathbf{H}_{i_m}),
    (\#eq:sum)
\end{equation}
其中，\(G_k \colon C^{i_k}(\mathcal{X})\to C^{j}(\mathcal{X}), k=1,\ldots,m\)，是共链映射。我们将
\(\mathcal{M}\) 视为一个消息传递函数，它考虑了由映射
\(G_1,\ldots,G_m\)输出的消息，这些映射共同作用于一个共链向量
\((\mathbf{H}_{i_1},\ldots,\mathbf{H}_{i_m})\)，从而得到一个更新的共链
\(\mathbf{K}_{j}\)。更多细节参看@ref(push-forward-operator-and-merge-node)
和
@ref(higher-order-message-passing-neural-networks-are-ccnns)，图@ref(fig:tensor)(c)就给出了这样的一个聚合节点示例。

\subsection{前推操作（Push-forward
operator）和聚合节点}\label{push-forward-operator-and-merge-node}

本节引入了前推操作，它是一种计算方案，可以将
\(i-cells\)胞腔支持的共链发送到 \(j-cells\)
胞腔。前推操作是一个计算构件，用于形式化定义方程
@ref(eq:sum)中给出的聚合节点、第
@ref(message-passing)章中介绍的高阶消息传递，以及第
@ref(push-forward-pooling-and-unpooling)节中介绍的(un)pooling操作。

\phantomsection\label{pushing-exact-definition}
在 CC \(\mathcal{X}\)上, 有共链映射
\(G\colon\mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})\) 和
\(\mathbf{H}_i\) in \(\mathcal{C}^i(\mathcal{X})\)。
\(G\)诱导的\emph{(cochain) push-forward} 就是算子
\(\mathcal{F}_G \colon \mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})\)，该算子定义如下
\begin{equation}
\mathbf{H}_i \to \mathbf{K}_j=[ \mathbf{k}_{y^j_1},\ldots,\mathbf{k}_{y^j_{|\mathcal{X}^j|} }] = \mathcal{F}_G(\mathbf{H}_i),
\end{equation} 使得 \(k=1,\ldots,|\mathcal{X}^j|\), \begin{equation}
\mathbf{k}_{y_k^j}= \bigoplus_{x_l^i \in \mathcal{N}_{G^T(y_k^j)}} \alpha_{G} ( \mathbf{ \mathbf{h}_{x_l^i}}),
(\#eq:functional)
\end{equation} 其中，\(\bigoplus\)
是具有置换不变性（permutation-invariant）的聚合函数，\(\alpha_G\)
是可微函数（differentiable function）。

\begin{quote}
译者注：置换不变性（permutation-invariant），元素的次序对函数的响应没有影响。假设函数\(f\)将\(\mathcal{X}\in \mathbb{R}^{d}\)域的向量空间映射到\(\mathcal{Y}\)域的离散空间，作用于集合上的函数\(f:\mathcal{X}\rightarrow \mathcal{Y}\)对集合中元素的作用在任何置换\(\pi\)下都不变，即：\(f(\{x_{1},\ldots,x_{M}\})=f(\{x_{\pi(1)},\ldots,x_{\pi(M)})\}\)，则说\(f\)具有置换不变性。
\end{quote}

算子\(\mathcal{F}_{G}\)前推\(\mathcal{X}^i\)支持的\(i-cochain\)
\(\mathbf{H}_i\)到\(\mathcal{X}^j\)支持的 \(j-cochain\)
\(\mathcal{F}_{G}(\mathbf{H}_i)\)。对于每个胞腔\(y \in \mathcal{X}^j\)，公式@ref(eq:functional)通过聚合所有隶属于\(y\)由邻域函\(\mathcal{N}_{G^T}\)确定的邻域\(x \in \mathcal{X}^i\)上的向量\(\mathbf{h}_x\)来构造向量\(\mathbf{k}_y\)，而且还对聚合向量集合\(\{ \mathbf{h}_x| x\in \mathcal{N}_{G^T}(y)\}\)施加了微分函数\(\alpha_G\)。

图@ref(fig:push-forward)可视化了两个前推算子的示例。示例@ref(exm:non-trainable-pushforward)提供了由关联矩阵（indicence
matrix）诱导的前推函数，而示例@ref(exm:non-trainable-pushforward)中的前推函数不包含任何参数，因此无法训练。在章节@ref(definition-of-combinatorial-complex-convolutional-networks)，将给出参数化的前推操作，其参数是可训练的。

\begin{figure}

{\centering \includegraphics{figures/push_forward} 

}

\caption{前推操作示例 (a): 令 $G_1\colon \mathcal{C}^1\to \mathcal{C}^2$ 是共链映射。由$G_1$诱导的前推 $\mathcal{F}_{G_1}$ 操作输入一个定义在底层CC $\mathcal{X}$的边上的1-cochain $\textbf{H}_{1}$，然后前推该共链到定义在$\mathcal{X}^2$上的2-cochain $\mathbf{K}_2$。通过用邻域函数 $\mathcal{N}_{G_1^T}$聚合$\mathbf{H}_1$上的消息就形成了共链$\mathbf{K}_2$。在这种情况下，相对于 $G_1$，2-rank（蓝色）胞腔的邻居是该胞腔边界上的四条（粉色）边。（b）类似的，$G_2\colon \mathcal{C}^0\to \mathcal{C}^2$诱导前推映射$\mathcal{F}_{G_2}\colon \mathcal{C}^0\to \mathcal{C}^2$，发送0-cochain $\mathbf{H}_0$到2-cochain $\mathbf{K}_2$。使用邻域函数$\mathcal{N}_{G_2^T}$聚合$\mathbf{H}_0$上的消息就形成了共链$\mathbf{K}_2$。}\label{fig:push-forward}
\end{figure}

\phantomsection\label{non-trainable-pushforward}
在维度2 CC \(\mathcal{X}\)上，令
\(B_{0,2}\colon \mathcal{C}^2 (\mathcal{X})\to \mathcal{C}^0 (\mathcal{X})\)
是关联矩阵。由\(\mathcal{F}^{m}_{B_{0,2}}(\mathbf{H_{2}})= B_{0,2} (\mathbf{H}_{2})\)定义的函数\(\mathcal{F}^{m}_{B_{0,2}}\colon\mathcal{C}^2 (\mathcal{X})\to \mathcal{C}^0 (\mathcal{X})\)是由\(B_{0,2}\)诱导的前推操作。\(\mathcal{F}^{m}_{B_{0,2}}\)
前推共链 \(\mathbf{H}_{2}\in \mathcal{C}^2\) 到共链
\(B_{0,2} (\mathbf{H}_{2}) \in \mathcal{C}^0\).

在定义 @ref(def:exact-definition-merge-node)中,
我们使用前推算子来表述聚合节点的概念.
图@ref(fig:merge-node)用张量图可视化了聚合节点的定义@ref(def:exact-definition-merge-node)
。

\phantomsection\label{exact-definition-merge-node}
令 \(\mathcal{X}\)是CC，令
\(G_1\colon\mathcal{C}^{i_1}(\mathcal{X})\to\mathcal{C}^j(\mathcal{X})\)和
\(G_2\colon\mathcal{C}^{i_2}(\mathcal{X})\to\mathcal{C}^j(\mathcal{X})\)是两个共链映射。给定共链向量
\((\mathbf{H}_{i_1},\mathbf{H}_{i_2}) \in \mathcal{C}^{i_1}\times \mathcal{C}^{i_2}\),
\emph{聚合节点}
\(\mathcal{M}_{G_1,G_2}\colon\mathcal{C}^{i_1} \times \mathcal{C}^{i_2} \to \mathcal{C}^j\)
被定义为 \begin{equation}
    \mathcal{M}_{G_1,G_2}(\mathbf{H}_{i_1},\mathbf{H}_{i_2})= \beta\left( \mathcal{F}_{G_1}(\mathbf{H}_{i_1})  \bigotimes \mathcal{F}_{G_2}(\mathbf{H}_{i_2}) \right),
\end{equation} 其中，
\(\bigotimes \colon \mathcal{C}^j \times \mathcal{C}^j \to \mathcal{C}^j\)
是聚合函数, \(\mathcal{F}_{G_1}\) 和 \(\mathcal{F}_{G_2}\)是\(G_1\) 和
\(G_2\)诱导的前推操作，并且\(\beta\)是激活函数。

\begin{figure}

{\centering \includegraphics{figures/merge_node_scaled} 

}

\caption{聚合节点定义的可视化.}\label{fig:merge-node}
\end{figure}

\subsection{三种主要的张量操作}\label{the-main-three-tensor-operations}

CCNN
的任何张量图都可以通过两个基本操作构建：前推算子和聚合节点。在实践中，引入其他操作可以更有效地构建相关的神经网络架构。例如，一个有用的操作是聚合节点的双重操作，称之为分裂节点。

\phantomsection\label{exact-definition-split-node}
令
\(\mathcal{X}\)是CC，令\(G_1\colon\mathcal{C}^{j}(\mathcal{X})\to\mathcal{C}^{i_1}(\mathcal{X})\)
和
\(G_2\colon\mathcal{C}^{j}(\mathcal{X})\to\mathcal{C}^{i_2}(\mathcal{X})\)是两个共链映射。给定共链\(\mathbf{H}_{j} \in \mathcal{C}^{j}\)，
\emph{分裂节点}操作
\(\mathcal{S}_{G_1,G_2}\colon\mathcal{C}^j \to \mathcal{C}^{i_1} \times \mathcal{C}^{i_2}\)
定义为: \begin{equation}
\mathcal{S}_{G_1,G_2}(\mathbf{H}_{j})= \left(  \beta_1(\mathcal{F}_{G_1}(\mathbf{H}_{j})) , \beta_2(\mathcal{F}_{G_2}(\mathbf{H}_{j})) \right),
\end{equation} 其中， \(\mathcal{F}_{G_i}\)
是\(G_i\)诱导的前推操作，并且\(\beta_i\)是激活函数，\(i=1, 2\)。

虽然从定义
@ref(def:exact-definition-split-node)中可以看出，分裂节点只是前推操作的元组，但使用分裂节点可以让我们更有效、更直观地构建神经网络。定义
@ref(def:elem-opers)提出了一组基本的张量运算，包括分裂节点，以方便用张量图来表述
CCNN。

\phantomsection\label{elem-opers}
我们将前推操作、聚合节点和分裂节点统称为基本张量操作。

图 @ref(fig:split-merge-pushforward) 给出了基本张量运算的张量图，图
@ref(fig:prior-work)
举例说明现有拓扑神经网络如何通过基于基本张量运算的张量图来表达。
例如，文献(\citeproc{ref-roddenberry2021principled}{T. Mitchell
Roddenberry, Glaze, and Segarra 2021})提出的基于霍奇分解（Hodge
decomposition）的神经网络--单纯复形网（SCoNe），就可以有效地通分裂分和聚合节点来实现，如图所示。
@ref(fig:prior-work)(a).

\begin{quote}
霍奇分解定理,曲面上任意一个光滑切向量场，可以被唯一地分解为三个向量场：梯度场、散度场和调和场
\end{quote}

\begin{figure}

{\centering \includegraphics{figures/split_merge_push} 

}

\caption{基本张量运算（即前推运算、聚合节点和分裂节点）的张量图。这三种基本映射张量运算是构建一般 CCNN 张量图的基石. 利用三种基本张量运算的合成和横向连接，可以形成一般的张量图。 (a): 由共链 $G\colon\mathcal{C}^i \to \mathcal{C}^j$ 定义的操作为。）（b）两个共链映射 $G_1\colon\mathcal{C}^{i_1} \to \mathcal{C}^j$和$G_2\colon\mathcal{C}^{i_2} \to \mathcal{C}^j$ 诱导生成的聚合节点。（c）两个共链映射公 $G_1\colon\mathcal{C}^{j}\to\mathcal{C}^{i_1}$ 和 $G_2\colon\mathcal{C}^{j}\to\mathcal{C}^{i_2}$ 诱导生成的分裂节点. 在图示中，函数 $\Delta \colon \mathcal{C}^{j}\to \mathcal{C}^{j}\times \mathcal{C}^{j}$ 被定义为 $\Delta(\mathbf{H}_j)= (\mathbf{H}_j,\mathbf{H}_j)$.}\label{fig:split-merge-pushforward}
\end{figure}

\begin{figure}

{\centering \includegraphics{figures/prior_work} 

}

\caption{可通过三种基本张量运算实现现有神经网络的示例。为简化说明，去掉了边标签. (a):文献[@roddenberry2021principled]提出的单纯复形网络（simplicial complex net，SCoNe）可以实现为分裂节点的组合，把输入的1-cochain分裂为三个0维、1维、2维共链，然后通过一个聚合节点再把这些共链聚合为 1-cochain。 (b): 文献[@ebli2020simplicial]提出的单纯形神经网络（simplicial neural network，SCN）可以实现为前推操作。 (c)--(e): 胞腔复形神经网络的示例（cell complex neural networks，CXNs)，参见文献 [@hajijcell]。 注意： (e) 通过将0-cochains和2-cochains聚合为1-cochains的聚合节点，以及将1-cochain分裂为0-cochain和2-cochains的分裂节点来实现。}\label{fig:prior-work}
\end{figure}

基本张量运算构成了定义任何参数化拓扑神经网络所需的唯一框架。事实上，既然可以通过三个基本张量运算建立张量图，那么只需定义前推运算和聚合运算，就可以完全定义一类参数化的
CCNN（回顾一下，分裂节点完全由前推运算决定）。
在章节@ref(definition-of-combinatorial-complex-convolutional-networks)
和
@ref(combinatorial-complex-attention-neural-networks)，我们建立了两类参数化的CCNN：卷积类和注意力类。在这两种情况下，我们只定义了相应的参数化基本张量运算。除了卷积和注意力版本的CCNNs，三种基本张量运算允许我们构建任意参数化的张量图，因此为发现新的拓扑神经网络架构提供了空间，而我们的理论仍适用于这些架构。

构建 CCNN 的另一种方法是借鉴拓扑量子场论（topological quantum field
theory，TQFT）的思想。在附录
@ref(ccnn-architecture-search-and-topological-quantum-field-theories)中，我们简要地讨论了他们之间的深层关系。

\subsection{组合复形卷积网络的定义（combinatorial complex convolutional
networks）}\label{definition-of-combinatorial-complex-convolutional-networks}

在高阶域上的深度学习的基本计算需求之一是具备定义和计算卷积运算的能力。在本节，我们将介绍配备卷积算子的CCNNs，称之为\emph{组合复形卷积神经网络（combinatorial
complex convolutional neural networks，CCCNN）}。特别地，为 CCCNNs
提出了两种卷积算子：CC-卷积（CC-convolutional）前推算子和CC-卷积聚合节点。

我们将演示如何从两个基本模块引入
CCCNN：在章节@ref(push-forward-operator-and-merge-node)抽象定义的前推和聚合操作。正如在定义
@ref(def:cc-conv-pushforward)所设想的那样，CC-convolutional前推操作用最简单的形式将文献(\citeproc{ref-kipf2016semi}{Kipf
and Welling 2016})中引入的卷积图神经网络进行了泛化。

\phantomsection\label{cc-conv-pushforward}
假设在一个CC \(\mathcal{X}\) 上, 有共链映射
\(G\colon \mathcal{C}^i (\mathcal{X}) \to \mathcal{C}^j(\mathcal{X})\)
和 \(\mathbf{H}_i \in C^i(\mathcal{X}, \mathbb{R}^{{s}_{in}})\)。
\emph{CC-convolutional push-forward} 操作就是共链映射
\(\mathcal{F}^{conv}_{G;W} \colon C^i(\mathcal{X}，\mathbb{R}^{{s}_{in}}) \to C^j(\mathcal{X}, \mathbb{R}^{{t}_{out}})\)
定义如下 \begin{equation}
\mathbf{H}_i \to  \mathbf{K}_j=  G \mathbf{H}_i W ,
(\#eq:cc-conv-push-forward-eq)
\end{equation} 其中，
\(W \in \mathbb{R}^{d_{s_{in}}\times d_{s_{out}}}\) 是可训练参数。

一旦有了CC-convolutional前推操作的定义，CC-convolutional聚合节点的定义（参见@ref(def:cc-convolutional)）就是定义@ref(def:exact-definition-merge-node)的直接应用。在最近的文献
(\citeproc{ref-bunch2020simplicial}{Bunch et al. 2020};
\citeproc{ref-ebli2020simplicial}{Ebli, Defferrard, and Spreemann 2020};
\citeproc{ref-hajijcell}{Hajij, Istvan, and Zamzmi 2020};
\citeproc{ref-schaub2020random}{Schaub et al. 2020},
\citeproc{ref-schaub2021signal}{2021};
\citeproc{ref-roddenberry2021principled}{T. Mitchell Roddenberry, Glaze,
and Segarra 2021}; \citeproc{ref-calmon2022higher}{Calmon, Schaub, and
Bianconi 2022}; \citeproc{ref-hajij2021simplicial}{Hajij, Zamzmi, et al.
2022}; \citeproc{ref-roddenberry2021signal}{T. Mitchell Roddenberry,
Schaub, and Hajij 2022}; \citeproc{ref-yang2023convolutional}{Yang and
Isufi 2023})中也出现了一些定义@ref(def:cc-convolutional)的变种。

\phantomsection\label{cc-convolutional}
令
\(\mathcal{X}\)是CC,令\(G_1\colon\mathcal{C}^{i_1}(\mathcal{X}) \to\mathcal{C}^j(\mathcal{X})\)
和
\(G_2\colon\mathcal{C}^{i_2}(\mathcal{X})\to\mathcal{C}^j(\mathcal{X})\)是两个共链映射。给定共链向量
\((\mathbf{H}_{i_1},\mathbf{H}_{i_2}) \in \mathcal{C}^{i_1}\times \mathcal{C}^{i_2}\)，
\emph{CC-convolutional聚合节点}
\(\mathcal{M}^{conv}_{\mathbf{G};\mathbf{W}} \colon\mathcal{C}^{i_1} \times \mathcal{C}^{i_2} \to \mathcal{C}^j\)
可以定义如下： \begin{equation}
\begin{aligned}
\mathcal{M}^{conv}_{\mathbf{G};\mathbf{W}}
(\mathbf{H}_{i_1},\mathbf{H}_{i_2}) &=
\beta\left( \mathcal{F}^{conv}_{G_1;W_1}(\mathbf{H}_{i_1})
+ \mathcal{F}^{conv}_{G_2;W_2}(\mathbf{H}_{i_2})  \right)\\
&= \beta ( G_1 \mathbf{H}_{i_1} W_1  +  G_2 \mathbf{H}_{i_2} W_2 ),\\
\end{aligned}
\end{equation} 其中， \(\mathbf{G}=(G_1, G_2)\),
\(\mathbf{W}=(W_1, W_2)\)是一组可训练参数，\(\beta\) 是激活函数。

事实上，定义@ref(def:cc-conv-pushforward)中共链映射\(G\)的矩阵表示可能在训练过程中需要针对具体问题进行归一化（normalization）。关于高阶卷积算子背景下的各种归一化，我们推荐读者参阅(\citeproc{ref-kipf2016semi}{Kipf
and Welling 2016}; \citeproc{ref-bunch2020simplicial}{Bunch et al.
2020}; \citeproc{ref-schaub2020random}{Schaub et al. 2020})
(\citeproc{ref-kipf2016semi}{Kipf and Welling 2016};
\citeproc{ref-bunch2020simplicial}{Bunch et al. 2020};
\citeproc{ref-schaub2020random}{Schaub et al. 2020})。

我们用 \(\mbox{CCNN}_{\mathbf{G}}\) 概念表示张量图及其对应的 CCNN。
我们所用的负号表明，CCNN 是由基于定义在底层 CC 上的共链映射序列
\(\mathbf{G}= (G_i)_{i=1}^l\) 的基本张量运算组成的。 当组成 CCNN
的基本张量运算由可训练参数序列 \(\mathbf{W}= (W_i)_{i=1}^k\)
参数化时，就可以用 \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) 表示 CCNN
及其张量图表示。

\subsection{组合复形注意力神经网络}\label{combinatorial-complex-attention-neural-networks}

大多数高阶深度学习模型都侧重于使用\emph{各向同性聚合(isotropic
aggregation)}的层，这意味着元素附近的邻近元素对元素表示的更新贡献相同。由于信息是以扩散的方式聚合的，这种各向同性的聚合会限制这些学习模型的表现力，从而导致过平滑（oversmoothing）等现象(\citeproc{ref-beaini2021directional}{Beaini
et al. 2021})。相比之下，\emph{基于注意力的学习（attention-based
learning）} (\citeproc{ref-choi2017gram}{Choi et al.
2017})允许深度学习模型为底层领域中元素局部附近的邻域分配概率分布，从而突出具有最相关任务信息的组件(\citeproc{ref-velickovic2017graph}{Veličković
et al. 2018})。
基于注意力的模型在实践中是成功的，因为它们忽略了域中的噪声，从而提高了信噪比（signal-to-noise
ratio）(\citeproc{ref-mnih2014recurrent}{Mnih et al. 2014};
\citeproc{ref-boaz2019}{J. B. Lee et al.
2019})。因此，基于注意力的模型在图的传统机器学习任务中取得了显著的成功，包括节点分类和链接预测
(\citeproc{ref-li2021learning}{Z. Li et al. 2021})、节点排名
(\citeproc{ref-sun2009rankclus}{Sun et al. 2009}) 和基于注意力的嵌入
(\citeproc{ref-choi2017gram}{Choi et al. 2017};
\citeproc{ref-lee2018graph}{J. B. Lee, Rossi, and Kong 2018}) 等。

章节@ref(definition-of-combinatorial-complex-convolutional-networks)给CC-convolutional引入了前推操作，本节将给出前推操作的第二个示例：CC注意力前推操作（CC-attention
push-forward）。

那么，就可以使用配备了CC-attention前推算子的CCNNs，称之为\emph{组合复形注意力神经网络（combinatorial
complex attention neural networks，CCANNs）}。我们首先提供一个 CC
的sub-CC \(\mathcal{Y}_0\) 相对于 CC 中其他sub-CC 的注意力的一般概念。

\phantomsection\label{hoa}
令 \(\mathcal{X}\) 是 CC,
\(\mathcal{N}\)是\(\mathcal{X}\)上的邻域函数，\(\mathcal{Y}_0\)是
\(\mathcal{X}\)的sub-CC。 令
\(\mathcal{N}(\mathcal{Y}_0)=\{ \mathcal{Y}_1,\ldots, \mathcal{Y}_{|\mathcal{N}(\mathcal{Y}_0)|} \}\)
是sub-CCs
集合，该集合的元素是邻域函数\(\mathcal{N}\)下\(\mathcal{Y}_0\)的邻居。那么，在\(\mathcal{N}\)下，\(\mathcal{Y}_0\)的\emph{higher-order
attention}就是函数
\(a\colon {\mathcal{Y}_0}\times \mathcal{N}(\mathcal{Y}_0)\to [0,1]\)，该函数给每个元素\(\mathcal{Y}_i\in\mathcal{N}(\mathcal{Y}_0)\)赋予权重
\(a(\mathcal{Y}_0, \mathcal{Y}_i)\)，使得
\(\sum_{i=1}^{| \mathcal{N}(\mathcal{Y}_0)|} a(\mathcal{Y}_0,\mathcal{Y}_i)=1\).

\begin{quote}
译者注：原文中，上述定义写的是Cigher-order
attention，我觉得是写错了，应该是Highr
\end{quote}

正如定义@ref(def:hoa)所反应的，邻域函数\(\mathcal{N}\)下sub-CC
\(\mathcal{Y}_0\)的高阶注意力给\(\mathcal{Y}_0\)的每个邻居赋予了一个离散分布值。

基于注意力的学习通常旨在学习函数 \(a\)。请注意，函数 \(a\)
依赖于邻域函数
\(\mathcal{N}\)。在我们的语境中，我们的目标是学习邻域函数\(a\)，它是一个关联或（共）邻接函数
，正如第@ref(neighbourhood-functions-on-ccs)节所介绍的那样。

回忆一下定义@ref(def:hoa)，权重\(a(\mathcal{Y}_0,\mathcal{Y}_i)\)需要一个源sub-CC
\(\mathcal{Y}_0\)和一个目标sub-CC
\(\mathcal{Y}_i\)作为输入。因此，一个CC-attention
push-forward操作需要两个共链空间。定义@ref(def:hoan-sym)
引入了概念CC-attention
push-forward，其中两个底层共链空间包含等秩胞腔上支持的共链。

\phantomsection\label{hoan-sym}
令 \(G\colon C^{s}(\mathcal{X})\to C^{s}(\mathcal{X})\)是邻域矩阵。
\(G\)诱导的\emph{CC-attention push-forward} 就是一个共链映射
\(\mathcal{F}^{att}_{G}\colon C^{s}(\mathcal{X}, \mathbb{R}^{d_{s_{in}}}) \to C^{s}(\mathcal{X},\mathbb{R}^{d_{s_{out}}})\)，定义为：
\begin{equation}
\mathbf{H}_s \to \mathbf{K}_{s} = (G \odot att)  \mathbf{H}_{s}  W_{s} ,
(\#eq:attention1)
\end{equation} 其中，\(\odot\) 是Hadamard积，
\(W_{s}  \in \mathbb{R}^{d_{s_{in}}\times d_{s_{out}}}\)可训练参数，
\(att\colon C^{s}(\mathcal{X})\to C^{s}(\mathcal{X})\) 是
\emph{高阶注意力矩阵（higher-order attention
matrix）}，该矩阵和矩阵\(G\)有同样的维度。
矩阵\(att\)的第\((i,j)\)-th项定义为： \begin{equation}
att(i,j) =  \frac{e_{ij}}{ \sum_{k \in \mathcal{N}_{G}(i) e_{ik} } },
\end{equation} 其中，
\(e_{ij}= \phi(a^T [W_{s} \mathbf{H}_{s,i}||W_{s} \mathbf{H}_{s,j} ] )\),
\(a \in \mathbb{R}^{2 \times s_{out}}\)是可训练向量， \([a ||b ]\)
表示\(a\) 和 \(b\)的连接（concatenation）， \(\phi\)
是激活函数，\(\mathcal{N}_{G}(i)\)是矩阵\(G\)下胞腔\(i\)的邻居。

\begin{quote}
译者注：哈达玛积(Hadamard
product)，若\(A=(a_{i,j})\)和\(B=(b_{i,j})\)是两个同阶矩阵，若\(c_{i,j}=a_{i,j}\times b_{i,j}\)，则称矩阵\(C=(c_{i,j})\)为A和B的哈达玛积，或称基本积
\end{quote}

定义 @ref(def:hoan-asym)处理的情况比定义
@ref(def:hoan-sym)更普遍。具体来说，定义 @ref(def:hoan-asym)引入了一个
CC-attention push-forward
的概念，在这个概念中，两个底层共链空间可以包含不同秩的胞腔上所支持的共链。

\phantomsection\label{hoan-asym}
对于 \(s\neq t\), 令
\(G\colon C^{s}(\mathcal{X})\to C^{t}(\mathcal{X})\)
是邻域矩阵，\(G\)诱导的\emph{CC-attention block} 是共链映射
\(\mathcal{F}_{G}^{att}  {\mathcal{A}}\colon C^{s}(\mathcal{X},\mathbb{R}^{d_{s_{in}}}) \times C^{t}(\mathcal{X},\mathbb{R}^{d_{t_{in}}}) \to C^{t}(\mathcal{X},\mathbb{R}^{d_{t_{out}}}) \times C^{s}(\mathcal{X},\mathbb{R}^{d_{s_{out}}})\)，定义为
\begin{equation}
(\mathbf{H}_{s},\mathbf{H}_{t}) \to  (\mathbf{K}_{t}, \mathbf{K}_{s} ),
\end{equation} 且 \begin{equation}
\mathbf{K}_{t} = ( G \odot att_{s\to t})  \mathbf{H}_{s} W_{s} ,\;
\mathbf{K}_{s} = (G^T \odot att_{t\to s})  \mathbf{H}_{t}  W_{t} ,
(\#eq:attention2)
\end{equation} 其中，
\(W_s \in \mathbb{R}^{d_{s_{in}}\times d_{t_{out}}} , W_t \in \mathbb{R}^{d_{t_{in}}\times d_{s_{out}}}\)是可训练参数，\(att_{s\to t}^{k}\colon C^{s}(\mathcal{X})\to C^{t}(\mathcal{X}) , att_{t\to s}^{k}\colon C^{t}(\mathcal{X})\to C^{s}(\mathcal{X})\)
是 \emph{高阶注意力矩阵（higher-order attention
matrices）}，这些矩阵分别和矩阵\(G\) 、 \(G^T\)有同样的维度。
矩阵\(att_{s\to t}\)的第 \((i,j)\)-th项和\(att_{t\to s}\)可定义为
\begin{equation}
(att_{s\to t})_{ij} =  \frac{e_{ij}}{ \sum_{k \in \mathcal{N}_{G} (i) e_{ik} } },\;
(att_{t\to s})_{ij} =  \frac{f_{ij}}{ \sum_{k \in \mathcal{N}_{G^T} (i) f_{ik} } },
(\#eq:ast-ats)
\end{equation} 且 \begin{equation}
e_{ij} = \phi((a)^T [W_s \mathbf{H}_{s,i}||W_t \mathbf{H}_{t,j}] ),\;
f_{ij} = \phi(rev(a)^T [W_t \mathbf{H}_{t,i}||W_s \mathbf{H}_{s,j}]),
(\#eq:ef)
\end{equation} 其中， \(a \in \mathbb{R}^{t_{out} + s_{out}}\)
是可训练向量，并且 \(rev(a)= [ a^l[:t_{out}]||a^l[t_{out}:]]\).

定义@ref(def:inc-mat)中的关联矩阵可用作定义@ref(def:hoan-asym)中的邻居矩阵。在图@ref(fig:B12)中，我们说明了关联邻域矩阵的
CC-attention 概念。图@ref(fig:B12)(c)给出了与图@ref(fig:B12)(a) 中显示的
CC 相关的非方正关联矩阵（non-squared incidence matrix）
\(B_{1,2}\)。注意力模块\(\mbox{HB}_{B_{1,2}}\)负责学习两个关联矩阵\(att_{s\to t}\)
和
\(att_{t\to s}\)，矩阵\(att_{s\to t}\)和\(B_{1,2}\)有同样的形状，并且非零元素恰好位于
\(B_{1,2}\) 中元素等于 1 的地方。

在 \(att_{s\to t}\) 中，每列 \(i\)都代表一个概率分布，它定义了第
\(i\)-th个 2-cell相对其关联1-cells的注意力。矩阵 \(att_{t\to s}\)
的形状与
\(B_{1,2}^T\)相同，并且1-cells到2-cells的注意力也有同样的表示形式。

\begin{figure}

{\centering \includegraphics{figures/B12} 

}

\caption{图示不等秩胞腔上的CC-attention (a): 一个CC。 CC的每个 $2$-cell (蓝色面)都连接着它自己的关联$1$-cells(粉色边). (b): 图上给出了由胞腔及其关联关系确定的注意力权重。 (c): (a)中CC的关联矩阵。列$[1,2,3]$中的非零元对应于$[1,2,3]$关于$B_{1,2}$的邻域$\mathcal{N}_{B_{1,2}}([1,2,3])$}\label{fig:B12}
\end{figure}

前推共链\(\mathbf{K}_t\)的计算需要两个共链，称作\(\mathbf{H}_s\) and
\(\mathbf{H}_t\)。在公式@ref(eq:attention2)中，\(\mathbf{K}_t\)直接依赖\(\mathbf{H}_s\)，但是从@ref(eq:attention2)--@ref(eq:ef)可见，它也通过\(att_{s\to t}\)非直接依赖于\(\mathbf{H}_t\)。此外，共链\(\mathbf{H}_t\)在训练期间仅需要有\(att_{s\to t}\)就行，但是在推断期间则不行。也就是说，CC-attention前推模块的计算在推断期间仅需要单个共链\(\mathbf{H}_s\)即可，这和定义@ref(def:pushing-exact-definition)中定义的通用共链前推操作的计算一样。

公式@ref(eq:attention1)和@ref(eq:attention2)中的算子\(G \odot att\) 和
\(G \odot att_{s\to t}\)
可被看作\(G\)的注意力学习版本，这使得可以使用CCANNs来学习任意类型的离散外积分算子（discrete
exterior calculus
operators），正如附录@ref(learning-discrete-exterior-calculus-operators-with-ccanns)中所描述的那样。

\section*{(PART*) 第三部分：高阶消息传递（Higher-order message
passing）}\label{part-ux7b2cux4e09ux90e8ux5206ux9ad8ux9636ux6d88ux606fux4f20ux9012higher-order-message-passing}
\addcontentsline{toc}{section}{(PART*)
第三部分：高阶消息传递（Higher-order message passing）}

\section{消息传递}\label{message-passing}

在本节中，我们将解释在第@ref(push-forward-operator-and-merge-node)节中引入的聚合节点与高阶消息传递之间的关系。
特别是，我们证明了 CC 上的高阶消息传递可以通过第
@ref(the-main-three-tensor-operations)节中介绍的基本张量运算来实现。
此外，我们还证明了
CCANNs（见章节@ref(combinatorial-complex-attention-neural-networks)）与高阶消息传递之间的联系，并介绍了高阶消息传递的注意力版本。我们首先定义了CC上的高阶消息传递，泛化了(\citeproc{ref-hajijcell}{Hajij,
Istvan, and Zamzmi 2020})中引入的概念。

我们要指出的是，这里讨论的许多构造都是最基本的形式，但还可以进一步扩展。这个方向的一个重要方面是构建与特定群的作用相关的不变量或等变量的信息传递协议。

\subsection{高阶消息传递的定义}\label{definition-of-higher-order-message-passing}

高阶消息传递指的是一种计算框架，它涉及使用一组邻域函数在高阶域中的实体和胞腔之间交换消息。
在定义@ref(def:
homp-definition)中，我们形式化了CCs的高阶消息传递概念,图@ref(fig:homp)说明了定义@ref(def:homp-definition)。

\phantomsection\label{homp-definition}
令 \(\mathcal{X}\)是CC，
\(\mathcal{N}=\{ \mathcal{N}_1,\ldots,\mathcal{N}_n\}\)是定义在
\(\mathcal{X}\)上的邻域函数几何，\(x\)是胞腔，
并且对于某个\(\mathcal{N}_k \in \mathcal{N}\)
有\(y\in \mathcal{N}_k(x)\)。 胞腔\(x\) 和 \(y\)之间的\emph{message}
\(m_{x,y}\)依赖于两个胞腔或胞腔上支持的数据。用\(\mathcal{N}(x)\)表示多集合\(\{\!\!\{ \mathcal{N}_1(x) , \ldots , \mathcal{N}_n (x) \}\!\!\}\)，用\(\mathbf{h}_x^{(l)}\)表示在层\(l\)的胞腔\(x\)上支持的某些数据。由\(\mathcal{N}\)诱导的\(\mathcal{X}\)上的高阶消息传递按如下四点规则来定义。

\begin{align}
m_{x,y} &= \alpha_{\mathcal{N}_k}(\mathbf{h}_x^{(l)},\mathbf{h}_y^{(l)}), (\#eq:homp0) \\
m_{x}^k &=  \bigoplus_{y \in \mathcal{N}_k(x)}  m_{x,y}, \; 1\leq k \leq n, (\#eq:homp1) \\
m_{x} &=  \bigotimes_{ \mathcal{N}_k \in \mathcal{N} } m_x^k, (\#eq:homp2) \\
\mathbf{h}_x^{(l+1)} &= \beta (\mathbf{h}_x^{(l)}, m_x).
(\#eq:homp3)
\end{align} 其中,
\(\bigoplus\)是置换不变性聚合函数（permutation-invariant aggregation
function），称作 \(x\)的\emph{内领域（intra-neighborhood）}；
\(\bigotimes\)是\(x\)的\emph{外邻域（inter-neighborhood）}
，称作聚合函；\(\alpha_{\mathcal{N}_k},\beta\) 等是可微函数。

\begin{figure}

{\centering \includegraphics{figures/homp} 

}

\caption{高阶信息传递示例. 左侧：选择一系列邻域函数 $\mathcal{N}_1,\ldots,\mathcal{N}_k$， 选择通常取决于学习任务。 右侧：对于每个 $\mathcal{N}_k$ ，使用内部邻域函数 $\bigoplus$ 聚合消息。 然后，用邻域间函数 $\bigotimes$ 汇集从所有邻域获得的最终信息。}\label{fig:homp}
\end{figure}

关于定义 @ref(def:homp-definition)的一些讨论如下。首先，方程
@ref(eq:homp0)中的信息\(m_{x,y}\)并不仅仅取决于胞腔 \(x、y\)
上支持的数据\(\mathbf{h}_x^{(l)}\)、\(\mathbf{h}_y^{(l)}\)，还取决于胞腔本身。例如，如果\(\mathcal{X}\)是一个胞腔复形，那么在计算信息\(m_{x,y}\)时，\(x\)和\(y\)的\emph{方向}都会被考虑在内。
另外，如果\(x\cup y\) 或 \(x\cap y\) 是 \(\mathcal{X}\)
中的胞腔，那么在计算信息\(m_{x,y}\)时包含它们的数据也可能是有用的。这种独特的特性只体现在高阶域中，而不会出现在基于图的消息传递框架中(\citeproc{ref-gilmer2017neural}{Gilmer
et al. 2017}; \citeproc{ref-bronstein2021geometric}{Bronstein et al.
2021})\footnote{\(m_{x,y}\)中的消息
  ``方向''是从\(y\)到\(x\)。一般来说，\(m_{x,y}\) 和
  \(m_{y,x}\)是不相等的}。其次，高阶消息传递依赖于邻域函数集\(\mathcal{N}\)的选择。这也是只有在高阶领域中才会出现的独特特征，在高阶领域中，邻域函数必然是由一组邻域关系来描述的，而不是像基于图的消息传递那样由图邻接关系来描述。
第三，在公式
@ref(eq:homp0)中，由于\(y\)是根据邻域关系\(\mathcal{N}_k \in\mathcal{N}\)来隐含定义的，所以函数\(\alpha_{\mathcal{N}_k}\)和消息\(m_{x,y}\)都取决于\(\mathcal{N}_k\)。第四，邻域间
\(\bigotimes\) 并不一定非得是一个置换不变的聚合函数。
例如，可以在多重集合 \(\mathcal{N}(x)\)
上设置一个阶（order），并根据这个阶计算\(m_x\)。最后，高阶消息传递依赖于两个聚合函数，即邻内聚合函数和邻间聚合函数，而基于图的消息传递依赖于一个聚合函数。正如第@ref(combinatorial-complexes)章节中所说明的，集合
\(\mathcal{N}\)的选择，使得可以在高阶消息传递中使用各种邻域函数。

定义 @ref(def:pushing-exact-definition)中给出的前推算子与公式
@ref(eq:homp0)的更新规则有关。
一方面，公式@ref(eq:homp0)需要两个共链\(\mathbf{X}_i= [\mathbf{h}_{x^i_1}^{(l)},\ldots,\mathbf{h}_{x^i_{|\mathcal{X}^i|}}^{(l)}]\)
和
\(\mathbf{Y}_{j}^{(l)}=[\mathbf{h}_{y^{j}_1}^{(l)},\ldots,\mathbf{h}_{y^{j}_{|\mathcal{X}^{j}|}}^{(l)}]\)
来计算
\(\mathbf{X}^{(l+1)}_i = [\mathbf{h}_{x^i_1}^{(l+1)},\ldots,\mathbf{h}_{x^i_{|\mathcal{X}^i|}}^{(l+1)}]\)，因此\(\mathcal{C}^j\)
和\(\mathcal{C}^i\)上的信号必须存在，才能计算公式@ref(eq:homp0)。
从这个角度来看，把这个操作看作更新规则是很自然和习惯的。另一方面，定义
@ref(def:pushing-exact-definition)中的前推算子计算的是在给定共链
\(\mathbf{H}_i\in \mathcal{C}^i\)时，\(\mathcal{C}^j\)中的共链\(\mathbf{K}_{j}\)。
由于只需要 \(\mathbf{H}_i\)
一条共链就可以完成这个计算，所以我们可以很自然地把方程
@ref(eq:functional)看作一个函数。更多详情，请参见第
@ref(merge-nodes-and-higher-order-message-passing-a-qualitative-comparison)节。

定义@ref(def:homp-definition)给出的高阶消息传递框架可用于构造CC上的新的神经网络架构，正如在图@ref(fig:tdl)中隐含反应的那样。首先，对于给定的的CC
\(\mathcal{X}\)，给出\(\mathcal{X}\)上支持的共链
\(\mathbf{H}_{i_1}\ldots, \mathbf{H}_{i_m}\)。
其次，根据期望的学习任务选择合适的邻域函数集合。然后,用选定的邻域函数在输入共链\(\mathbf{H}_{i_1}\ldots, \mathbf{H}_{i_m}\)上执行定义@ref(def:homp-definition)中的更新规则。
重复第2、3步，直到获得最终结果。

\phantomsection\label{hmpsnn}
凡是基于定义@ref(def:homp-definition)构造的任何神经网络都可称作\emph{高阶消息传递神经网络}.

\subsection{高阶消息传递神经网络就是CCNNs}\label{higher-order-message-passing-neural-networks-are-ccnns}

在本节，将表明高阶消息传递计算可以用聚合节点计算来实现，因此，高阶消息传递神经网络就可认为是CCNNs。结果，高阶消息传递就可以通过一致性更新规则集合来统一单纯复形、胞腔复形、超图上的消息传递，作为可选项，也可以通过张量图表示语言来实现统一。

\phantomsection\label{mn}
定义@ref(def:homp-definition)中的高阶消息传递计算可以用聚合节点计算来实现。

令
\(\mathcal{X}\)是CC，\(\mathcal{N}=\{ \mathcal{N}_1,\ldots,\mathcal{N}_n\}\)是定义@ref(def:homp-definition)中规定的一组邻域函数，\(G_k\)是邻域函数\(\mathcal{N}_k\)诱导的矩阵。假定定义@ref(def:homp-definition)指定的胞腔
\(x\)是 \(j\)-cell，\(y \in \mathcal{N}_k(x)\)的邻居是 \(i_k\)-cells。
这里将指出公式
@ref(eq:homp0)--@ref(eq:homp3)可以看作是聚合节点的应用实现。接下来，我们将定义的邻域函数是\(\mathcal{N}_{Id}(x)=\{x\}\)
for \(x\in \mathcal{X}\)。
此外，我们将用\(Id\colon\mathcal{C}^j\to \mathcal{C}^j\)来标记与\(\mathcal{N}_{Id}\)关联的的邻居矩阵，因为它就是一个单位阵（identity
matrix）。

\begin{quote}
译者注：最后一句不知道翻的对不对，感觉句子不通顺，意思不明确。
\end{quote}

计算公式@ref(eq:homp0)中的消息\(m_{x,y}\)涉及到两个共链:
\begin{equation*}
        \mathbf{X}_j^{(l)}=
        [\mathbf{h}_{x^j_1}^{(l)},\ldots,\mathbf{h}_{x^j_{|\mathcal{X}^j|}}^{(l)}],~
        \mathbf{Y}_{i_k}^{(l)}=
        [\mathbf{h}_{y^{i_k}_1}^{(l)},\ldots,\mathbf{h}_{y^{i_k}_{|\mathcal{X}^{i_k}|}}^{(l)}].
    \end{equation*}
每个消息\(m_{x^{^j}_t, y^{i_k}_s }\)都与矩阵\(G_k\)中的一个项\([G_k]_{st}\)相对应。换句话说，矩阵\(G_k\)的每个非零项和消息\(m_{x^{^j}_t, y^{i_k}_s }\)都是一一对应的。

从第@ref(push-forward-operator-and-merge-node)节可以看出，计算
\(\{m_x^k\}_{k=1}^n\) 相当于聚合节点
\(\mathcal{M}_{Id_j、 G_k}\colon \mathcal{C}^j\times \mathcal{C}^{i_k}\to \mathcal{C}^j\)
执行通过 \(\alpha_k\) 和 \(\bigoplus\) 确定的计算，并且得出
\begin{equation*}
        \mathbf{m}_j^k=[m_{x^j_1}^k,\ldots,m_{x^j_{|\mathcal{X}^j|}}^k]=
        \mathcal{M}_{Id_j,G_k}(\mathbf{X}_j^{(l)},\mathbf{Y}_{i_k}^{(l)}) \in \mathcal{C}^{j}.
    \end{equation*} 在这个阶段， 有 \(n\) \(j\)-cochains
\(\{\mathbf{m}_j^k\}_{k=1}^n\)。公式@ref(eq:homp2) 和 @ref(eq:homp3)
，将这些共链与输入的\(j\)-cochain \(\mathbf{X}_j^{(l)}\)聚合。
尤其，计算公式 @ref(eq:homp2)中的 \(m_x\) 相当于在共链
\(\{\mathbf{m}_j^k\}_{k=1}^n\)上应用\(n-1\)次
\$\mathcal{M}\_\{Id\_k,Id\_k\}\colon\mathcal{C}\^{}j
\(的聚合节点操作。 很明显，首先通过聚合\)\mathbf{m}\_j\^{}1\$和
\(\mathbf{m}_j^2\) 来获得
\(\mathbf{n}_j^1=\mathcal{M}_{Id_j,Id_j}(\mathbf{m}_j^1,\mathbf{m}_j^2)\)。然后，聚合\(j\)-cochain
\(\mathbf{n}_j^1\) 和 \(j\)-cochain
\(\mathbf{m}_j^3\)，等等。最后的聚合节点是执行聚合操作
\(\mathbf{n}_j^{n-1}=\mathcal{M}_{Id_j,Id_j}(\mathbf{n}_j^{n-2},\mathbf{m}_j^n)\)，
也即
\(\mathbf{m}_j = [ m_{x_1^j},\ldots, m_{x_{|\mathcal{X}^j|}^j }]\)\footnote{请注意，虽然我们对所有聚合节点都使用了相同的符号
  \(\mathcal{M}_{Id_k,Id_k}\)，但这些节点一般都有不同的参数。}.
最后，计算由聚合节点\(\mathcal{M}_{(Id_j,Id_j)}(\mathbf{m}_j, \mathbf{X}_j^{(l)})\)（该计算由公式@ref(eq:homp3)中的函数\(\beta\)确定）实现的
\(\mathbf{X}_j^{(l+1)}\) 。

定理
@ref(thm:mn)表明CCs上定义的高阶消息传递网络可以从基本张量操作来构造，
因此它们是 CCNN
的特例。我们在定理@ref(thm:thm-unifying)中正式阐述了这一结果。

\phantomsection\label{thm-unifying}
高阶信息传递神经网络就是 CCNN。

从定义 @ref(def:hmpsnn)和定理 @ref(thm:mn)可以立即得出该结论。

由定理 @ref(thm:thm-unifying)可知，定义在通用性不如 CCs
的高阶域（如单纯复形、胞腔复形和超图）上的高阶消息传递神经网络也是 CCNNs
的特例。因此，定义@ref(def:tdd)中引入的张量图形成了一种通用的图解方法，用于表达定义在常用高阶域上的神经网络。

\phantomsection\label{unifying1}
Message-passing neural networks defined on simplicial complexes, cell
complexes or hypergraphs can be expressed in terms of tensor diagrams
and their computations can be realized in terms of the three elementary
tensor operators.

从两点可得出这一结论，即：\href{mailto:定理@ref}{\nolinkurl{定理@ref}}(thm:thm-unifying)，以及单纯复形、胞腔复形和超图可看作是
CCs 的特例实现。

定理 @ref(thm:thm-unifying) 和 @ref(thm:unifying1)
基于张量图提出了一个统一的TDL框架，从而为今后的发展提供了空间。 例如,
文献(\citeproc{ref-mathilde2023}{Papillon et al. 2023})
已经利基于该框架，用张量图来表达了单纯复形、胞腔复形和超图等现有的 TDL
架构。

\subsection{聚合节点和高阶消息传递：量化比较}\label{merge-nodes-and-higher-order-message-passing-a-qualitative-comparison}

定义
@ref(def:homp-definition)中给出的高阶消息传递提供了一种更新规则，可以使用由
\(\mathcal{N}(x)\) 确定的一组邻域向量 \(\mathbf{h}_y^{l}\) 从向量
\(\mathbf{h}_x^{l}\) 获得向量
\(\mathbf{h}_x^{l+1}\)。显然，这个计算框架假定向量
\(\mathbf{h}_x^{(l)}\) 和 \(\mathbf{h}_{y}^{(l)}\) 是作为输入提供的。
换句话说，根据定义 @ref(def:homp-definition)需要目标域中的
共链\(\mathbf{X}_j^{(l)} \in \mathcal{C}^{j}\)，以及共链
\(\mathbf{Y}_{i_k}^{(l)} \in \mathcal{C}^{i_k}\)，以便计算更新后的
\(j-\)cochain \(\mathbf{X}_j^{(l+1)}\) 。
另一方面，进行聚合节点计算需要一个共链向量
\((\mathbf{H}_{i_1},\mathbf{H}_{i_2})\)，这可以从公式 @ref(eq:sum)和定义
@ref(def:exact-definition-merge-node)中看出。

这两种计算框架之间的差异可能看起来是符号性的，并且消息传递的视角可能看起来更直观，尤其是在处理基于图的模型时。然而，我们认为，在存在自定义高阶网络架构的情况下，聚合节点框架在计算上更自然、更灵活。为了说明这一点，我们考虑一下图
@ref(fig:merge-example)中可视化的示例。

在图@ref(fig:merge-example)中显示的神经网络有一个共链输入向量
\((\mathbf{H}_0,\mathbf{H}_2) \in  \mathcal{C}^0 \times \mathcal{C}^2\)，第一层的神经网络负责计算共链\(\mathbf{H}_1 \in \mathcal{C}^1\)，第二层的神经网络负责计算共链
\(\mathbf{H}_3\in \mathcal{C}^3\)。为了在第一层获得共链\(\mathbf{H}_1\)，需要考察由\(B_{0,1}^T\)
和 \(B_{1,2}\)诱导的邻域函数。 然而，如果使用公式 @ref(eq:homp0) 和
@ref(eq:homp1)执行在图@ref(fig:merge-example)中张量图第一层确定的计算时，那么应该注意到在\(\mathcal{C}^1\)上并没有提供共链来作为输入的一部分。因此，当使用公式@ref(eq:homp0)
和@ref(eq:homp1)时，需要特殊处理，因为向量 \(\mathbf{h}_{x^1_j}\)
还没有计算出来。 请注意，GNN
中不存在这种人工制品，因为它们经常更新节点特征，而节点特征通常是作为输入的一部分提供的。具体来说，在
GNN 中，公式 @ref(eq:homp0)更新规则中的前两个参数是在底层图的
0-cells上支持的共链。

\begin{figure}

{\centering \includegraphics{figures/merge_example_scaled} 

}

\caption{图中描述的神经网络可以实现为两个聚合节点的组合。更具体的说，输入是共链向量 $(\mathbf{H}_0,\mathbf{H}_2)$，第一个聚合节点负责计算1-chain $\mathbf{H}_1 = \mathcal{M}_{B_{0,1}^T,B_{1,2}} (\mathbf{H}_0,\mathbf{H}_2)$；第二个聚合节点 $\mathcal{M}_{B_{1,3}^T, B_{2,3}} \colon\mathcal{C}^1 \times \mathcal{C}^2 \to \mathcal{C}^3$负责计算3-cochain $\mathbf{H}_3=\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\mathbf{H}_1,\mathbf{H}_2)$。从聚合节点的角度来看，可以计算出 1-cells 胞腔和 3-cells 胞腔支持的共链，而无需这些胞腔的初始共链。另一方面，高阶消息传递框架有两个主要限制：一是假定输入域所有维度所有胞腔上都有支持的初始共链，二是假定每次迭代都能更新输入域所有维度所有胞腔上所支持的所有共链。}\label{fig:merge-example}
\end{figure}

类似的，为了计算图@ref(fig:merge-example)中第二层的共链\(\mathbf{H}_3 \in  \mathcal{C}^3\)，就必须考虑由\(B_{1,3}^T\)
and
\(B_{2,3}\)诱导的邻域函数，而且必须使用共链向量\((\mathbf{H}_1,\mathbf{H}_2)\)。
这意味着图@ref(fig:merge-example)中给出的神经网络计算得出的
共链\(\mathbf{H}_1\) 和 \(\mathbf{H}_3\) 不能通过迭代过程得到。
此外，输入向量 \(\mathbf{H}_0\) 和\(\mathbf{H}_2\)
在迭代过程的任何一步都不会被更新，结果就导致共链 \(\mathbf{H}_1\) 和
\(\mathbf{H}_3\)
也从不会被更新。从更新规则（如高阶消息传递框架中出现的更新规则）的角度来看（定义@ref(def:homp-definition)），这种设置是不自然的，因为它假定所有维度的\emph{所有胞腔上的初始共链}都可作为输入，并在每次迭代时都要对输入域的的复形上\emph{所有胞腔所支持的所有共链进行更新}。

事实上，如果在使用高阶消息传递框架时遇到这样的困难，可以使用一些临时的解决办法，比如通过开启或关闭某些共链的迭代，或通过引入辅助共链来解决，聚合节点就是为了克服这些限制而设计的。
尤其,
从聚合节点的角度来看，我们可以把图@ref(fig:merge-example)的第一层看作是函数\(\mathcal{M}_{B_{0,1}^T,B_{1,2}}\colon \mathcal{C}^0 \times \mathcal{C}^1 \to \mathcal{C}^1\)，参见公式@ref(eq:functional)。函数
\(\mathcal{M}_{B_{0,1}^T,B_{1,2}}\) 把共链向量
\((\mathbf{H}_0,\mathbf{H}_2)\)作为输入，然后来计算1-chain
\(\mathbf{H}_1 = \mathcal{M}_{B_{0,1}^T,B_{1,2}} (\mathbf{H}_0,\mathbf{H}_2)\)。
类似的，可以用聚合节点\(\mathcal{M}_{B_{1,3}^T, B_{2,3}} \colon
\mathcal{C}^1 \times \mathcal{C}^2 \to \mathcal{C}^3\)来计算3-cochain
\(\mathbf{H}_3=\mathcal{M}_{B_{1,3}^T, B_{2,3}}(\mathbf{H}_1,\mathbf{H}_2)\)。

\subsection{注意力高阶消息传递和CCANNs}\label{attention-higher-order-message-passing-and-ccanns}

在这里，我们展示了高阶消息传递（定义 @ref(def:homp-definition)）与
CCANNs（章节@ref(combinatorial-complex-attention-neural-networks)）之间的联系。我们将首先介绍一个注意力版本的定义
@ref(def:homp-definition)。

\phantomsection\label{attention-homp}
令
\(\mathcal{X}\)是CC，\(\mathcal{N}=\{ \mathcal{N}_1,\ldots,\mathcal{N}_n\}\)是定义在\(\mathcal{X}\)上的邻域函数\(\mathcal{X}\)，\(x\)是胞腔，并且对于某个\(\mathcal{N}_k \in \mathcal{N}\)，存在\(y\in \mathcal{N}_k(x)\)。所谓胞腔\(x\)和\(y\)之间的\emph{消息}\(m_{x,y}\)就是要完成某种计算，它依赖于两个胞腔或他们所支持的数据。用\(\mathcal{N}(x)\)表示多集
\(\{\!\!\{ \mathcal{N}_1(x) , \ldots ,  \mathcal{N}_n (x) \}\!\!\}\)，\(\mathbf{h}_x^{(l)}\)表示层\(l\)的胞腔\(x\)支持的数据。\(\mathcal{N}\)诱导的\(\mathcal{X}\)上的\emph{注意力高阶消息传递}可沟通过如下四条规则来定义：
\begin{align}
m_{x,y} &= \alpha_{\mathcal{N}_k}(\mathbf{h}_x^{(l)},\mathbf{h}_y^{(l)}), (\#eq:ahomp0) \\
m_{x}^k &=  \bigoplus_{y \in \mathcal{N}_k(x)} a^k(x,y)  m_{x,y}, \; 1\leq k \leq n , (\#eq:ahomp1) \\
m_{x} &=  \bigotimes_{ \mathcal{N}_k \in \mathcal{N} } b^k m_x^k , (\#eq:ahomp2) \\
\mathbf{h}_x^{(l+1)} &= \beta (\mathbf{h}_x^{(l)}, m_x) . (\#eq:ahomp3)
\end{align} 其中, \(a^k \colon \{x\} \times \mathcal{N}_k(x)\to [0,1]\)
是高阶注意力函数(参见定义@ref(def:hoa)), \(b^k\)
是满足\(\sum_{k=1}^n b^k=1\)的可训练的注意力权重, \(\bigoplus\)
是置换不变性聚合函数，\(\bigotimes\) 是一般的聚合函数，
\(\alpha_{\mathcal{N}_k}\) 和 \(\beta\) 是可微函数。

定义@ref(def:attention-homp) 区别了两类，第一种由函数
\(a^k\)确定，公式@ref(eq:ahomp1)的权重 \(a^k(x,y)\)依赖于邻域函数
\(\mathcal{N}_k\)和胞腔 \(x\) and \(y\)。更进一步，\(a^k(x,y)\)确定了
确定了由邻域函数 \(\mathcal{N}_k\) 决定的胞腔 \(x\) 对其周围邻域
\(y\in\mathcal{N}_k\)
的关注度。在章节@ref(combinatorial-complex-attention-neural-networks)提到的CC-attention前推操作由这些权重来特定的参数化实现。另一方面，公式@ref(eq:ahomp2)的权重
\(b^k\) 仅是邻域\(\mathcal{N}_k\)的函数， 并由此决定了胞腔 \(x\)
对从每个邻域函数 \(\mathcal{N}_k\)
中获得的消息的关注程度。在章节@ref(combinatorial-complex-attention-neural-networks)给出的CC-attention前推操作中，我们设置\(b^k\)等于1。然而，聚合节点的概念（参见定义
@ref(def:exact-definition-merge-node)）可以很容易地扩展为到引入相应的\emph{注意力聚合节点}的概念，这反过来又可以用来在实践中实现公式
@ref(eq:ahomp2)。 请注意，由权重 \(b^k\)
决定的注意力是高阶领域所独有的，在基于图的注意力模型中不会出现。

\section{前推、池化和反池化}\label{push-forward-pooling-and-unpooling}

本节将展示如何利用定义@ref(def:pushing-exact-definition)的前推操作来实现对CCc的（反）池化操作，随后接着介绍CCNNs的（反）池化操作。此外，本节还展示了基于
CC 的池化如何为基于图像和图的池化提供统一框架，以及 CC
s上的保形池化(shape-preserving pooling)与图上的映射器是如何相关的。

特别是，我们建立了另一个统一的数学原理：作为消息传递的池化，可以从根本上由前推操作（push-forward）构建而成。因此，前推操作构成了主要的基本构件，所有高阶计算都可以从中实现。这种实现方式非常重要，因为它为在复形上建立统一的深度学习应用编程接口（API）奠定了数学基础，该接口将池化和基于消息传递的计算结合为单一操作。事实上，在我们贡献的
Python 软件包之一
\href{https://github.com/pyt-team/TopoModelX}{TopoModelX}中，高阶消息传递和池化/反池化操作就是作为一个单一函数在各种拓扑域中实现的。

\subsection{CC池化和反池化}\label{cc-pooling-and-unpooling}

我们定义了一种基于 CC
的池化操作，它扩展了基于图像和基于图的池化操作的主要特点。
具体来说，我们建立了一种池化操作，可以 ``下采样（downscales）''CC
\(\mbox{X}\) 上支持的信号大小。 To
为此，我们利用CCs自然的层次性，将池化操作定义为由共链映射
\(G\colon\mathcal{C}^{i}(\mathcal{X})\to \mathcal{C}^{j}(\mathcal{X})\)
引导的前推操作，该共链映射将\(i\)-cochain推向\(j\)-cochain。为了得到一个有用的池化操作，这需要缩减其输入
\(i\)-cochain 的大小，我们施加了 \(j>i\) 的约束。定义
@ref(def:pooling-exact-definition)实现了我们对 CC 池化的设想。图
@ref(fig:pooling-hoans)形象地展示了定义@ref(def:pooling-exact-definition)背后的直觉。尤其是，图@ref(fig:pooling-hoans)展示了在三维
CC 上支持的共链上连续应用池操化作的例子。

\phantomsection\label{pooling-exact-definition}
令
\(\mbox{X}\)是一个CC，\(G\colon\mathcal{C}^{i}( \mbox{X})\to \mathcal{C}^{j}( \mbox{X})\)是共链映射，则如果
\(j>i\)，那么\(G\)引导的前推操作被称为\emph{CC池化操作}。

\begin{figure}

{\centering \includegraphics{figures/pooling} 

}

\caption{连续CC（反）池化操作的示例. CC 池化操作利用了底层 CC 的分层结构，通过将低阶共链推向高阶胞腔来粗化低阶共链，从而提高对某些结构扭曲的不变性能力。图中粉色、蓝色和绿色胞腔分别有秩1、2、3，图（a）是维度为3的CC $\mbox{X}$。在$\mbox{X}$上，我们将考察三个共链操作: $B_{0,1} \colon\mathcal{C}^1\to\mathcal{C}^0$, $B_{1,2} \colon\mathcal{C}^2\to \mathcal{C}^1$ 和 $B_{2,3}\colon\mathcal{C}^3\to\mathcal{C}^2$。 对于图（b）、（c）、（d）中的顶行，假定首先给定0-cochain $\mathbf{H}_0$；而对于这三幅图的底部行，则假定首先给定3-cochain$\mathbf{H}_3$。例如，在图（b）的顶部行，输入0-cochain $\mathbf{H}_0$ 通过函数$\mathcal{F}_{B_{0,1}^T}\colon\mathcal{C}^0\to \mathcal{C}^1$前推到1-cochain $\mathbf{H}_1$。$B_{0,1}^T$诱导的前推操作就是CC-pooling操作，因为该操作发送0-cochain到了高阶共链。在图（b）的底部行，$B_{0,1}$诱导的前推操作 $\mathcal{F}_{B_{0,1}}\colon \mathcal{C}^1\to \mathcal{C}^0$则是反池化操作，因为该操作发送 $1$-cochain 到了 $0$-cochain。图 (c) 和 (d)和图（b）类似，这些图就说明了$B_{1,2}^T$和$B_{2,3}^T$ (顶部)诱导的池化操作, 以及$B_{1,2}$ 和 $B_{2,3}$诱导的反池化操作。}\label{fig:pooling-hoans}
\end{figure}

在定义@ref(def:unpooling-exact-definition)中，我们引入了CCs上的反池化操作来将共链前推到低秩共链，图@ref(fig:pooling-hoans)给出了CCs上的反池化操作示例。

\phantomsection\label{unpooling-exact-definition}
令\(\mbox{X}\)是CC，\(G\colon\mathcal{C}^i( \mbox{X})\to \mathcal{C}^j( \mbox{X})\)是共链映射。如果\(j<i\)，那么称\(G\)
诱导的前推操为\emph{CC反池化操作（CC-unpooling operation）}。

\subsection{将常见的池化操作表述为
CC-pooling}\label{formulating-common-pooling-operations-as-cc-pooling}

在本节中，我们将用 CC
池化来表述常见的池化操作。特别是，我们将证明图形和图像池可以被视为 CC
池化。

\subsubsection{用CC-pooling表示图池化操作}\label{graph-pooling-as-cc-pooling}

在此，我们将简要说明CC-pooing操作（参见定义@ref(def:pooling-exact-definition)）和基于图的池化算法是一致的。令\(\mathbf{H}_0\)定义在图\(\mathcal{G}\)的顶点和边上的共链，\(\mathbf{H}^{\prime}_0\)是定义在\(\mathcal{G}\)的粗粒度版本\(\mathcal{G}^{\prime }\)上的共链。在这种情况下，\(\mathbf{H}^{\prime}_0\)就是\(\mathbf{H}_0\)的粗粒度版，元组
\((\mathcal{G},\mathbf{H}_0)\)上支持的图池化函数是\(\mathcal{POOL} \colon (\mathcal{G},\mathbf{H}_0) \to (\mathcal{G}^{\prime},\mathbf{H}^{\prime}_0)\)，该函数发送\(\mathcal{G}\)中的每个顶点到\(\mathcal{G}^{\prime}\)中的每一个顶点，这对应于\(\mathcal{G}\)上顶点的聚类操作。接下来，将正式阐述怎么用CC-pooling来实现\(\mathcal{POOL}\)函数。

\phantomsection\label{pool}
函数\(\mathcal{POOL}\)可用CC-pooling操作实现。

图\(\mathcal{G}^{\prime}\)中的每个顶点都可表示原图\(\mathcal{G}\)中的一个顶点聚类。
利用这些聚类的成员元素，我们能够通过一个 2-cells 胞腔集合来扩展
\(\mathcal{G}\) 以构建 CC，这样得到的每个胞腔都对应于
\(\mathcal{G}^{\prime}\) 的一个超节点。我们用
\(\mathcal{X}_{\mathcal{G}}\) 来表示由此产生的 CC 结构，它由 2-cells
胞腔增强的 \(\mathcal{G}\)
组成。因此，任何定义在\(\mathcal{G}^{\prime}\)上的0-cochain
\(\mathbf{H}^{\prime}_0\)都能写作2-cochain
\(\mathbf{H}_2 \in \mathcal{C}^2(\mathcal{X}_{\mathcal{G}})\)。在原图
\(\mathcal{G}\)的顶点和池化图\(\mathcal{G}^{\prime}\)的顶点，或其等价描述形式CC
\(\mathcal{X}_{\mathcal{G}}\)的顶点间的关系，都可以通过关联矩阵\(B_{0,2}^T\)来描述。因此，为了学习信号\(\mathbf{H}_2\)可以用映射\(B_{0,2}^T \colon \mathcal{C}^{2} (\mathcal{X}_{\mathcal{G}}) \to \mathcal{C}^{0}(\mathcal{X}_{\mathcal{G}})\)来实现，该操作把共链\(\mathbf{H}_0\)
推向 \(\mathbf{H}_2\)。

定义\(\mathcal{X}_{\mathcal{G}}\)中的2-cells可以用 \emph{图映射（mapper
on graphs）}工具 (\citeproc{ref-hajij2018mog}{Hajij, Wang, and Rosen
2018})来实现,
它是一个TDA分类工，更详细的构造方法参见章节@ref(mapper-and-the-cc-pooling-operation)。

\subsubsection{图像池化作为CC-pooing}\label{image-pooling-as-cc-pooling}

由于图像可以实现为网格图(lattice graphs)，因此存储在图像网格(image
grid)上的信号可以实现为与图像相对应的网格图的
0-cochain，参见图@ref(fig:image-pooling)(a--b)。在此，我们将阐明
CC-pooling（定义@ref(def:pooling-exact-definition)）与已知的图像池化的定义是一致的。事实上，可以用2-cells来增强@ref(fig:image-pooling)(b)中的网格图，正如图
@ref(fig:image-pooling)(c)所示，以便执行图像池化操作。通常，这些胞腔都有规则的窗口大小。在图@ref(fig:image-pooling)(c)中，我们选择的池化窗口大小，或者说是
2-cell的大小，是 \(2\times 2\)，池化步长为 1。
这种情况下的图像池化操作可以通过共链映射\(B_{0,2}^T \colon\mathcal{C}^0 \to \mathcal{C}^2\)诱导的CC-pooling来实现，如图
@ref(fig:image-pooling)(d).所示。我们在下面的命题中正式正式阐述这一点。

\begin{figure}

{\centering \includegraphics{figures/image_pooling} 

}

\caption{用CC-pooling实现图像池化。 (a): 尺寸为$3\times3$的图像。 (b): 与图（a）对应的网格图。(c): 2-cells增强的网格图。选择 (c) 中的那些特定胞腔，相当于选择图像池化窗口大小为$2\times 2$ ，池化步长为 1。 (d): 执行图像池化计算等价于执行由共链映射$B_{0,2}^T \colon\mathcal{C}^0 \to \mathcal{C}^2$诱导的CC-pooling操作，该共链映射前推图像信号（$\mathcal{X}^2$支持的$0$-cochain）到$\mathcal{X}^2$支持的信号。}\label{fig:image-pooling}
\end{figure}

\phantomsection\label{image-pool}
图像池化操作可以通过从底层图像域到二维 CC 的前推操作来实现，该二维
CC是通过适当的二维胞腔对图像进行增强而得到的，图像池化计算就在该二维胞腔中进行。

从图像池化的定义中可以直接得出结论。

\subsection{池化与反池化CCNNs}\label{pooling-and-unpooling-ccnns}

定义@ref(def:pooling-exact-definition)中的池化操作仅考虑了CCNN的张量图仅有单一边的特殊情况，在下文中，我们将通过确定
CCNN 作为池化 CCNN 的定义属性来概括池化概念。为此，我们从张量图高度为 1
的 CCNN 开始讲解。

\phantomsection\label{height-1-pooling}
对于高度为1的张量图\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)表示的CCNN，令\(\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\)是CCNN的域，令\(\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)是CCNN的共域（codomain），令\(i_{min}=min(i_1,\ldots,i_m)\)和\(j_{min}=min(j_1,\ldots,j_n)\)。如果满足如下条件，则说CCNN是\emph{高度为1的池化CCNN}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(i_{min}< j_{min}\), 且
\item
  张量图\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)
  对于某个\(k\geq j_{min}\)，有共链操作\(G\colon \mathcal{C}^{i_{min}} \to \mathcal{C}^{k}\)标记的边。
\end{enumerate}

直观地说，如果一个高度为1的张量图所代表的 CCNN
将其最低秩的信号推送给较高等级的胞腔，那么它就是高度为1的池化
CCNN。请注意，读出操作可以作为高度为1的池化 CCNN 来实现，见图
@ref(fig:readout)中的说明。

\begin{figure}

{\centering \includegraphics{figures/read_out} 

}

\caption{池化操作示例。在这个例子中，展示了一个维度为3的CC。秩为 3 的胞腔（绿色表示）在 CC中的所有胞腔中秩最高。(a): 高度为 1 的池化 CCNN 会将一个共链向量$(\mathbf{H}_0,\mathbf{H}_1)$ 池化为一个 2-cochain $\mathbf{H}_2$ 。 (b):读出操作可以作为高度为 1 的池化 CCNN来实现，方法是由一个秩数高于CC中所有其他胞腔的单个（绿色）胞腔封装整个CC，并将低秩胞腔的所有信号池化（读出）到封装（绿色）胞腔。}\label{fig:readout}
\end{figure}

CCNN
可能不会在每一层都执行池化操作，它可能会保全最低秩信号的维度。在给出池化
CCNN 的一般性定义之前，我们首先定义高度为1的最低秩保留 （rank-preserving
）CCNN。

\phantomsection\label{lowest-rank}
对于由高度为1的张量图表示的CCNN，令
\(\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\)是CCNN的域，令
\(\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)
是CCNN的共域，令 \(i_{min}=min(i_1,\ldots,i_m)\)
且\(j_{min}=min(j_1,\ldots,i_n)\)。如果
\(i_{min}= j_{min}\)，那么就说CCNN高度为1的最低秩保全CCNN。

每个 CCNN 都是由高度为1的张量图表示的 CCNN 组成的。因此，正如定义
@ref(def:general-pooling-hoan)所阐述的，池化 CCNN
可以用高度为1的张量图来表征。

\phantomsection\label{general-pooling-hoan}
令 \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)是CCNN的张量图表示，
则可将CCNN分解作 \begin{equation*}
\mbox{CCNN}_{\mathbf{G};\mathbf{W}}=
\mbox{CCNN}_{\mathbf{G}_N;\mathbf{W}_N} \circ \cdots \circ \mbox{CCNN}_{\mathbf{G}_1;\mathbf{W}_1},
\end{equation*} 其中，
\(\mbox{CCNN}_{\mathbf{G}_i;\mathbf{W}_i},i=1,\ldots,N\),
是高度为1的张量图，表示了CCNN的第\(i\)-th层，并且
\(\mathbf{G}_i \subseteq \mathbf{G}\)。称
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)表示的CCNN是\emph{池化
CCNN}，如果：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  每个 \(\mbox{CCNN}_{\mathbf{G}_i;\mathbf{W}_i}\)
  都是高度为1的池化CCNN，或者是高度为1的最低秩保全CCNN，且
\item
  在CCNN的层
  \(\mbox{CCNN}_{\mathbf{G}_i;\mathbf{W}_i}\)中至少有一个是高度为1的池化CCNN。
  \textbackslash end\{enumerate\}
\end{enumerate}

直观地说，池化CCNN 是一种 CCNN，其张量图形成了一个
``阶梯''，将信号推送到每一层的高秩胞腔。图 @ref(fig:tensor)(d)
给出了一个高度为二的池化 CCNN 的例子。

高度为1的反池化CCNN的定义类似高度为1的池化CCNN(参见定义@ref(def:height-1-pooling))，唯一的不同是不等式\(i_{min}<j_{min}\)
变成了 \(i_{min}>j_{min}\)。除此之外，反池化 CCNN
(参见定义@ref(def:general-unpooling-hoan)) 的定义都类似于池化CCNN
(定义@ref(def:general-pooling-hoan))。

\phantomsection\label{general-unpooling-hoan}
令
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)是CCNN的张量图表示，那么可解耦CCNN为
\begin{equation*}
\mbox{CCNN}_{\mathbf{G};\mathbf{W}}=
\mbox{CCNN}_{\mathbf{G}_N;\mathbf{W}_N} \circ \cdots \circ \mbox{CCNN}_{\mathbf{G}_1;\mathbf{W}_1},
\end{equation*}
其中，\(\mbox{CCNN}_{\mathbf{G}_i;\mathbf{W}_i},i=1,\ldots,N\),
是表征CCNN第\(i\)-th层的高度为1的张量图，并且
\(\mathbf{G}_i \subseteq \mathbf{G}\)，则称\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)表征的CCNN是反池化CCNN，如果：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  每个\(\mbox{CCNN}_{\mathbf{G}_i;\mathbf{W}_i}\)
  都要么是高度为1的反池化CCNN，要么是高度为1的最低秩保全CCNN，且
\item
  在CCNN的层
  \(\mbox{CCNN}_{\mathbf{G}_i;\mathbf{W}_i}\)中至少有一个是高度为1的反池化CCNN。
\end{enumerate}

\subsection{映射器和CC池化操作}\label{mapper-and-the-cc-pooling-operation}

在实际使用时，在高阶域上构建有用的 CC 池化操作取决于输入 CC
中的高秩胞腔。
与基于图像的模型类似，CC-池化操作可在高阶网络的末端依次应用，以提供输入域的汇总表示，参见图@ref(fig:pooling-hoans)。
输入的 CC 中可能并没有这样的分层摘要，例如, 如果
\(\mathcal{X}\)是图，那么一个CC-pooling操作，正如定义@ref(def:pooling-exact-definition)中给出的那样,
可能仅仅是前推输入的节点信号到边信号，而并不总是能提供输入信号的简明摘要。

在这种情况下，我们可以用一组维度为 \(\dim(\mathcal{X})+1\)
的新胞腔来\emph{增强输入 CC} \(\mathcal{X}\) ，以使得新胞腔近似于输入 CC
\(\mathcal{X}\) 的形状。图@ref(fig:graph-tda) 给出了一个增强图
\(\mathcal{X}\)的示例，它是一个维度为1的CC，其新胞腔的维度为2，新胞腔根据文献(\citeproc{ref-dey2016multiscale}{Dey,
Mémoli, and Wang 2016}; \citeproc{ref-singh2007topological}{Singh et al.
2007}; \citeproc{ref-hajij2018mog}{Hajij, Wang, and Rosen
2018})\footnote{使用mapper算法来图骨架化（graph
  skeletonization）的方法(\citeproc{ref-singh2007topological}{Singh et
  al. 2007})已经在文献(\citeproc{ref-dey2016multiscale}{Dey, Mémoli, and
  Wang
  2016})中研究过，我们的实现和讨论依赖于文献(\citeproc{ref-hajij2018mog}{Hajij,
  Wang, and Rosen 2018})中提到的概念。}中的方法使用\emph{图mapper(mapper
on graph, MOG)}来构造。通过 MOG
构造方法得到的增强高秩胞腔概要了底层图的形状特征，这是一种理想的池化特征（例如在形状分析中）。关于
MOG 构建的拓扑保全（ topology-preserving） CC 池化操作的详情，请参阅附录
@ref(a-mapper-induced-topology-preserving-cc-pooling-operation) 。

\begin{figure}

{\centering \includegraphics{figures/graph_tda} 

}

\caption{构造CC$\mathcal{X}$上形状保全（shape-preserving）的池化操作示例。在此，我们将演示 $\mathcal{X}$ 是图的情况。构建方法使用*图mapper（mapper on graphs，MOG）*[@singh2007topological; @hajij2018mog]，它是一种图骨架化算法，可用于为 $\mathcal{X}$ 增添秩为 2 的拓扑保全胞腔（topology-preserving）。（a）输入图$\mathcal{X}$是维度为1的CC。（b）MOG算法的输入需要三个参数：图$\mathcal{X}$，特征保全标量函数$g\colon\mathcal{X}^0\to [a,b]$，范围$[a,b]$的覆盖（cover）$\mathcal{U}$（覆盖闭区间$[a,b]$的开集的集合）。标量函数$g$用于将范围 $[a,b]$ 上的覆盖 $\mathcal{U}$ 拉回到 $\mathcal{X}$ 上的覆盖, 图 (b) 中节点的颜色表示 $g$ 的标量值。图（b）中，$\mathcal{X}$被分裂为四段，每段都对应于$\mathcal{U}$的一个覆盖元素。（c）图中显示了拉回的覆盖元素的连接组件，每个相连的分量都被一个蓝色胞腔所包围，每个蓝色胞腔也都被视为秩为 2 的单元。蓝色胞腔用来增强$\mathcal{X}$，以形成维度2的CC，增强后的$\mathcal{X}$由原$\mathcal{X}$上的0-cells、1-cells，以及增强的2-cells来组成实现。增强的CC用$\mathcal{X}_{g,\mathcal{U}}$来标记。(d): MOG 算法构建了一个图，其节点是覆盖 $\mathcal{U}$的回拉过程中每个覆盖元素所包含的连通成分，边由这些连通成分组成。换句话说，MOG 生成的图概要了通过 MOG 算法添加的秩为 2的增强胞腔之间的连接情况。图（d）中给出的MOG生成图的邻接矩阵等价于$\mathcal{X}_{g,\mathcal{U}}$的胞腔$A_{2,2}$的邻接矩阵。当且仅当 $\mathcal{X}$ 中存在的 2-cells胞腔相交于一个节点时（后者发生的前提是当且仅当 MOG生成的图的节点之间存在一条边时），它们才是 2 相邻（adjacent）的 。给定CC结构，共链映射$B_{0,2}^T\colon\mathcal{C}^0(X_{g,\mathcal{U}})\to \mathcal{C}^2(X_{g,\mathcal{U}})$可用于诱导形状保全的操作。此外，$\mathcal{X}$顶点集上支持的信号 $\mathbf{H}_0$可以被前推和池化为增强2-cells上支持的信号$\mathbf{H}_2$。这里给出的图受了文献 [@hajij2018mog]的启发。}\label{fig:graph-tda}
\end{figure}

\section{CCNNs的Hasse图解释}\label{hasse-graph-interpretation-of-ccnns-1}

在本节中，我们将通过与已有的
GNN研究成果建立联系，仔细研究拓扑学习机的特性。首先将CCs解释为专门的图，例如哈斯图（\emph{Hasse
graphs}），然后针对置换和方向的作用描述它们的等变特性，还将进一步将等变性定义与Hasse图表示法下的传统等变定义联系起来。

\begin{quote}
译者注：Hasse
graphs，用来表示有限偏序集的一种数学图表，它是一种图形形式的对偏序集的传递简约。具体的说，对于偏序集合\((S, ≤)\)，把\(S\)的每个元素表示为平面上的顶点，并绘制从\(x\)到\(y\)向上的线段或弧线，只要\(y\)
覆盖\(x\)(就是说，只要\(x < y\)并且没有\(z\)使得\(x < z < y\))。这些弧线可以相互交叉但不能触及任何非其端点的顶点。带有标注的顶点的这种图唯一确定这个集合的偏序。
\end{quote}

\subsection{CCNNs的Hasse图解释}\label{hasse-graph-interpretation-of-ccnns-2}

我们将首先阐明，每个 CC 都可以规约为一个唯一且特定的图，即 \emph{Hasse
图}。通过这种规约，就可以用基于图的模型来分析和理解 CCNN
的各种计算和概念。

\subsubsection{CCs作为Hasse图}\label{ccs-as-hasse-graphs}

定义 @ref(def:maps)表明CC是偏序的（poset）,
是一个部分有序集合，其部分有序关系是集合包含关系。
这也意味着，当且仅当两个 CC 的 偏序关系
等价时，它们才是等价的\footnote{对于相关结构 (例如,
  单纯形/胞腔/胞腔复形), 偏序关系一般称为 \emph{面偏序（face poset）}
  (\citeproc{ref-wachs2006poset}{Wachs 2006})。}。 定义@ref(def:hg)
给CC引入了\emph{Hasse图} (\citeproc{ref-wachs2006poset}{Wachs 2006};
\citeproc{ref-abramenko2008buildings}{Abramenko and Brown
2008})，它是和有限偏序关系关联的有向图。

\phantomsection\label{hg}
CC \((S, \mathcal{X},\mbox{rk})\) 的\emph{Hasse图}是有向图
\(\mathcal{H}_{\mathcal{X}}= (V (\mathcal{H}_{\mathcal{X}}), E(\mathcal{H}_{\mathcal{X}}) )\)，顶点集为\(V (\mathcal{H}_{\mathcal{X}})=\mathcal{X}\)，边集为
\(E(\mathcal{H}_{\mathcal{X}})=\{ (x,y) : x\subsetneq y, \mbox{rk}(x)=\mbox{rk}(y)-1 \}\)。

CC \((S, \mathcal{X},\mbox{rk})\)的Hasse图
\(\mathcal{H}_{\mathcal{X}}\)的顶点是\(\mathcal{X}\)中的胞腔，\(\mathcal{H}_{\mathcal{X}}\)的边由胞腔间的直接互连关系（immediate
incidence）确定，图 @ref(fig:hasse-diagram) 提供了一个CC的Hasse图示例。

\begin{figure}

{\centering \includegraphics{figures/poset} 

}

\caption{Example of the Hasse graph of a CC. (a): CC of a M"{o}bius strip. (b): Hasse graph of the CC, describing the poset structure between cells. (c): Hasse graph augmented with the edges defined via $A_{0,1}$ and $coA_{2,1}$.}\label{fig:hasse-diagram}
\end{figure}

The \emph{CC structure class} is the set of CCs determined up to
isomorphism, according to Definition @ref(def:maps). Proposition
@ref(prp:structure) provides sufficient criteria for determining CC
structure classes. The proof of Proposition @ref(prp:structure) relies
on the observations that CC structure classes are determined by the
underlying Hasse graph representation and that the Hasse graph provides
the same information as the incidence matrices
\(\{B_{k,k+1}\}_{k=0}^{\dim(\mathcal{X})-1}\). Figure
@ref(fig:vis-structure) supports visually the proofs of parts 2 and 3 in
Proposition @ref(prp:structure).

\phantomsection\label{structure}
Let \((S, \mathcal{X},\mbox{rk})\) be a CC. For the CC structure class
indicated by \((S, \mathcal{X},\mbox{rk})\), the following sufficient
conditions hold:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The CC structure class is determined by the incidence matrices
  \(\{B_{k,k+1}\}_{k=0}^{ \dim(\mathcal{X}) -1}\).
\item
  The CC structure class is determined by the adjacency matrices
  \(\{A_{k,1}\}_{k=0}^{\dim(\mathcal{X})-1}\).
\item
  The CC structure class is determined by the coadjacency matrices
  \(\{coA_{k,1}\}_{k=1}^{\dim(\mathcal{X})}\).
\end{enumerate}

The proof of the three parts of the proposition follows by noting that
the structure of a CC is determined completely by its Hasse graph
representation. The first part of the proposition follows from the fact
that the edges in the Hasse graph are precisely the non-zero entries of
matrices \(\{B_{k,k+1}\}_{k=0}^{\dim(\mathcal{X}-1)}\). The second part
follows by observing that two \((k-1)\) cells \(x^{k-1}\) and
\(y^{k-1}\) are 1-adjacent if and only if there exists a \(k\)-cell
\(z^k\) that is incident to \(x^{k-1}\) and \(y^{k-1}\). The third part
is confirmed by noting that two \((k+1)\)-cells \(x^{k+1}\) and
\(y^{k+1}\) are 1-coadjacent if and only if there exists a \(k\)-cell
\(z^k\) that is incident to \(x^{k+1}\) and \(y^{k+1}\).

\begin{figure}

{\centering \includegraphics{figures/prop_structure} 

}

\caption{Relation between immediate incidence and (co)adjacency on the Hasse graph of a CC. (a): Two $(k-1)$ cells $x^{k-1}$ and $y^{k-1}$ (orange vertices) are 1-adjacent if and only if there exists a $k$-cell $z^k$ (pink vertex) that is incident to $x^{k-1}$ and $y^{k-1}$. (b): Two $(k+1)$ cells $x^{k+1}$ and $y^{k+1}$ (blue vertices) are 1-coadjacent if and only if there exists a $k$-cell $z^k$ (pink vertex) that is incident to $x^{k+1}$ and $y^{k+1}$.}\label{fig:vis-structure}
\end{figure}

\subsubsection{Augmented Hasse graphs}\label{augmented-hasse-graphs}

The Hasse graph of a CC is useful because it shows that computations for
a higher-order deep learning model can be reduced to computations for a
graph-based model. Particularly, a \(k\)-cochain (signal) being
processed on a CC \(\mathcal{X}\) can be thought as a signal on the
corresponding vertices of the associated Hasse graph
\(\mathcal{H}_{\mathcal{X}}\). The edges specified by the matrices
\(B_{k,k+1}\) determine the message-passing structure of a given
higher-order model defined on \(\mathcal{X}\). However, the
message-passing structure determined via the matrices \(A_{r,k}\) is not
directly supported on the corresponding edges of
\(\mathcal{H}_{\mathcal{X}}\). Thus, it is sometimes desirable to
\emph{augment the Hasse graph} with additional edges other than the ones
specified by the poset partial order relation of the CC. Along these
lines, Definition @ref(def:ahg) introduces the notion of augmented Hasse
graph.

\phantomsection\label{ahg}
Let \(\mathcal{X}\) be a CC, and let \(\mathcal{H}_{\mathcal{X}}\) be
its Hasse graph with vertex set \(V(\mathcal{H}_{\mathcal{X}})\) and
edge set \(E(\mathcal{H}_{\mathcal{X}})\). Let
\(\mathcal{N}=\{\mathcal{N}_1,\ldots,\mathcal{N}_n\}\) be a set of
neighborhood functions defined on \(\mathcal{X}\). We say that
\(\mathcal{H}_{\mathcal{X}}\) has an augmented edge \(e_{x,y}\) induced
by \(\mathcal{N}\) if there exist \(\mathcal{N}_i \in \mathcal{N}\) such
that \(x \in \mathcal{N}_i(y)\) or \(y \in \mathcal{N}_i(x)\). Denote by
\(E_{\mathcal{N}}\) the set of all augmented edges induced by
\(\mathcal{N}\). The \emph{augmented Hasse graph} of \(\mathcal{X}\)
induced by \(\mathcal{N}\) is defined to be the graph
\(\mathcal{H}_{\mathcal{X}}(\mathcal{N})= (V(\mathcal{H}_{\mathcal{X}}), E(\mathcal{H}_{\mathcal{X}}) \cup E_{\mathcal{N}})\).

It is easier to think of the augmented Hasse graph in Definition
@ref(def:ahg) in terms of the matrices \(\mathbf{G}=\{G_1,\ldots,G_n\}\)
associated with the neighborhood functions
\(\mathcal{N}=\{\mathcal{N}_1,\ldots,\mathcal{N}_n\}\). Each augmented
edge in \(\mathcal{H}_{\mathcal{X}}(\mathcal{N})\) corresponds to a
non-zero entry in some \(G_i\in \mathbf{G}\). Since \(\mathcal{N}\) and
\(\mathbf{G}\) store equivalent information, we use
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\) to denote the augmented Hasse
graph induced by the edges determined by \(\mathbf{G}\). For instance,
the graph given in Figure @ref(fig:hasse-diagram)(c) is denoted by
\(\mathcal{H}_{\mathcal{X}}( A_{0,1},coA_{2,1})\).

\subsubsection{Reducibility of CCNNs to graph-based
models}\label{reducibility-of-ccnns-to-graph-based-models}

In this section, we show that any CCNN-based computational model can be
realized as a message-passing scheme over a subgraph of the augmented
Hasse graph of the underlying CC. Every CCNN is determined via a
computational tensor diagram, which can be built using the elementary
tensor operations, namely push-forward operations, merge nodes and split
nodes. Thus, the reducibility of CCNN-based computations to
message-passing schemes over graphs can be achieved by proving that
these three tensor operations can be executed on an augmented Hasse
graph. Proposition @ref(prp:hasse-pushforward) states that push-forward
operations are executable on augmented Hasse graphs.

\phantomsection\label{hasse-pushforward}
Let \(\mathcal{X}\) be a CC and let
\(\mathcal{F}_G \colon \mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})\)
be a push-forward operator induced by a cochain map
\(G\colon\mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})\). Any
computation executed via \(\mathcal{F}_G\) can be reduced to a
corresponding computation over the augmented Hassed graph
\(\mathcal{H}_{\mathcal{X}}(G)\) of \(\mathcal{X}\).

Let \(\mathcal{X}\) be a CC. Let \(\mathcal{H}_{\mathcal{X}}(G)\) be the
augmented Hasse graph of \(\mathcal{X}\) determined by \(G\). The
definition of the augmented Hasse graph implies that there is a
one-to-one correspondence between the vertices
\(\mathcal{H}_{\mathcal{X}}(G)\) and the cells in \(\mathcal{X}\). Given
a cell \(x\in \mathcal{X}\), let \(x^{\prime}\) be the corresponding
vertex in \(\mathcal{H}_{\mathcal{X}}(G)\). Let \(y\) be a cell in
\(\mathcal{X}\) with a feature vector \(\mathbf{h}_y\) computed via the
push-forward operation specified by Equation @ref(eq:functional). Recall
that the vector \(\mathbf{h}_y\) is computed by aggregating all vectors
\(\mathbf{h}_x\) attached to the neighbors \(x \in \mathcal{X}^i\) of
\(y\) with respect to the neighborhood function \(\mathcal{N}_{G^T}\).
Let \(m_{x,y}\) be a computation (message) that is executed between two
cells \(x\) and \(y\) of \(\mathcal{X}\) as a part of the computation of
push-forward \(\mathcal{F}_G\). It follows from the augmented Hasse
graph definition that the cells \(x\) and \(y\) must have a
corresponding non-zero entry in matrix \(G\). Moreover, this non-zero
entry corresponds to an edge in \(\mathcal{H}_{\mathcal{X}}(G)\) between
\(x^{\prime}\) and \(y^{\prime}\). Thus, the computation \(m_{x,y}\)
between the cells \(x\) and \(y\) of \(\mathcal{X}\) can be carried out
as the computation (message) \(m_{x^{\prime},y^{\prime}}\) between the
corresponding vertices \(x^{\prime}\) and \(y^{\prime}\) of
\(\mathcal{H}_{\mathcal{X}}(G)\).

Similarly, computations on an arbitrary merge node can be characterized
in terms of computations on a subgraph of the augmented Hasse graph of
the underlying CC. Proposition @ref(prp:hasse) formalizes this
statement.

\phantomsection\label{hasse}
Any computation executed via a merge node
\(\mathcal{M}_{\mathbf{G},\mathbf{W}}\) as given in Equation
@ref(eq:sum) can be reduced to a corresponding computation over the
augmented Hasse graph \(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\) of the
underlying CC.

Let \(\mathcal{X}\) be a CC. Let \(\mathbf{G}=\{ G_1,\ldots,G_n\}\) be a
sequence of cochain operators defined on \(\mathcal{X}\). Let
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\) be the augmented Hasse graph
determined by \(\mathbf{G}\). By the augmented Hasse graph definition,
there is a one-to-one correspondence between the vertices of
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\) and the cells of
\(\mathcal{X}\). For each cell \(x\in \mathcal{X}\), let \(x^{\prime}\)
be the corresponding vertex in
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\). Let \(m_{x,y}\) be a
computation (message) that is executed between two cells \(x\) and \(y\)
of \(\mathcal{X}\) as part of the evaluation of function
\(\mathcal{M}_{\mathbf{G},W}\). Hence, the two cells \(x\) and \(y\)
must have a corresponding non-zero entry in a matrix
\(G_i\in\mathbf{G}\). By the augmented Hasse graph definition, this
non-zero entry corresponds to an edge in
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\) between \(x^{\prime}\) and
\(y^{\prime}\). Thus, the computation \(m_{x,y}\) between the cells
\(x\) and \(y\) of \(\mathcal{X}\) can be carried out as the computation
(message) \(m_{x^{\prime},y^{\prime}}\) between the corresponding
vertices \(x^{\prime}\) and \(y^{\prime}\) of
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\).

Propositions @ref(prp:hasse-pushforward) and @ref(prp:hasse) ensure that
push-forward and merge node computations can be realized on augmented
Hasse graphs. Theorem @ref(thm:hasse-theorem) generalizes Propositions
@ref(prp:hasse-pushforward) and @ref(prp:hasse), stating that any
computation on tensor diagrams is realizable on augmented Hasse graphs.

\phantomsection\label{hasse-theorem}
Any computation executed via a tensor diagram
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) can be reduced to a
corresponding computation on the augmented Hasse graph
\(\mathcal{H}_{\mathcal{X}}(\mathbf{G})\).

The conclusion follows directly from Propositions @ref(prp:hasse) and
@ref(prp:hasse-pushforward), along with the fact that any tensor diagram
can be realized in terms of the three elementary tensor operations.

According to Theorem @ref(thm:hasse-theorem), a tensor diagram and its
corresponding augmented Hasse graph encode the same computations in
alternative forms. Figure @ref(fig:hasse-diagram-examples) illustrates
that the augmented Hasse graph provides a computational summary of the
associated tensor diagram representation of a CCNN.

\begin{figure}

{\centering \includegraphics{figures/augmented_hasse_graph_examples} 

}

\caption{Tensor diagrams of two CCNNs and their corresponding augmented Hasse graphs. Edge labels are dropped from the tensor diagrams to avoid clutter, as they can be inferred from the corresponding augmented Hasse graphs. (a): A tensor diagram obtained from a higher-order message-passing scheme. (b): A tensor diagram obtained by using the three elementary tensor operations.}\label{fig:hasse-diagram-examples}
\end{figure}

\subsubsection{Augmented Hasse graphs and
CC-pooling}\label{augmented-hasse-graphs-and-cc-pooling}

The Hasse graph and its augmented version are graph representations of
the poset structure of the underlying CC. It is instructive to interpret
the (un)pooling operations (Definitions
@ref(def:pooling-exact-definition) and
@ref(def:unpooling-exact-definition)) with respect to these graphs. The
CC-pooling operation of Definition @ref(def:pooling-exact-definition)
maps a signal in the poset structure from lower-rank cells to
higher-rank ones. On the other hand, the CC-unpooling operation of
Definition @ref(def:unpooling-exact-definition) maps a signal in the
opposite direction. Figure @ref(fig:hasse-graph-pooling) presents an
example of CC (un)-pooling operations visualized over the augmented
Hasse graph of the underlying CC.

\begin{figure}

{\centering \includegraphics{figures/hasse_graph_pooling_scaled} 

}

\caption{CC (un)-pooling operations viewed on the augmented Hasse graph of a CC. The vertices in the figure represent skeletons in the underlying CC. Black edges represent edges in the Hasse graph between these vertices, whereas red edges represent edges obtained from the augmented Hasse graph structure. CC-pooling corresponds to pushing a signal in the poset structure from lower-rank to higher-rank vertices, whereas CC-unpooling corresponds to pushing a signal in the poset structure from higher-rank to lower-rank vertices.}\label{fig:hasse-graph-pooling}
\end{figure}

\subsubsection{Augmented Hasse diagrams, message passing and merge
nodes}\label{augmented-hasse-diagrams-message-passing-and-merge-nodes}

The difference between constructing a CCNN using the higher-order
message passing paradigm given in Section
@ref(definition-of-higher-order-message-passing) versus using the three
elementary tensor operations given in Section
@ref(the-main-three-tensor-operations) has been demonstrated in Section
@ref(merge-nodes-and-higher-order-message-passing-a-qualitative-comparison).
In particular, Section
@ref(merge-nodes-and-higher-order-message-passing-a-qualitative-comparison)
mentions that merge nodes naturally allow for a more flexible
computational framework in comparison to the higher-order
message-passing paradigm. This flexibility manifests in terms of the
underlying tensor diagram as well as the input for the network under
consideration. The difference between tensor operations and higher-order
message passing can also be highlighted with augmented Hasse graphs, as
demonstrated in Figure @ref(fig:hasse-diagram-examples). Figure
@ref(fig:hasse-diagram-examples)(a) shows a tensor diagram obtained from
a higher-order message-passing scheme on a CCNN. We observe two key
properties of this CCNN: the initial input cochains are supported on all
cells of all dimensions of the domain, and the CCNN updates all cochains
supported on all cells of all dimensions of the domain at every
iteration given a predetermined set of neighborhood functions. As a
consequence, the corresponding augmented Hasse graph exhibits a uniform
topological structure. In contrast, Figure
@ref(fig:hasse-diagram-examples)(b) shows a tensor diagram constructed
using the three elementary tensor operations. As the higher-order
message-passing rules do not impose constraints, the resulting augmented
Hasse graph exhibits a more flexible structure.

\subsubsection{Higher-order representation
learning}\label{higher-order-representation-learning}

The relation between augmented Hasse graphs and CCs given by Theorem
@ref(thm:hasse-theorem) suggests that many graph-based deep learning
constructions have analogous constructions for CCs. In this section, we
demonstrate how \emph{higher-order representation learning} can be
reduced to graph representation learning
(\citeproc{ref-hamilton2017representation}{Hamilton, Ying, and Leskovec
2017}), as an application of certain CC computations as augmented Hasse
graph computations.

The goal of graph representation is to learn a mapping that embeds the
vertices, edges or subgraphs of a graph into a Euclidean space, so that
the resulting embedding captures useful information about the graph.
Similarly, higher-order representation learning
(\citeproc{ref-hajijcell}{Hajij, Istvan, and Zamzmi 2020}) involves
learning an embedding of various cells in a given topological domain
into a Euclidean space, preserving the main structural properties of the
topological domain. More precisely, given a complex \(\mathcal{X}\),
higher-order representation learning refers to learning a pair
\((enc, dec)\) of functions, consisting of the \emph{encoder map}
\(enc \colon \mathcal{X}^k \to \mathbb{R}^d\) and the \emph{decoder map}
\(dec \colon \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\). The
encoder function associates to every \(k\)-cell \(x^k\) in
\(\mathcal{X}\) a feature vector \(enc(x^k)\), which encodes the
structure of \(x^k\) with respect to the structures of other cells in
\(\mathcal{X}\). On the other hand, the decoder function associates to
every pair of cell embeddings a measure of similarity, which quantifies
some notion of relation between the corresponding cells. We optimize the
trainable functions \((enc, dec)\) using a context-specific
\emph{similarity measure}
\(sim \colon \mathcal{X}^k \times \mathcal{X}^k \to \mathbb{R}\) and an
objective function \begin{equation}
\mathcal{L}_k=\sum_{ x^k \in \mathcal{X}^k     } l(  dec(  enc(x^{k}), enc(y^{k})),sim(x^{k},y^k)),
(\#eq:loss)
\end{equation} where
\(l \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}\) is a loss
function. The precise relation between higher-order and graph
representation learning is given by Proposition
@ref(prp:convert-graphtocc).

\phantomsection\label{convert-graphtocc}
Higher-order representation learning can be reduced to graph
representation learning.

Let \(sim\colon \mathcal{X}^k \times \mathcal{X}^k \to \mathbb{R}\) be a
similarity measure. The graph \(\mathcal{G}_{\mathcal{X}^k}\) is defined
as the graph whose vertex set corresponds to cells in \(\mathcal{X}^k\)
and whose edges correspond to cell pairs in
\(\mathcal{X}^k \times \mathcal{X}^k\) mapped to non-zero values by the
function \(sim\). Thus, the pair \((enc, dec)\) corresponds to a pair
\((enc_{\mathcal{G}}, dec_{\mathcal{G}})\) of the form
\(enc_{\mathcal{G}}\colon \mathcal{G}_{\mathcal{X}^k} \to \mathbb{R}\)
and
\(dec_{\mathcal{G}}\colon \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\).
Thereby, learning the pair \((enc, dec)\) is reduced to learning the
pair \((enc_{\mathcal{G}}, dec_{\mathcal{G}})\).

\href{https://github.com/pyt-team/TopoEmbedX}{TopoEmbedX}, one of our
three contributed software packages, supports higher-order
representation learning on cell complexes, simplicial complexes, and
CCs. The main computational principle underlying TopoEmbedX is
Proposition @ref(prp:convert-graphtocc). Specifically, TopoEmbedX
converts a given higher-order domain into a subgraph of the
corresponding augmented Hasse graph, and then utilizes existing graph
representation learning algorithms to compute the embedding of elements
of this subgraph. Given the correspondence between the elements of the
augmented Hasse graph and the original higher-order domain, this results
in obtaining embeddings for the higher-order domain.

Following our discussion on Hasse graphs, and particularly the ability
to transform computations on a CCNN to computations on a (Hasse) graph,
one may argue that GNNs are sufficient and that there is no need for
CCNNs. However, this is a misleading clue, in the sense that any
computation can be represented by a computational graph. Applying a
standard GNN over the augmented Hasse graph of a CC is not equivalent to
applying a CCNN. This point will become clearer in Section
@ref(on-the-equivariance-of-ccnns), where we introduce CCNN
\emph{equivariances}.

\subsection{On the equivariance of
CCNNs}\label{on-the-equivariance-of-ccnns}

Analogous to their graph counterparts, higher-order deep learning
models, and CCNNs in particular, should always be considered in
conjunction with their underlying \emph{equivariance}
(\citeproc{ref-bronstein2021geometric}{Bronstein et al. 2021}). We now
provide novel definitions for \emph{permutation} and \emph{orientation
equivariance for CCNNs} and draw attention to their relations with
conventional notions of equivariance defined for GNNs.

\subsubsection{Permutation equivariance of
CCNNs}\label{permutation-equivariance-of-ccnns}

Motivated by Proposition @ref(prp:structure), which characterizes the
structure of a CC, this section introduces permutation-equivariant
CCNNs. We first define the action of the permutation group on the space
of cochain maps.

\phantomsection\label{perm}
Let \(\mathcal{X}\) be a CC. Define
\(\mbox{Sym}(\mathcal{X}) = \prod_{i=0}^{\dim(\mathcal{X})} \mbox{Sym}(\mathcal{X}^k)\)
the group of rank-preserving permutations of the cells of
\(\mathcal{X}\). Let \(\mathbf{G}=\{G_k\}\) be a sequence of cochain
maps defined on \(\mathcal{X}\) with
\(G_k  \colon \mathcal{C}^{i_k}\to \mathcal{C}^{j_k}\),
\(0\leq i_k,j_k\leq \dim(\mathcal{X})\). Let
\(\mathcal{P}=(\mathbf{P}_i)_{i=0}^{\dim(\mathcal{X})} \in \mbox{Sym}(\mathcal{X})\).
Define the \emph{permutation (group) action} of \(\mathcal{P}\) on
\(\mathbf{G}\) by
\(\mathcal{P}(\mathbf{G}) = (\mathbf{P}_{j_k} G_{k} \mathbf{P}_{i_k}^T )_{i=0}^{\dim(\mathcal{X})}\).

We introduce permutation-equivariant CCNNs in Definition @ref(def:eqv),
using the group action given in Definition @ref(def:perm). Definition
@ref(def:eqv) generalizes the relevant definitions in
(\citeproc{ref-roddenberry2021principled}{T. Mitchell Roddenberry,
Glaze, and Segarra 2021}; \citeproc{ref-schaub2021signal}{Schaub et al.
2021}). We refer the reader to (\citeproc{ref-joglwe2022}{Jogl 2022};
\citeproc{ref-velivckovic2022message}{Veličković 2022}) for a related
discussion. Hereafter, we use
\(\mbox{Proj}_k \colon \mathcal{C}^1\times \cdots \times \mathcal{C}^m \to \mathcal{C}^k\)
to denote the standard \(k\)-th projection for \(1\leq k \leq m\),
defined via
\(\mbox{Proj}_k ( \mathbf{H}_{1},\ldots, \mathbf{H}_{k},\ldots,\mathbf{H}_{m})= \mathbf{H}_{k}\).

\phantomsection\label{eqv}
Let \(\mathcal{X}\) be a CC and let \(\mathbf{G}= \{G_k\}\) be a finite
sequence of cochain maps defined on \(\mathcal{X}\). Let
\(\mathcal{P}=(\mathbf{P}_i)_{i=0}^{\dim(\mathcal{X})} \in \mbox{Sym}(\mathcal{X})\).
A CCNN of the form \begin{equation*}
\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}
\end{equation*} is called a \emph{permutation-equivariant CCNN} if
\begin{equation}
\mbox{Proj}_k \circ \mbox{CCNN}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})=
\mathbf{P}_{k} \mbox{Proj}_k \circ
\mbox{CCNN}_{\mathcal{P}(\mathbf{G});\mathbf{W}}(\mathbf{P}_{i_1} \mathbf{H}_{i_1}, \ldots ,\mathbf{P}_{i_m} \mathbf{H}_{i_m})
\end{equation} for all \(1 \leq k\leq m\) and for any
\((\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m}) \in\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\).

Definition @ref(def:eqv) generalizes the corresponding notion of
permutation equivariance of GNNs. Consider a graph with \(n\) vertices
and adjacency matrix \(A\). Denote a GNN on this graph by
\(\mathrm{GNN}_{A;W}\). Let \(H \in \mathbb{R}^{n \times k}\) be vertex
features. Then \(\mathrm{GNN}_{A;W}\) is permutation equivariant in the
sense that for \(P \in \mbox{Sym}(n)\) we have
\(P \,\mathrm{GNN}_{A;W}(H) = \mathrm{GNN}_{PAP^{T};W}(PH)\).

In general, working with Definition @ref(def:eqv) may be cumbersome. It
is easier to characterize the equivariance in terms of merge nodes. To
this end, recall that the height of a tensor diagram is the longest path
from any source node to any target node. Proposition @ref(prp:simple)
allows us to express tensor diagrams of height one in terms of merge
nodes.

\phantomsection\label{simple}
Let
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)
be a CCNN with a tensor diagram of height one. Then \begin{equation}
        \label{merge_lemma}
        \mbox{CCNN}_{\mathbf{G};\mathbf{W}}=(
        \mathcal{M}_{\mathbf{G}_{j_1};\mathbf{W}_1},\ldots,
        \mathcal{M}_{\mathbf{G}_{j_n};\mathbf{W}_n}),
(\#eq:merge-lemma)
\end{equation} where \(\mathbf{G}_k \subseteq \mathbf{G}\).

Let
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)
be a CCNN with a tensor diagram of height one. Since the codomain of the
function \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) is
\(\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times \mathcal{C}^{j_n}\),
then \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) is determined by \(n\)
functions
\(F_k\colon  \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_k}\)
for \(1 \leq k \leq n\). Since the height of the tensor diagram of
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) is one, then each function
\(F_k\) is also of height one and it is thus a merge node by definition.
The result follows.

Proposition @ref(prp:simple) states that every target node \(j_k\) in a
tensor diagram of height one is a merge node specified by the operators
\(\mathbf{G}_{j_k}\) formed by the labels of the edges with target
\(j_k\). Definition @ref(def:eqv) introduces the general notion of
permutation equivariance of CCNNs. Definition
@ref(def:node-equivariance) introduces the notion of
permutation-equivariant merge node. Since a merge node is a CCNN,
Definition @ref(def:node-equivariance) is a special case of Definition
@ref(def:eqv).

\phantomsection\label{node-equivariance}
Let \(\mathcal{X}\) be a CC and let \(\mathbf{G}= \{G_k\}\) be a finite
sequence of cochain operators defined on \(\mathcal{X}\) with
\(G_k\colon C^{i_k}(\mathcal{X})\to C^{j}(\mathcal{X})\). Let
\(\mathcal{P}=(\mathbf{P}_i)_{i=0}^{\dim(\mathcal{X})} \in \mbox{Sym}(\mathcal{X})\).
We say that the merge node given in Equation @ref(eq:sum) is a
\emph{permutation-equivariant merge node} if \begin{equation}
\mathcal{M}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})= \mathbf{P}_{j}  \mathcal{M}_{\mathcal{P}(\mathbf{G});\mathbf{W}}(\mathbf{P}_{i_1} \mathbf{H}_{i_1}, \ldots ,\mathbf{P}_{i_1} \mathbf{H}_{i_m})
\end{equation} for any
\((\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m}) \in \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\).

\phantomsection\label{height1}
Let
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}\)
be a CCNN with a tensor diagram of height one. Then
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) is permutation equivariant if
and only the merge nodes \(\mathcal{M}_{\mathbf{G}_{j_k};\mathbf{W}_k}\)
given in Equation @ref(eq:merge-lemma) are permutation equivariant for
\(1 \leq k \leq n\).

If a CCNN is of height one, then by Proposition @ref(prp:simple),
\(\mbox{Proj}_k \circ \mbox{CCNN}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})= \mathcal{M}_{\mathbf{G}_{j_k};\mathbf{W}_k}\).
Hence, the result follows from the definition of merge node permutation
equivariance (Definition @ref(def:node-equivariance)) and the definition
of CCNN permutation equivariance (Definition @ref(def:eqv)).

Finally, Theorem @ref(thm:height2) characterizes the permutation
equivariance of CCNNs in terms of merge nodes. From this point of view,
Theorem @ref(thm:height2) provides a practical version of permutation
equivariance for CCNNs.

\phantomsection\label{height2}
A \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) is permutation equivariant if
and only if every merge node in \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)
is permutation equivariant.

Proposition @ref(prp:height1) proves this fact for CCNNs of height one.
For CCNNs of height \(n\), it is enough to observe that a CCNN of height
\(n\) is a composition of \(n\) CCNNs of height one and that the
composition of two permutation-equivariant networks is a
permutation-equivariant network.

Our permutation equivariance assumes that all cells in each dimension
are independently labeled with indices. However, if we label the cells
in a CC with subsets of the powerset \(\mathcal{P}(S)\) rather than with
indices, then we only need to consider permutations of the powerset that
are induced by permutations of the 0-cells in order to ensure
permutation equivariance.

A GNN is equivariant in that a permutation of the vertex set of the
graph and the input signal over the vertex set yields the same
permutation of the GNN output. Applying a standard GNN over the
augmented Hasse graph of the underlying CC is thus not equivalent to
applying a CCNN. Although the message-passing structures are the same,
the weight-sharing and permutation equivariance of the standard GNN and
CCNN are different. In particular, Definition @ref(def:maps) gives
additional structure, which is not preserved by an arbitrary permutation
of the vertices in the augmented Hasse graph. Thus, care is required in
order to reduce message passing over a CCNN to message passing over the
associated augmented Hasse graph. Specifically, one need only consider
the subgroup of permutations of vertex labels in the augmented Hasse
graph which are induced by permutations of 0-cells in the corresponding
CC. Thus, there is merit in adopting the rich notions of topology to
think about distributed, structured learning architectures, as
topological constructions facilitate reasoning about computation in ways
that are not within the scope of graph-based approaches.

Note that Proposition @ref(prp:convert-graphtocc) does not contradict
the previous remark. In fact, the computations described in Proposition
@ref(prp:convert-graphtocc) are conducted on a particular subgraph of
the Hasse graph whose vertices are the \(k\)-cells of the underlying
complex. Differences between graph-based networks and TDL networks start
to emerge particularly once different dimensions are considered
simultaneously during computations.

\subsubsection{Orientation equivariance of
CCNNs}\label{orientation-equivariance-of-ccnns}

When CC are reduced to regular cell complexes, then orientation
equivariance can also be introduced to CCNNs. Analogous to Definition
@ref(def:perm), we introduce the following definition of orientation
actions on CCs.

\phantomsection\label{orientation}
Let \(\mathcal{X}\) be a CC. Let \(\mathbf{G}=\{G_k\}\) be a sequence of
cochain operators defined on \(\mathcal{X}\) with
\(G_k  \colon \mathcal{C}^{i_k}\to \mathcal{C}^{j_k}\),
\(0\leq i_k,j_k\leq \dim(\mathcal{X})\). Let \(O(\mathcal{X})\) be the
group of tuples \(\mathcal{D}=(\mathbf{D}_i)_{i=0}^{\dim(\mathcal{X})}\)
of diagonal matrices with diagonals \(\pm 1\) of size
\(|\mathcal{X}^k| \times |\mathcal{X}^k|\) such that \(\mathbf{D}_0=I\).
Define the \emph{orientation (group) action} of \(\mathcal{D}\) on
\(\mathbf{G}\) by
\(\mathcal{D}(\mathbf{G}) = (\mathbf{D}_{j_k} G_{k} \mathbf{D}_{i_k})_{i=0}^{\dim(\mathcal{X})}\).

We introduce orientation-equivariant CCNNs in Definition @ref(def:oe),
using the group action given in Definition @ref(def:orientation).
Orientation equivariance for CCNNs (Definition @ref(def:oe)) is put
forward in analogous way to permutation equivariance for CCNNs
(Definition @ref(def:eqv)).

\phantomsection\label{oe}
Let \(\mathcal{X}\) be a CC and let \(\mathbf{G}= \{G_k\}\) be a finite
sequence of cochain operators defined on \(\mathcal{X}\). Let
\(\mathcal{D} \in O(\mathcal{X})\). A CCNN of the form \begin{equation*}
\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m} \to \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \cdots \times \mathcal{C}^{j_n}
\end{equation*} is called an \emph{orientation-equivariant CCNN} if
\begin{equation}
\mbox{Proj}_k \circ \mbox{CCNN}_{\mathbf{G};\mathbf{W}}(\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m})=\mathbf{D}_{k} \mbox{Proj}_k \circ \mbox{CCNN}_{\mathcal{D}(\mathbf{G});\mathbf{W}}((\mathbf{D}_{i_1} \mathbf{H}_{i_1}, \ldots ,\mathbf{D}_{i_1} \mathbf{H}_{i_m}))
\end{equation} for all \(1 \leq k\leq m\) and for any
\((\mathbf{H}_{i_1},\ldots ,\mathbf{H}_{i_m}) \in \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \cdots \times  \mathcal{C}^{i_m}\).

Propositions @ref(prp:simple) and @ref(prp:height1) can be stated
analogously for the orientation equivariance case. We skip stating these
facts here, and only state the main theorem that characterizes the
orientation equivariance of CCNNs in terms of merge nodes.

\phantomsection\label{height3}
A \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) is orientation equivariant if
and only if every merge node in \(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\)
is orientation equivariant.

The proof of Theorem @ref(thm:height3) is similar to the proof of
Theorem @ref(thm:height2).

\section*{(PART*) Part IV: Applications, literature and
conclusions}\label{part-part-iv-applications-literature-and-conclusions}
\addcontentsline{toc}{section}{(PART*) Part IV: Applications, literature
and conclusions}

\section{Implementation and numerical
results}\label{implementation-and-numerical-results}

The proposed CCNNs can be used to construct different neural network
architectures for diverse learning tasks. In this section, we
demonstrate the generality and the efficacy of CCNNs by evaluating their
predictive performance on shape analysis and graph learning tasks. In
our geometric processing experiments, we compare CCNNs against
state-of-the-art methods, which are highly engineered and trained
towards specific tasks. Furthermore, we conduct our experiments on
various data modalities commonly studied in geometric data processing,
namely on point clouds and 3D meshes. We also perform experiments on
graph data. In our experiments, we tune three main components: the
choice of CCNN architecture, the learning rate, and the number of
replications in data augmentation. We justify the choice of CCNN
architecture for each learning task. We have implemented our pipeline in
PyTorch, and ran the experiments on a single GPU NVIDIA GeForce RTX 3060
Ti using a Microsoft Windows back-end.

\subsection{Software: TopoNetX, TopoEmbedX, and
TopoModelX}\label{software-toponetx-topoembedx-and-topomodelx}

All our software development and experimental analysis have been carried
out using Python. We have developed three Python packages, which we have
also used to run our experiments:
\href{https://github.com/pyt-team/TopoNetX}{TopoNetX},
\href{https://github.com/pyt-team/TopoEmbedX}{TopoEmbedX} and
\href{https://github.com/pyt-team/TopoModelX}{TopoModelX}. TopoNetX
supports the construction of several topological structures, including
cell complex, simplicial complex, and combinatorial complex classes.
These classes provide methods for computing boundary operators, Hodge
Laplacians and higher-order adjacency operators on cell, simplicial, and
combinatorial complexes, respectively. TopoEmbedX supports
representation learning of higher-order relations on cell complexes,
simplicial complexes, and combinatorial complexes. TopoModelX supports
computing deep learning models defined on these topological domains.

In addition to our software package implementations, we have utilized
PyTorch (\citeproc{ref-paszke2017automatic}{Paszke et al. 2017}) for
training the neural networks reported in this section. Also, we have
utilized Scikit-learn (\citeproc{ref-scikit-learn}{Pedregosa et al.
2011}) to compute the eigenvectors of the 1-Hodge Laplacians. The normal
vectors of point clouds have been computed using the Point Cloud Utils
package (\citeproc{ref-point-cloud-utils}{Williams 2022}). Finally, we
have used NetworkX (\citeproc{ref-hagberg2008exploring}{Hagberg, Swart,
and S Chult 2008}) and HyperNetX
(\citeproc{ref-joslyn2021hypernetwork}{Joslyn et al. 2021}), both in the
development of our software packages as well as in our computations.

\subsection{Datasets}\label{datasets}

In our evaluation of CCNNs, we use four datasets: the Human Body, the
COSEG, the SHREC11, and a benchmark dataset for graph classification
taken from (\citeproc{ref-bianchi2020mincutpool}{Bianchi, Gallicchio,
and Micheli 2022}). A summary of these datasets follows.

\textbf{Human Body segmentation dataset}. The original Human Body
segmentation dataset presented in
(\citeproc{ref-atzmon2018point}{Atzmon, Maron, and Lipman 2018})
contains relatively large meshes with a size up to 12,000 vertices. The
segmentation labels provided in this dataset are set per-face, and the
segmentation accuracy is defined to be the ratio of the correctly
classified faces over the total number of faces in the entire dataset.
In this work, we use a simplified version of the original Human Body
dataset, as provided by (\citeproc{ref-hanocka2019meshcnn}{Hanocka et
al. 2019}), in which meshes have less than 1,000 nodes and segmentation
labels are remapped to edges. We use this simplified version of the
Human Body dataset for the shape analysis (i.e., mesh segmentation) task
in Section @ref(mesh-segmentation).

\textbf{COSEG segmentation dataset}. The original COSEG dataset
(\citeproc{ref-wang2012active}{Y. Wang et al. 2012}) contains eleven
sets of shapes with ground-truth segmentation. In this work, we use a
subset of the original COSEG dataset that contains relatively large sets
of aliens, vases, and chairs. These three sets consist of 200, 300, and
400 shapes, respectively. We use this custom subset of the COSEG dataset
for the shape analysis (i.e., mesh segmentation) task in Section
@ref(mesh-segmentation).

\textbf{SHREC11 classification dataset}. SHREC 2011
(\citeproc{ref-lian2011shape}{Lian et al. 2011}), abbreviated as
SHREC11, is a large-scale dataset that contains 600 nonrigid deformed
shapes (watertight triangle meshes) from 30 categories, where each
category contains an equal number of objects. Examples of these
categories include hand, lamp, woman, man, flamingo, and rabbit. The
dataset is available as a training and test set consisting of 480 and
120 shapes, respectively. We use the SHREC11 dataset for the shape
analysis tasks in Sections @ref(mesh-and-point-cloud-classification) and
@ref(pooling-with-mapper-on-graphs-and-data-classification).

\textbf{Benchmark dataset for graph classification}. This dataset has
graphs belonging to three different classes
(\citeproc{ref-bianchi2020mincutpool}{Bianchi, Gallicchio, and Micheli
2022}). For each graph, the feature vector on each vertex (the
0-cochain) is a one-hot vector of size five, and it stores the relative
position of the vertices on the graph. This dataset has easy and hard
versions. The easy version has highly connected graphs, while the hard
version has sparse graphs. We use this dataset for the graph
classification task in Section @ref(graph-classification).

\subsection{Shape analysis: mesh segmentation and
classification}\label{shape-analysis-mesh-segmentation-and-classification}

The CC structure used for the shape analysis experiments (mesh
segmentation and classification) is simply induced by the triangulation
of the meshes. Specifically, the 0-, 1-, and 2-cells are the vertices,
edges, and faces of the mesh, respectively. The matrices used for the
CCNNs are \(B_{0,1},~B_{0,2}\), their transpose matrices, and the
(co)adjacency matrices \(A_{1,1}\), \(coA_{1,1}\), and \(coA_{2,1}\).

A CCNN takes a vector of cochains as input features. For shape analysis
tasks, we consider cochains, whose features are built directly from the
vertex coordinates of the underlying mesh. We note that other choices
(e.g., spectral-based cochains as in
(\citeproc{ref-mejia2017spectral}{Mejia, Ruiz-Salguero, and Cadavid
2017})) can also be included. Our shape analysis tasks have three input
cochains: the vertex, edge and face cochains. Each vertex cochain has
two input features: the position and the normal vectors associated with
the vertex. Similar to (\citeproc{ref-hanocka2019meshcnn}{Hanocka et al.
2019}), each edge cochain consists of five features: the length of the
edge, the dihedral angle, two inner angles, and two edge-length ratios
for each face. Finally, each input face cochain consists of three input
features: the face area, face normal, and the three face angles.

\subsubsection{Mesh segmentation}\label{mesh-segmentation}

For the Human Body dataset (\citeproc{ref-maron2017convolutional}{Maron
et al. 2017}), we built a CCNN that produces an edge class. The tensor
diagram of the architecture is shown in Figure @ref(fig:mesh-net)(a).
For the COSEG dataset (\citeproc{ref-wang2012active}{Y. Wang et al.
2012}), we built a CCNN that combines our proposed feature vectors
defined on vertices, edges, and faces to learn the final face class. The
architecture uses incidence matrices as well as (co)adjacency matrices
to construct a signal flow as demonstrated in Figure
@ref(fig:mesh-net)(b). Specifically, the tensor diagram displays three
non-squared attention-blocks and three squared attention blocks. The
depth of the model is chosen to be two, as indicated in Figure
@ref(fig:mesh-net)(b).

\begin{figure}

{\centering \includegraphics{figures/experiment} 

}

\caption{The tensor diagrams of the CCNNs used in our experiments. (a): The CCNNs used in the mesh segmentation tasks. In particular, $\mbox{CCNN}_{HB}$ and $\mbox{CCNN}_{COSEG}$ are the architectures used on the Human Body dataset [@atzmon2018point] and on the COSEG dataset [@wang2012active], respectively. (b): The mesh classification CCNN used on the SHREC11 dataset [@lian2011shape]. (c): The graph classification CCNN used on the dataset provided in [@bianchi2020mincutpool]. (d): The mesh/point cloud classification CCNNs used in conjunction with the MOG algorithm on the SHREC11 dataset.}\label{fig:mesh-net}
\end{figure}

Note that the architectures chosen for the COSEG and for the Human Body
datasets have the same number and types of building blocks; compare
Figures @ref(fig:mesh-net)(a) and (b). We use a random 85\%-15\%
train-test split. For both of these architectures, a softmax activation
is applied to the output tensor. All our segmentation models are trained
for 600 epochs using a learning rate of 0.0001 and the standard
cross-entropy loss. These results are consistent across Human Body and
Shape COSEG datasets.

We test the proposed CCNNs on mesh segmentation using the Human Body
(\citeproc{ref-maron2017convolutional}{Maron et al. 2017}) and the Shape
COSEG (vase, chair, and alien) (\citeproc{ref-wang2012active}{Y. Wang et
al. 2012}) datasets. For each mesh in these datasets, the utilized CC
structure is the one induced by the triangulation of the meshes,
although other variations in the CC structure yield comparable results.
Further, three \(k\)-cochains are constructed for \(0\leq k \leq 2\) and
are utilized in CCNN training. As shown in Table @ref(tab:shape-xp),
CCNNs outperform three neural networks tailored to mesh analysis
(HodgeNet (\citeproc{ref-smirnov2021hodgenet}{Smirnov and Solomon
2021}), PD-MeshNet (\citeproc{ref-milano2020primal}{Milano et al. 2020})
and MeshCCN (\citeproc{ref-hanocka2019meshcnn}{Hanocka et al. 2019})) on
two out of four datasets, and are among the best two neural networks on
all four datasets.

\begin{longtable}[]{@{}lcccc@{}}
\caption{Predictive accuracy on test sets related to shape analysis,
namely on Human Body and COSEG (vase, chair, alien) datasets. The
results reported here are based on the \(\mbox{CCNN}_{COSEG}\) and
\(\mbox{CCNN}_{HB}\) architectures. In particular, the result for
\(\mbox{CCNN}_{HB}\) is reported in the first column, whereas the
results for \(\mbox{CCNN}_{COSEG}\) are reported in the second, third
and forth columns.}\tabularnewline
\toprule\noalign{}
Method & Human Body & COSEG vase & COSEG chair & COSEG alien \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Method & Human Body & COSEG vase & COSEG chair & COSEG alien \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
HodgeNet & 85.03 & 90.30 & 95.68 & 96.03 \\
PD-MeshNet & 85.61 & 95.36 & 97.23 & 98.18 \\
MeshCNN & 85.39 & 92.36 & 92.99 & 96.26 \\
CCNN & 87.30 & 93.40 & 98.30 & 93.70 \\
\end{longtable}

\textbf{Architecture of \(\mbox{CCNN}_{COSEG}\) and
\(\mbox{CCNN}_{HB}\)}. In \(\mbox{CCNN}_{COSEG}\), as shown in Figure
@ref(fig:mesh-net)(a), we choose a CCNN pooling architecture as given in
Definition @ref(def:general-pooling-hoan), which pushes signals from
vertices, edges and faces, and aggregates their information towards the
final face prediction class. We choose \(\mbox{CCNN}_{HB}\) similarly,
except that the predicted signal is an edge class. The reason for this
choice is that the Human Body dataset
(\citeproc{ref-atzmon2018point}{Atzmon, Maron, and Lipman 2018}) encodes
the segmentation information on edges.

\subsubsection{Mesh and point cloud
classification}\label{mesh-and-point-cloud-classification}

We evaluate our method on mesh classification using the SHREC11 dataset
(\citeproc{ref-lian2011shape}{Lian et al. 2011}) based on the same
cochains and CC structure used in the segmentation experiment of Section
@ref(mesh-segmentation). The CCNN architecture for our mesh
classification task, denoted by \(\mbox{CCNN}_{SHREC}\), is demonstrated
in Figure @ref(fig:mesh-net)(b). The final layer of
\(\mbox{CCNN}_{SHREC}\), depicted as a grey node in Figure
@ref(fig:mesh-net)(b), is a simple pooling operation that sums all
embeddings of the CC after mapping them to the same Euclidean space. The
\(\mbox{CCNN}_{SHREC}\) is trained for 40 epochs with both tanh and
identity activation functions using a learning rate of 0.005 and the
standard cross-entropy loss. We use anisotropic scaling and random
rotations for data augmentation. Each mesh is augmented 30 times, is
centered around the vertex center of the mass, and is rescaled to fit
inside the unit cube.

The \(\mbox{CCNN}_{SHREC}\) with identity activations and \(\tanh\)
activations achieve predictive accuracies of 96.67\% and 99.17\%,
respectively. Table @ref(tab:shrec) shows that CCNNs outperform two
neural networks tailored to mesh analysis (HodgeNet and MeshCCN), being
the second best model behind PD-MeshNet in mesh and point cloud
classification. It is worth mentioning that the mesh classification CCNN
requires a significantly lower number of epochs to train (40 epochs) as
compared to the mesh segmentation CCNNs (600 epochs).

\begin{longtable}[]{@{}lcc@{}}
\caption{Predictive accuracy on the SHREC11 test dataset. The left and
right column report the mesh and point cloud classification results,
respectively. The CCNN for mesh classification is
\(\mbox{CCNN}_{SHREC}\), while the CCNN for point cloud classification
is \(\mbox{CCNN}_{MOG2}\).}\tabularnewline
\toprule\noalign{}
Method & Mesh & Point cloud \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Method & Mesh & Point cloud \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
HodgeNet & 99.10 & 94.70 \\
PD-MeshNet & 99.70 & 99.10 \\
MeshCNN & 98.60 & 91.00 \\
CCNN & 99.17 & 95.20 \\
\end{longtable}

\textbf{Architecture of \(\mbox{CCNN}_{SHREC}\)}. The
\(\mbox{CCNN}_{SHREC}\) has two layers and is chosen as a pooling CCNN
in the sense of Definition @ref(def:general-pooling-hoan), similar to
\(\mbox{CCNN}_{COSEG}\) and \(\mbox{CCNN}_{HB}\). The main difference is
that the final layer of \(\mbox{CCNN}_{SHREC}\), represented by the grey
point in Figure @ref(fig:mesh-net)(b), is a global pooling function that
sums all embeddings of all dimensions (zero, one and two) of the
underlying CC after mapping them to the same Euclidean space.

\subsubsection{Graph classification}\label{graph-classification}

For the graph classification task, we use the graph classification
benchmark provided in (\citeproc{ref-bianchi2020mincutpool}{Bianchi,
Gallicchio, and Micheli 2022}); the dataset consists of graphs with
three different labels. For each graph, the feature vector on each
vertex (the 0-cochain) is a one-hot vector of size five, and it stores
the relative position of the vertex on the graph. To construct the CC
structure, we use the 2-clique complex of the input graph. We then
proceed to build the CCNN for graph classification, denoted by
\(\mbox{CCNN}_{Graph}\), which is visualized in Figure
@ref(fig:mesh-net)(c). The matrices used for the construction of
\(\mbox{CCNN}_{Graph}\) are \(B_{0,1},~B_{1,2},~B_{0,2}\), their
transpose matrices, and the (co)adjacency matrices
\(A_{0,1},A_{1,1},~coA_{2,1}\). The cochains of \(\mbox{CCNN}_{Graph}\)
are constructed as follows. For each graph in the dataset, we set the
0-cochain to be the one-hot vector of size 5 provided by the dataset.
This one-hot vector stores the relative position of the vertex on the
graph. We also construct the 1-cochain and 2-cochain on the 2-clique
complex of the graph by considering the coordinate-wise max value of the
one-hot vectors attached to the vertices of each cell. The input to
\(\mbox{CCNN}_{graoh}\) consists of the 0-cochain provided as a part of
the dataset as well as the constructed 1 and 2-cochains. The grey node
in Figure @ref(fig:mesh-net)(c) indicates a simple mean pooling
operation. We train this network with a learning rate of 0.005 and no
data augmentation.

Table @ref(tab:wrap-tab) reports the results on the \emph{easy} and the
\emph{hard} versions of the datasets\footnote{The difficulty in these
  datasets is controlled by the compactness degree of the graph
  clusters; clusters in the `easy' data have more in-between cluster
  connections, while clusters in the `hard' data are more isolated
  (\citeproc{ref-bianchi2020mincutpool}{Bianchi, Gallicchio, and Micheli
  2022}).}, and compares them to six state-of-the-art GNNs. As shown in
Table @ref(tab:wrap-tab), CCNNs outperform all six GNNs on the hard
dataset, and five of the GNNs on the easy dataset. The proposed CCNN
outperforms MinCutPool on the hard dataset, while it attains comparable
performance to MinCutPool on the easy dataset.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1176}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1324}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1029}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1471}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1029}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1176}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1765}}
  >{\centering\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1029}}@{}}
\caption{Predictive accuracy on the test set of
(\citeproc{ref-bianchi2020mincutpool}{Bianchi, Gallicchio, and Micheli
2022}) related to graph classification. All results are reported using
the \(\mbox{CCNN}_{Graph}\) architecture.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dataset
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Graclus
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
NDP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DiffPool
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Top-K
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAGPool
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
MinCutPool
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
CCNN
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dataset
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Graclus
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
NDP
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
DiffPool
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Top-K
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAGPool
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
MinCutPool
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
CCNN
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Easy & 97.81 & 97.93 & 98.64 & 82.47 & 84.23 & 99.02 & 98.90 \\
Hard & 69.08 & 72.67 & 69.98 & 42.80 & 37.71 & 73.80 & 75.59 \\
\end{longtable}

\textbf{Architecture of \(\mbox{CCNN}_{Graph}\)}. In the
\(\mbox{CCNN}_{Graph}\) displayed in Figure @ref(fig:mesh-net)(c) we
choose a CCNN pooling architecture as given in Definition
@ref(def:general-pooling-hoan) that pushes signals from vertices, edges
and faces, and aggregate their information towards the higher-order
cells before making making the final prediction. For the dataset of
(\citeproc{ref-bianchi2020mincutpool}{Bianchi, Gallicchio, and Micheli
2022}), we experiment with two architectures; the first one is identical
to the \(\mbox{CCNN}_{SHREC}\) shown in Figure @ref(fig:mesh-net)(b),
and the second one is the \(\mbox{CCNN}_{Graph}\) shown in Figure
@ref(fig:mesh-net)(c). We report the results for
\(\mbox{CCNN}_{Graph}\), as it provides superior performance. Note that
when this neural network is conducted on an underlying simplicial
complex, the neighborhood matrices \(B_{0,1}\) and \(B_{1,3}\) are
typically not considered, hence the CC-structure equipped with these
additional incidence matrices improves the generalization performance of
the \(\mbox{CCNN}_{Graph}\).

\subsection{Pooling with mapper on graphs and data
classification}\label{pooling-with-mapper-on-graphs-and-data-classification}

We perform experiments to measure the effectiveness of the MOG pooling
strategy discussed in Section @ref(mapper-and-the-cc-pooling-operation).
Recall that the MOG algorithm requires two pieces of input: the
1-skeleton of a CC \(\mathcal{X}\), and a scalar function on the
vertices of \(\mathcal{X}\). Our choice for the input scalar function is
the average geodesic distance (AGD) (\citeproc{ref-KimLipmanChen2010}{V.
G. Kim et al. 2010}), which is suitable for shape detection as it is
invariant to reflection and rotation. For two entities \(u\) and \(v\)
on a graph, the geodesic distance between \(u\) and \(v\), denoted by
\(d(v,u)\), is computed using Dijkstra's shortest path algorithm. The
AGD is given by the following equation: \begin{equation}
AGD(v)=\frac{1}{|V|}\sum_{u\in V}d(v,u).
(\#eq:agd)
\end{equation}

From Equation @ref(eq:agd), it is immediate that the vertices near the
center of the graph are likely to have low function values, while points
on the periphery are likely to have high values. This observation has
been utilized to study graph symmetry
(\citeproc{ref-KimLipmanChen2010}{V. G. Kim et al. 2010}), and it
provides a justification for selecting the AGD for the MOG pooling
strategy. Figure @ref(fig:pooling-examples) presents a few examples of
applying the MOG pooling strategy using AGD on the SHREC11 dataset.

\begin{figure}

{\centering \includegraphics{figures/pooling_examples} 

}

\caption{Examples of applying the MOG algorithm on the SHREC11 dataset [@lian2011shape]. In each figure, we show the original mesh graph on the left and the mapper graph on the right. The scalar function chosen for the MOG algorithm is the average geodesic distance (AGD). We observe that the pooled mapper graph has similar overall shape to the original graphs.}\label{fig:pooling-examples}
\end{figure}

In order to demonstrate the effectiveness of our MOG pooling approach,
we conduct three experiments on the SHREC11 dataset: mesh classification
based on CC-pooling with input vertex and edge features (Section
@ref(mesh-classification-cc-pooling-with-input-vertex-and-edge-features)),
mesh classification based on CC-pooling with input vertex features only
(Section
@ref(mesh-classification-cc-pooling-with-input-vertex-features-only)),
and point cloud classification based on CC-pooling with input vertex
features only (Section
@ref(point-cloud-classification-cc-pooling-with-input-vertex-features-only)).
The experiments in Sections
@ref(mesh-classification-cc-pooling-with-input-vertex-and-edge-features)
and @ref(mesh-classification-cc-pooling-with-input-vertex-features-only)
utilize the mesh structure in the SHREC11 dataset, whereas the
experiment in Section
@ref(point-cloud-classification-cc-pooling-with-input-vertex-features-only)
utilizes its own point cloud version. In particular, we choose two
simple CCNN architectures shown in Figure @ref(fig:mesh-net)(d), denoted
by \(\mbox{CCNN}_{MOG1}\) and \(\mbox{CCNN}_{MOG2}\), as opposed to the
more complicated architecture of \(\mbox{CCNN}_{SHREC}\) in Figure
@ref(fig:mesh-net)(b). The main difference between
\(\mbox{CCNN}_{MOG1}\) and \(\mbox{CCNN}_{MOG2}\) is the choice of the
input feature vectors as described next.

\subsubsection{Mesh classification: CC-pooling with input vertex and
edge
features}\label{mesh-classification-cc-pooling-with-input-vertex-and-edge-features}

In this experiment, we consider the vertex feature vector to be the
position concatenated with the normal vectors for each vertex in the
underlying mesh. For the edge features, we compute the first ten
eigenvectors of the 1-Hodge Laplacian
(\citeproc{ref-dodziuk1976finite}{Dodziuk 1976};
\citeproc{ref-eckmann1944harmonische}{Eckmann 1944}) and attach a
10-dimensional feature vector to the edges of the underlying mesh. The
CC that we consider here is 3-dimensional, as it consists of the
triangular mesh (vertices, edges and faces) and of 3-cells. The 3-cells
are obtained using the MOG algorithm, and are used for augmenting each
mesh. We calculate the 3-cells via the MOG algorithm using the AGD
scalar function as input. We conduct this experiment using the CCNN
defined via the tensor diagram \(\mbox{CCNN}_{MOG1}\) given in Figure
@ref(fig:mesh-net)(d). During training, we augment each mesh with ten
additional meshes, with each of these additional meshes being obtained
by a random rotation as well as 0.1\% noise perturbation to the vertex
positions. We train \(\mbox{CCNN}_{MOG1}\) for 100 epochs using a
learning rate of 0.0002 and the standard cross-entropy loss, and obtain
an accuracy of 98.1\%. While the accuracy of \(\mbox{CCNN}_{MOG1}\) is
lower than the one we report for \(\mbox{CCNN}_{SHREC}\) (99.17\%) in
Table @ref(tab:shrec), we note that \(\mbox{CCNN}_{MOG1}\) requires a
significantly smaller number of replications for mesh augmentation to
achieve a similar accuracy (\(\mbox{CCNN}_{MOG1}\) requires 10, whereas
\(\mbox{CCNN}_{SHREC}\) required 30 replications).

\textbf{Architecture of \(\mbox{CCNN}_{MOG1}\)}. The tensor diagram
\(\mbox{CCNN}_{MOG1}\) of Figure @ref(fig:mesh-net)(d) corresponds to a
pooling CCNN. In particular, \(\mbox{CCNN}_{MOG1}\) pushes forward the
signal towards two different higher-order cells: the faces of the mesh
as well as the 3-cells obtained from the MOG algorithm.

\subsubsection{Mesh classification: CC-pooling with input vertex
features
only}\label{mesh-classification-cc-pooling-with-input-vertex-features-only}

In this experiment, we consider the position and the normal vectors of
the input vertices. The CC structure that we consider is the underlying
graph structure obtained from each mesh; i.e., we only use the vertices
and the edges, and ignore the faces. We augment this structure by
2-cells obtained via the MOG algorithm using the AGD scalar function as
input. We choose the network architecture to be relatively simpler than
\(\mbox{CCNN}_{MOG1}\), and report it in Figure @ref(fig:mesh-net)(d) as
\(\mbox{CCNN}_{MOG2}\). During training we augment each mesh with 10
additional meshes, with each of these additional meshes being obtained
by a random rotation as well as 0.05\% noise perturbation to the vertex
positions. We train \(\mbox{CCNN}_{MOG2}\) for 100 epochs using a
learning rate of 0.0003 and the standard cross-entropy loss, and obtain
an accuracy of 97.1\%.

\textbf{Architecture of \(\mbox{CCNN}_{MOG2}\) for mesh classification}.
The tensor diagram \(\mbox{CCNN}_{MOG2}\) of Figure
@ref(fig:mesh-net)(d) corresponds to a pooling CCNN. In particular,
\(\mbox{CCNN}_{MOG2}\) pushes forward the signal towards a single 2-cell
obtained from the MOG algorithm. Observe that the overall architecture
of \(\mbox{CCNN}_{MOG2}\) is similar in principle to AlexNet
(\citeproc{ref-krizhevsky2017imagenet}{Krizhevsky, Sutskever, and Hinton
2017}), where convolutional layers are followed by pooling layers.

\subsubsection{Point cloud classification: CC-pooling with input vertex
features
only}\label{point-cloud-classification-cc-pooling-with-input-vertex-features-only}

In this experiment, we consider point cloud classification on the
SHREC11 dataset. The setup is similar in principle to the one studied in
Section
@ref(mesh-classification-cc-pooling-with-input-vertex-features-only)
where we consider only the features supported on the vertices of the
point cloud as input. Specifically, for each mesh in the SHREC11
dataset, we sample 1,000 points from the surface of the mesh.
Additionally, we estimate the normal vectors of the resulting point
clouds using the Point Cloud Utils package
(\citeproc{ref-point-cloud-utils}{Williams 2022}). To build the CC
structure, we first consider the \(k\)-nearest neighborhood graph
obtained from each point cloud using \(k=7\). We then augment this graph
by 2-cells obtained via the MOG algorithm using the AGD scalar function
as input. We train the \(\mbox{CCNN}_{MOG2}\) shown in Figure
@ref(fig:mesh-net)(d). During training, we augment each point cloud with
12 additional instances, each one of these instances being obtained by
random rotation. We train \(\mbox{CCNN}_{MOG2}\) for 100 epochs using a
learning rate of 0.0003 and the standard cross-entropy loss, and obtain
an accuracy of 95.2\% (see Table @ref(tab:shrec)).

\subsection{Ablation studies}\label{ablation-studies}

In this section, we perform two ablation studies. The first ablation
study reveals that pooling strategies in CCNNs have a crucial effect on
predictive performance. The second ablation study demonstrates that
CCNNs have better predictive capacity than GNNs; the advantage of CCNNs
arises from their topological pooling operations and from their ability
to learn from topological features.

\textbf{Pooling strategies in CCNNs}. To evaluate the impact of the
choice of pooling strategy on predictive performance, we experiment with
two pooling strategies using the SHREC11 classification dataset. The
first pooling strategy is the MOG algorithm described in Section
@ref(pooling-with-mapper-on-graphs-and-data-classification); the results
of this pooling strategy based on \(\mbox{CCNN}_{MOG2}\) are discussed
in Section
@ref(mesh-classification-cc-pooling-with-input-vertex-features-only)
(97.1\%). The second pooling strategy is briefly described as follows.
For each mesh, we consider the 2-dimensional CC obtained by considering
each 1-hop neighborhood to be the 1-cells in the CC and each 2-hop
neighborhood to be the 2-cells in the CC. We train
\(\mbox{CCNN}_{MOG2}\), and obtain an accuracy of 89.2\%, which is lower
than 97.1\%. These experiments suggest that the choice of pooling
strategy has a crucial effect on predictive performance.

\textbf{Comparing CCNNs to GNNs in terms of predictive performance}.
Observe that \(\mbox{CCNN}_{SHREC}\) has topological features of
dimension one and two as inputs. On the other hand,
\(\mbox{CCNN}_{MOG2}\) has only vertex features as input, but it learns
the higher-order cell latent features by using the push-forward
operation that pushes the signal from 0-cells to the 2-cells obtained
from the MOG algorithm. In both cases, using a higher-order structure is
essential for improving predictive performance, even though two
different strategies towards exploiting the higher-order structures are
utilized. To support our claim, we run an experiment in which we replace
the pooling layer in \(\mbox{CCNN}_{MOG2}\) by the cochain operator
induced by \(A_{0,1}\), effectively rendering the neural network as a
GNN. In this setting, using the same setup as in experiment
@ref(mesh-classification-cc-pooling-with-input-vertex-features-only), we
obtain an accuracy of 84.56\%. This experiment reveals the performance
advantages of employing higher-order structures, either by utilizing the
input topological features supported on higher-order cells or via
pooling strategies that augment higher-order cells.

\section{Related work}\label{related-work}

Topological deep learning (TDL) has recently emerged as a new research
frontier that lies at the intersection of several areas, including
geometric and topological machine learning, and network science. To
demonstrate where TDL fits in the existing literature, we review a broad
spectrum of prior works, and categorize them into graph-based models,
higher-order deep learning models, graph-based pooling, attention-based
models, and applied algebraic topology.

\subsection{Graph-based models}\label{graph-based-models}

Graph-based models have been widely used for modeling pairwise
interactions (edges) between elements (vertices) of different systems,
including social systems (e.g., social network analysis) and biological
systems (e.g., protein-protein interactions), see
(\citeproc{ref-knoke2019social}{Knoke and Yang 2019};
\citeproc{ref-jha2022prediction}{Jha, Saha, and Singh 2022}). Based on
their edge or vertex properties, graphs can be classified as unweighted
graphs (unweighted edges), weighted graphs (weighted edges), signed
graphs (signed edges), undirected or directed graphs (undirected or
directed edges), and spatio-temporal graphs (spatio-temporal vertices),
as discussed in (\citeproc{ref-goyal2018graph}{Goyal and Ferrara 2018};
\citeproc{ref-wu2020comprehensive}{Zonghan Wu et al. 2020}). Each of
these graph types can be combined with neural networks to form graph
neural networks and model different interactions in various systems
(\citeproc{ref-goyal2018graph}{Goyal and Ferrara 2018};
\citeproc{ref-wu2020comprehensive}{Zonghan Wu et al. 2020}). For
example, unweighted and undirected graph-based models have been used for
omic data mapping (\citeproc{ref-amar2014constructing}{Amar and Shamir
2014}) and mutual friendship detection in social networks
(\citeproc{ref-tabassum2018social}{Tabassum et al. 2018}); weighted
graph-based models have been widely used with systems related to traffic
forecasting (\citeproc{ref-halaoui2010smart}{Halaoui 2010};
\citeproc{ref-zhang2018kernel}{Q. Zhang et al. 2018}) and
epidemiological modeling/forecasting
(\citeproc{ref-linka2020outbreak}{Linka et al. 2020};
\citeproc{ref-manriquez2021protection}{Manrı́quez, Guerrero-Nancuante,
and Taramasco 2021}); signed graph-based models are suitable for tasks
such as segmentation (\citeproc{ref-bailoni2022gasp}{Bailoni et al.
2022}) and clustering (\citeproc{ref-kunegis2010spectral}{Kunegis et al.
2010}; \citeproc{ref-gallier2016spectral}{Gallier 2016});
spatio-temporal graph-based models can describe systems that are
spatio-temporal in nature, such as human activity and different types of
motion (\citeproc{ref-yan2018spatial}{Yan, Xiong, and Lin 2018};
\citeproc{ref-bhattacharya2020step}{Bhattacharya et al. 2020};
\citeproc{ref-plizzari2021spatial}{Plizzari, Cannici, and Matteucci
2021}).

As graph-based approaches that utilize single-layer or monolayer graphs
cannot model multiple types of relations between vertices in a network
(\citeproc{ref-goyal2018graph}{Goyal and Ferrara 2018};
\citeproc{ref-wu2020comprehensive}{Zonghan Wu et al. 2020}), multilayer
or multiplex networks have been proposed
(\citeproc{ref-kivela2014multilayer}{Kivelä et al. 2014};
\citeproc{ref-zhang2020multiplex}{W. Zhang et al. 2020};
\citeproc{ref-chang2022graphrr}{Chang et al. 2022}). Similar to
monolayer graphs, multiplex networks contain vertices and edges, but the
edges exist in separate layers, where each layer represents a specific
type of interaction or relation. Multiplex networks have been used in
various applications, including multilayer modeling of the human brain
(\citeproc{ref-de2017multilayer}{De Domenico 2017};
\citeproc{ref-anand2023hodge}{Anand and Chung 2023}) and online gaming
(\citeproc{ref-chang2022graphrr}{Chang et al. 2022}). All these types of
networks can only model pairwise relations between vertices, motivating
the need for higher-order networks, as discussed in Section
@ref(the-utility-of-topology).

\subsection{Higher-order deep learning
models}\label{higher-order-deep-learning-models}

In recent years, there has been an increasing interest in higher-order
networks (\citeproc{ref-mendel1991tutorial}{Mendel 1991};
\citeproc{ref-battiston2020networks}{Battiston et al. 2020};
\citeproc{ref-bick2021higher}{Bick et al. 2021}) due to the ability of
these networks to adequately capture higher-order interactions.
Hodge-theoretic approaches, message passing schemes, and skip
connections have been developed for higher-order networks in the signal
processing and deep learning literature.

A Hodge-theoretic approach (\citeproc{ref-lim2020hodge}{Lim 2020}) over
simplicial complexes has been introduced by
(\citeproc{ref-barbarossa2020topological}{Barbarossa and Sardellitti
2020a}; \citeproc{ref-schaub2021signal}{Schaub et al. 2021}). This
effort has been extended to hypergraphs by
(\citeproc{ref-barbarossa2016introduction}{Barbarossa and Tsitsvero
2016}; \citeproc{ref-schaub2021signal}{Schaub et al. 2021}) and to cell
complexes by (\citeproc{ref-roddenberry2021signal}{T. Mitchell
Roddenberry, Schaub, and Hajij 2022};
\citeproc{ref-sardellitti2021topological}{Sardellitti, Barbarossa, and
Testa 2021}). The work of (\citeproc{ref-roddenberry2019hodgenet}{T.
Mitchell Roddenberry and Segarra 2019}) has defined an edge-based
convolutional neural network by exploiting the 1-Hodge Laplacian
operator for linear filtering
(\citeproc{ref-barbarossa2018learning}{Barbarossa, Sardellitti, and Ceci
2018}; \citeproc{ref-schaub2018denoising}{Schaub and Segarra 2018};
\citeproc{ref-barbarossa2020topological}{Barbarossa and Sardellitti
2020a}, \citeproc{ref-barbarossa2020topologicalmag}{2020b};
\citeproc{ref-schaub2021signal}{Schaub et al. 2021}).

Convolutional operators and message-passing algorithms have been
developed for higher-order neural networks. For example, a convolutional
operator on hypergraphs has been proposed by
(\citeproc{ref-arya2018exploiting}{Arya and Worring 2018};
\citeproc{ref-feng2019hypergraph}{Feng et al. 2019};
\citeproc{ref-jiang2019dynamic}{J. Jiang et al. 2019}) and has been
investigated further by (\citeproc{ref-jiang2019dynamic}{J. Jiang et al.
2019}; \citeproc{ref-gao2020hypergraph}{Y. Gao et al. 2020};
\citeproc{ref-bai2021hypergraph}{S. Bai, Zhang, and Torr 2021};
\citeproc{ref-bai2021multi}{J. Bai et al. 2021};
\citeproc{ref-giusti2022cell}{L. Giusti, Battiloro, Testa, et al. 2022};
\citeproc{ref-wu2022hypergraph}{H. Wu and Ng 2022};
\citeproc{ref-gong2023generative}{Gong, Higham, and Zygalakis 2023}). A
unifying framework for learning on graphs and hypergraphs has been
proposed recently in (\citeproc{ref-huang2021unignn}{Jing Huang and Yang
2021}). The authors in (\citeproc{ref-gao2022hgnn}{Y. Gao et al. 2022})
have introduced the so-called general hypergraph neural networks, which
constitute a multi-modal/multi-type data correlation modeling framework.
As for message passing on complexes, the work of
(\citeproc{ref-hajijcell}{Hajij, Istvan, and Zamzmi 2020}) has
introduced a higher-order message-passing framework that encompasses
those proposed by (\citeproc{ref-gilmer2017neural}{Gilmer et al. 2017};
\citeproc{ref-bunch2020simplicial}{Bunch et al. 2020};
\citeproc{ref-ebli2020simplicial}{Ebli, Defferrard, and Spreemann 2020};
\citeproc{ref-hayhoe2022stable}{Hayhoe et al. 2022}) and has utilized
various local neighborhood aggregation schemes. In
(\citeproc{ref-mitchell2022topological}{E. C. Mitchell et al. 2022}),
recurrent simplicial neural networks have been proposed and applied to
trajectory prediction. The authors in
(\citeproc{ref-calmon2022higher}{Calmon, Schaub, and Bianconi 2022})
have addressed the challenge of processing signals supported on multiple
cell dimensions concurrently, by introducing a coupling multi-signal
approach on higher-order networks that utilizes the Dirac operator.
Several simplicial and cellular neural networks have been introduced
recently, including (\citeproc{ref-burnssimplicial}{Burns and Fukai
2023}; \citeproc{ref-bodnar2021weisfeiler}{Bodnar et al. 2021};
\citeproc{ref-roddenberry2021signal}{T. Mitchell Roddenberry, Schaub,
and Hajij 2022}; \citeproc{ref-sardellitti2021topological}{Sardellitti,
Barbarossa, and Testa 2021};
\citeproc{ref-sardellitti2022topological}{Sardellitti and Barbarossa
2022}; \citeproc{ref-battiloro2023topological}{Battiloro et al. 2023};
\citeproc{ref-yang2023convolutional}{Yang and Isufi 2023}). For more
details, the reader is referred to the recent survey of
(\citeproc{ref-mathilde2023}{Papillon et al. 2023}) on TDL.

A generalization of skip connections
(\citeproc{ref-ronneberger2015u}{Ronneberger, Fischer, and Brox 2015};
\citeproc{ref-he2016deep}{He et al. 2016}) to simplicial complexes has
been introduced by (\citeproc{ref-hajij2022high}{Hajij, Ramamurthy, et
al. 2022}), which allows the training of higher-order deep neural
networks. The authors in (\citeproc{ref-morris2019weisfeiler}{Morris et
al. 2019}) have proposed a higher-order graph neural network that takes
into account higher-order graph structures at multiple scales. While
these methods allow for multi-way hierarchical coupling, the coupling is
isotropic and weight differences within a particular multi-way
connection can not be learned. These limitations can be alleviated by
attention-based models.

Higher-order models have achieved promising performance in several
real-world applications, including link prediction
(\citeproc{ref-hajij2022high}{Hajij, Ramamurthy, et al. 2022};
\citeproc{ref-piaggesi2022effective}{Piaggesi, Panisson, and Petri
2022}; \citeproc{ref-chen2021bscnets}{Yuzhou Chen, Gel, and Poor 2022}),
action recognition (\citeproc{ref-wang2023survey}{C. Wang et al. 2023}),
visual classification (\citeproc{ref-shi2018hypergraph}{Shi et al.
2018}), optimal homology generator detection
(\citeproc{ref-keros2021dist2cycle}{Keros, Nanda, and Subr 2022}), time
series (\citeproc{ref-santoro2023higher}{Andrea Santoro et al. 2023}),
dynamical systems (\citeproc{ref-majhi2022dynamics}{Majhi, Perc, and
Ghosh 2022}), spectral clustering
(\citeproc{ref-reddy2023clustering}{Reddy, Chepuri, and Borgnat 2023}),
node classification (\citeproc{ref-hajij2022high}{Hajij, Ramamurthy, et
al. 2022}), and trajectory prediction
(\citeproc{ref-benson2018simplicial}{Benson et al. 2018};
\citeproc{ref-roddenberry2021principled}{T. Mitchell Roddenberry, Glaze,
and Segarra 2021}).

\subsection{Attention-based models}\label{attention-based-models}

Real-world relational data is large, unstructured, sparse and noisy. As
a result, graph neural networks (GNNs) may learn suboptimal data
representations, and therefore may exhibit compromised performance
(\citeproc{ref-wu2020comprehensive}{Zonghan Wu et al. 2020};
\citeproc{ref-asif2021graph}{Asif et al. 2021};
\citeproc{ref-dai2021nrgnn}{Dai, Aggarwal, and Wang 2021}). To address
these issues, various attention mechanisms
(\citeproc{ref-chaudhari2021attentive}{Chaudhari et al. 2021}) have been
incorporated in GNNs, which allow to learn neural architectures that
detect the most relevant parts of a given graph while ignoring
irrelevant parts. Based on the used attention mechanism, existing graph
attention approaches can be divided into weight-based attention,
similarity-based attention, and attention-guided walk
(\citeproc{ref-boaz2019}{J. B. Lee et al. 2019}).

The majority of attention-based mechanisms, with the exception of
(\citeproc{ref-bai2021hypergraph}{S. Bai, Zhang, and Torr 2021};
\citeproc{ref-kim2020hypergraph}{E.-S. Kim et al. 2020};
\citeproc{ref-georgiev2022heat}{Georgiev, Brockschmidt, and Allamanis
2022}; \citeproc{ref-giusti2022cell}{L. Giusti, Battiloro, Testa, et al.
2022}; \citeproc{ref-giusti2022simplicial}{L. Giusti, Battiloro, Di
Lorenzo, et al. 2022}; \citeproc{ref-goh2022simplicial}{Goh, Bodnar, and
Lio 2022}), are designed for graphs. For example, the attention model
proposed by (\citeproc{ref-goh2022simplicial}{Goh, Bodnar, and Lio
2022}) is a generalization of the graph attention model of
(\citeproc{ref-velickovic2017graph}{Veličković et al. 2018}). In
(\citeproc{ref-giusti2022simplicial}{L. Giusti, Battiloro, Di Lorenzo,
et al. 2022}), the authors have utilized a model based on Hodge
decomposition, similar to the one suggested in
(\citeproc{ref-roddenberry2021principled}{T. Mitchell Roddenberry,
Glaze, and Segarra 2021}), to introduce an attention model for
simplicial complexes. The hypergraph attention models introduced in
(\citeproc{ref-kim2020hypergraph}{E.-S. Kim et al. 2020};
\citeproc{ref-bai2021hypergraph}{S. Bai, Zhang, and Torr 2021}) provide
alternative generalizations of the graph attention model of
(\citeproc{ref-velickovic2017graph}{Veličković et al. 2018}). The
aforementioned attention models neither allow nor combine higher-order
attention blocks of entities of different dimensions. This limits the
space of neural architectures and the scope of applications of existing
attention models.

\subsection{Graph-based pooling}\label{graph-based-pooling}

Several attempts have been made to emulate the success of image-based
pooling layers in the context of graphs. Some of the early work employs
popular graph clustering algorithms
(\citeproc{ref-kushnir2006fast}{Kushnir, Galun, and Brandt 2006};
\citeproc{ref-dhillon2007weighted}{Dhillon, Guan, and Kulis 2007}) to
achieve graph-based pooling architectures
(\citeproc{ref-bruna2013spectral}{Bruna et al. 2014}). Coarsening
operations have been applied to graphs to attain the invariance
properties needed in learning tasks
(\citeproc{ref-ying2018hierarchical}{Ying et al. 2018};
\citeproc{ref-mesquita2020rethinking}{Mesquita, Souza, and Kaski 2020};
\citeproc{ref-gao2021topology}{H. Gao, Liu, and Ji 2021}). The current
state-of-the-art graph-based pooling approaches mostly rely on
dynamically learning the pooling needed for the learning task
(\citeproc{ref-grattarola2022understanding}{Grattarola et al. 2022}).
This includes spectral methods (\citeproc{ref-ma2019graph}{Ma et al.
2019}), clustering methods such as DiffPool
(\citeproc{ref-ying2018hierarchical}{Ying et al. 2018}) and MinCut
(\citeproc{ref-bianchi2020spectral}{Bianchi, Grattarola, and Alippi
2020}), top-K methods (\citeproc{ref-gao2019graph}{H. Gao and Ji 2019};
\citeproc{ref-lee2019self}{J. Lee, Lee, and Kang 2019};
\citeproc{ref-zhang2021hierarchical}{Z. Zhang et al. 2021}), and
hierarchical graph pooling (\citeproc{ref-huang2019attpool}{Jingjia
Huang et al. 2019}; \citeproc{ref-lee2019self}{J. Lee, Lee, and Kang
2019}; \citeproc{ref-zhang2019hierarchical}{Z. Zhang et al. 2019},
\citeproc{ref-zhang2021hierarchical}{2021};
\citeproc{ref-li2020graph}{J. Li et al. 2020};
\citeproc{ref-pang2021graph}{Pang, Zhao, and Li 2021}). Pooling on
higher-order networks remains unstudied, with the exception of a general
simplicial complex pooling strategy developed by
(\citeproc{ref-cinque2022pooling}{Cinque, Battiloro, and Di Lorenzo
2022}) along the lines of the proposal made by
(\citeproc{ref-grattarola2022understanding}{Grattarola et al. 2022}).

\subsection{Applied algebraic
topology}\label{applied-algebraic-topology}

Although algebraic topology (\citeproc{ref-hatcher2005algebraic}{Hatcher
2005}) is a relatively old field, applications of this field have only
recently started to crystallize (\citeproc{ref-carlsson2009topology}{G.
Carlsson 2009};
\citeproc{ref-edelsbrunner2010computational}{Edelsbrunner and Harer
2010}). Indeed, topological constructions have been found to be natural
tools for the formulation of longstanding problems in many fields. For
instance, persistent homology
(\citeproc{ref-edelsbrunner2010computational}{Edelsbrunner and Harer
2010}) has been successful at finding solutions to various complex data
problems (\citeproc{ref-boyell1963hybrid}{Boyell and Ruston 1963};
\citeproc{ref-kweon1994extracting}{Kweon and Kanade 1994};
\citeproc{ref-bajaj1997contour}{Bajaj, Pascucci, and Schikore 1997};
\citeproc{ref-attene2003shape}{Attene, Biasotti, and Spagnuolo 2003};
\citeproc{ref-carr2004simplifying}{Carr, Snoeyink, and Panne 2004};
\citeproc{ref-LeeChungKang2011}{H. Lee et al. 2011b},
\citeproc{ref-LeeChungKang2011b}{2011a},
\citeproc{ref-LeeKangChung2012}{2012a},
\citeproc{ref-LeeKangChung2012b}{2012b};
\citeproc{ref-DabaghianMemoliFrank2012}{Dabaghian et al. 2012};
\citeproc{ref-nicolau2011topology}{Nicolau, Levine, and Carlsson 2011};
\citeproc{ref-lum2013extracting}{Lum et al. 2013};
\citeproc{ref-giusti2016two}{C. Giusti, Ghrist, and Bassett 2016};
\citeproc{ref-curto2017can}{Curto 2017};
\citeproc{ref-rosen2017using}{Rosen et al. 2017}). Recent years have
witnessed increased interest in the role of topology in machine learning
and data science (\citeproc{ref-hensel2021survey}{Hensel, Moor, and
Rieck 2021}; \citeproc{ref-DW22}{Dey and Wang 2022b}). Topology-based
machine learning models have been applied in many areas, including
topological signatures of data
(\citeproc{ref-biasotti2008describing}{Biasotti et al. 2008};
\citeproc{ref-carlsson2005persistence}{G. Carlsson et al. 2005};
\citeproc{ref-rieck2015persistent}{Rieck and Leitte 2015}), neuroscience
(\citeproc{ref-LeeChungKang2011}{H. Lee et al. 2011b},
\citeproc{ref-LeeChungKang2011b}{2011a},
\citeproc{ref-LeeKangChung2012}{2012a},
\citeproc{ref-LeeKangChung2012b}{2012b};
\citeproc{ref-DabaghianMemoliFrank2012}{Dabaghian et al. 2012};
\citeproc{ref-giusti2016two}{C. Giusti, Ghrist, and Bassett 2016};
\citeproc{ref-curto2017can}{Curto 2017}), bioscience
(\citeproc{ref-dewoskin2010applications}{DeWoskin et al. 2010};
\citeproc{ref-nicolau2011topology}{Nicolau, Levine, and Carlsson 2011};
\citeproc{ref-chan2013topology}{Chan, Carlsson, and Rabadan 2013};
\citeproc{ref-taylor2015topological}{Taylor et al. 2015};
\citeproc{ref-topaz2015topological}{Topaz, Ziegelmeier, and Halverson
2015}; \citeproc{ref-lo2016modeling}{Lo and Park 2016}), the study of
graphs (\citeproc{ref-HorakMaleticRajkovic2009}{Horak, Maletić, and
Rajković 2009}; \citeproc{ref-ELuYao2012}{Weinan, Jianfeng, and Yuan
2013}; \citeproc{ref-BampasidouGentimis2014}{Bampasidou and Gentimis
2014}; \citeproc{ref-CarstensHoradam2013}{Carstens and Horadam 2013};
\citeproc{ref-PetriScolamieroDonato2013}{Petri et al. 2013a},
\citeproc{ref-PetriScolamieroDonato2013b}{2013b};
\citeproc{ref-rieck2019persistent}{Rieck, Bock, and Borgwardt 2019};
\citeproc{ref-hajij2020efficient}{Hajij and Rosen 2020}), time series
forcasting (\citeproc{ref-zeng_topological_2021}{Zeng et al. 2021}),
Trojan detection (\citeproc{ref-hu_trigger_2022}{Hu et al. 2022}), image
segmentation (\citeproc{ref-hu_topology-preserving_2019}{Hu et al.
2019}), 3D reconstruction (\citeproc{ref-waibel_capturing_2022}{Waibel
et al. 2022}), and time-varying setups
(\citeproc{ref-edelsbrunner2004time}{Edelsbrunner et al. 2004};
\citeproc{ref-perea2015sw1pers}{Perea et al. 2015};
\citeproc{ref-maletic2016persistent}{Maletić, Zhao, and Rajković 2016};
\citeproc{ref-rieck_uncovering_2020}{Rieck et al. 2020}).

Topological data analysis (TDA) (\citeproc{ref-carlsson2009topology}{G.
Carlsson 2009};
\citeproc{ref-edelsbrunner2010computational}{Edelsbrunner and Harer
2010}; \citeproc{ref-ghrist2014elementary}{Ghrist 2014};
\citeproc{ref-love2020topological}{Love et al. 2023b};
\citeproc{ref-DW22}{Dey and Wang 2022b}) has emerged as a scientific
area that harnesses topological tools to analyze data and develop
machine learning algorithms. TDA has found many applications in machine
learning, including enhancing existing machine learning models
(\citeproc{ref-bentaieb2016topology}{BenTaieb and Hamarneh 2016};
\citeproc{ref-hofer2017deep}{Hofer et al. 2017};
\citeproc{ref-clough2019explicit}{Clough et al. 2019};
\citeproc{ref-bruel2019topology}{Gabrielsson et al. 2020};
\citeproc{ref-wangtopogan}{F. Wang et al. 2020};
\citeproc{ref-leventhal2023exploring}{Leventhal et al. 2023}), improving
the explainability of deep learning models
(\citeproc{ref-carlsson2020topological}{G. Carlsson and Gabrielsson
2020}; \citeproc{ref-elhamdadi2021affectivetda}{Elhamdadi, Canavan, and
Rosen 2021}; \citeproc{ref-love2023topological}{Love et al. 2023a}),
dimensionality reduction (\citeproc{ref-moor2020topological}{Moor et al.
2020}), filtration learning (\citeproc{ref-hofer2020graph}{Hofer et al.
2020}), and topological layers constructions
(\citeproc{ref-kim2020pllay}{K. Kim et al. 2020}). A notable research
trend has been the vectorization of persistence diagrams. Vector
representations of persistence diagrams are constructed in order to be
utilized in downstream machine learning tasks. These methods include
Betti curves (\citeproc{ref-umeda2017time}{Umeda 2017}), persistence
landscapes (\citeproc{ref-bubenik2015statistical}{Bubenik 2015}),
persistence images (\citeproc{ref-adams2017persistence}{Adams et al.
2017}), and other vectorization constructions
(\citeproc{ref-chen2015statistical}{Y.-C. Chen et al. 2015};
\citeproc{ref-kusano2016persistence}{Kusano, Hiraoka, and Fukumizu
2016}; \citeproc{ref-berry2020functional}{Berry et al. 2020}). A
unification of these methods has been proposed recently in
(\citeproc{ref-carriere_perslay_2020}{Carriere et al. 2020}).

Our work introduces combinatorial complexes (CCs) as a generalized
higher-order network on which deep learning models can be defined and
studied in a unifying manner. Hence, our work expands TDA by formalizing
deep learning notions in topological terms and by realizing
constructions in TDA, e.g., mapper
(\citeproc{ref-singh2007topological}{Singh et al. 2007}), in terms of
our TDL framework. The construction of CCs and of combinatorial complex
neural networks (CCNNs), which are neural networks defined on CCs, is
inspired by classical notions in algebraic topology
(\citeproc{ref-hatcher2005algebraic}{Hatcher 2005}) and in topological
quantum field theory (\citeproc{ref-turaev2016quantum}{Turaev 2016}),
and by recent advances in TDA (\citeproc{ref-collins2004barcode}{Collins
et al. 2004}; \citeproc{ref-carlsson2005persistence}{G. Carlsson et al.
2005}, \citeproc{ref-carlsson2008local}{2008};
\citeproc{ref-carlsson2006algebraic}{E. Carlsson, Carlsson, and De Silva
2006}; \citeproc{ref-carlsson2008persistent}{G. Carlsson and Mémoli
2008}; \citeproc{ref-carlsson2009theory}{G. Carlsson and Zomorodian
2009}; \citeproc{ref-carlsson2009topology}{G. Carlsson 2009}) as applied
to machine learning (\citeproc{ref-pun2018persistent}{Pun, Xia, and Lee
2018}; \citeproc{ref-DW22}{Dey and Wang 2022b}).

\section{Conclusions}\label{conclusions}

We have established a topological deep learning (TDL) framework that
enables the learning of representations for data supported on
topological domains. To this end, we have introduced combinatorial
complexes (CCs) as a new topological domain to model and characterize
the main components of TDL. Our framework provides a unification for
many concepts that may be perceived as separate in the current
literature. Specifically, we can reduce most deep learning architectures
presented thus far in the literature to particular instances of
combinatorial complex neural networks (CCNNs), based on computational
operations defined on CCs. Our framework thus provides a platform for a
more systematic exploration and comparison of the large space of deep
learning protocols on topological spaces.

\textbf{Limitations}. This work has laid out the foundations of a novel
TDL framework. While TDL has great potential, similar to other novel
learning frameworks, there are limitations, many of which are still not
well-understood. Specifically, some known limitations involve:

\begin{itemize}
\tightlist
\item
  \emph{Computational complexity}: The primary challenge for moving from
  graphs to richer topological domains is the combinatorial increase in
  the complexity to store and process data defined on such domains.
  Training a TDL network can be a computationally intensive task,
  requiring careful consideration of neighborhood functions and
  generalization performance. TDL networks also require a large amount
  of memory, especially when working with a large number of matrices
  during network construction. The topology of the network can also
  increase the computational complexity of training.
\item
  \emph{The choice of the neural network architecture}: Choosing the
  appropriate neural network architecture for a given dataset and a
  given learning task can be challenging. The performance of a TDL
  network can be highly dependent on the choice of architecture and its
  hyperparameters.
\item
  \emph{Interpretability and explainability}: The architecture of a TDL
  network can make it difficult to interpret the learnt representations
  and understand how the network is making predictions.
\item
  \emph{Limited availability of datasets}: TDL networks require
  topological data, which we have found to be of limited availability.
  Ad hoc conversions of existing data to include higher-order relations
  may not always be ideal.
\end{itemize}

\textbf{Future work}. Aforementioned limitations leave ample room for
future studies, making the realization of the full potential of TDL an
interesting endeavour. While the flexible definition of CCs, as compared
to, e.g., simplicial complexes, already provides some mitigation to the
associated computational challenges, improving the scaling of CCNNs even
further will require the exploration of sparsification techniques,
randomization and other algorithmic improvements. Besides addressing the
aforementioned limitations, promising directions not treated within this
paper include explorations of directed
(\citeproc{ref-ausiello2017directed}{Ausiello and Laura 2017}), weighted
(\citeproc{ref-battiloro2023topological}{Battiloro et al. 2023}),
multilayer (\citeproc{ref-menichetti2016control}{Menichetti, Dall'Asta,
and Bianconi 2016}), and time-varying dynamic topological domains
(\citeproc{ref-torres2021and}{Torres et al. 2021};
\citeproc{ref-anwar2022synchronization}{Anwar and Ghosh 2022};
\citeproc{ref-yin2022dynamic}{Yin et al. 2022}). There are also several
issues related to the selection of the most appropriate topological
domain for a given dataset in the first place, which need further
exploration in the future. Additionally, there is a need, but also a
research opportunity, to better understand CCNN architectures from a
theoretical perspective. This could in turn lead to better
architectures. To illustrate this point, consider graph neural networks
(GNNs) based on message passing (\citeproc{ref-gilmer2017neural}{Gilmer
et al. 2017}), which have recently been shown to be as powerful as the
Weisfeiler--Lehman isomorphism test\footnote{The Weisfeiler--Lehman
  isomorphism test (\citeproc{ref-weisfeiler1968reduction}{Weisfeiler
  and Leman 1968}) is a widely-used graph isomorphism algorithm that
  provides a coloring of a graph's vertices, and this coloring gives a
  necessary condition for two graphs to be isomorphic.}.
(\citeproc{ref-xu2018powerful}{K. Xu et al. 2018};
\citeproc{ref-maron2019provably}{Maron et al. 2019};
\citeproc{ref-morris2019weisfeiler}{Morris et al. 2019}). This
connection between GNNs and classical graph-based combinatorial
invariants has driven theoretical developments of the graph isomorphism
problem and has inspired new architectures
(\citeproc{ref-xu2018powerful}{K. Xu et al. 2018};
\citeproc{ref-maron2019provably}{Maron et al. 2019};
\citeproc{ref-arvind2020weisfeiler}{Arvind et al. 2020};
\citeproc{ref-bouritsas2020improving}{Bouritsas et al. 2023}). We expect
that connecting similar developments will also be important for TDL.

The topological viewpoint we adopt brings about many interesting
properties. For example, we are able to model other types of
\emph{topological inductive biases} in our computational architectures,
such as the properties that do not change under different
discretizations of the underlying domain, e.g., the Euler characteristic
that is commonly used to distinguish topological spaces. While
isomorphisms are the primary \emph{equivalence relation} in graph
theory, \emph{homeomorphisms} and \emph{topological equivalence} are
more relevant for data defined on topological spaces, and invariants
under homeomorphisms have different machine learning
applications\footnote{Intuitively, two topological spaces are equivalent
  if one of them can be deformed to the other via a continuous
  transformation.}. Homeomorphism equivalence is more relevant in
various applications in which domain discretization is an artifact of
data processing, and not an intrinsic part of the data
(\citeproc{ref-sharp2022diffusionnet}{Sharp et al. 2022}). Further,
homeomorphism equivalence translates to a similarity question between
two structures. Indeed, topological data analysis has been extensively
utilized towards addressing the problem of similarity between meshes
(\citeproc{ref-dey2010persistent}{Dey et al. 2010};
\citeproc{ref-hajij2018visual}{Hajij et al. 2018};
\citeproc{ref-rieck2019persistent}{Rieck, Bock, and Borgwardt 2019}). In
geometric data processing, neural network architectures that are
agnostic to mesh discretization are often desirable and perform better
in practice (\citeproc{ref-sharp2022diffusionnet}{Sharp et al. 2022}).
We anticipate that the development of TDL models will open up new
avenues to explore topological invariants across topological domains.

\cleardoublepage

\section*{(APPENDIX) Appendix}\label{appendix-appendix}
\addcontentsline{toc}{section}{(APPENDIX) Appendix}

\section{术语}\label{glossary}

本表列出了本文所涉及的相关概念

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6389}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
符号标记
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
描述
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \\
\(S\) & 抽象实体的非空有限集 \\
\(P_{S}\) & 索引集 \\
\(\mathcal{P}(S)\) & 集合\(𝑆\)的幂集 \\
\((S,\mathcal{N})\) & 非空集 \(S\) 和邻域拓扑 \(\mathcal{N}\)
构成的拓扑空间 \\
\(\mathcal{N}_{a}(x)\) & 胞腔\(x\)的邻接集合 \\
\(\mathcal{N}_{co}(x)\) & 胞腔\(x\)的共邻集合 \\
\(\mathcal{N}_{\{ G_1,\ldots,G_n\}}(x)\) &
由邻域矩阵\(\{G_1,\dots,G_n\}\)确定的\(x\)的邻居 \\
\(\mathcal{N}_{\searrow}(x)\) & Set of down-incidence of a cell \(x\) \\
\(\mathcal{N}_{\nearrow}(x)\) & Set of up-incidence of a cell \(x\) \\
\(\mathcal{N}_{\searrow,k}(x)\) & Set of \(k\)-down incidence of a cell
\(x\) \\
\(\mathcal{N}_{\nearrow,k}(x)\) & Set of \(k\)-up incidence of a cell
\(x\) \\
\(\mathbb{N}\) and \(\mathbb{Z}_{\ge 0}\) & Set of positive integers and
non-negative integers, respectively \\
& \\
\(\mathcal{G}\) & Graph \\
\(x^k\) & Cell \(x\) of rank \(k\) \\
\(\mbox{rk}\) & Rank function \\
\((S, \mathcal{X}, \mbox{rk})\) & CC, consisting of a set \(S\), a
subset \(\mathcal{X}\) of \(\mathcal{P}(S)\setminus\{\emptyset\}\), and
a rank function \(\mbox{rk}\) \\
\(\dim (\mathcal{X})\) & Dimension of a CC \(\mathcal{X}\) \\
\(\{c_\alpha\}_{\alpha \in I}\) & Partition into subspaces (cells)
indexed by an index set \(I\) \\
\(\mbox{int}(x)\) & Interior of a cell \(x\) in a regular cell
complex \\
\(n_\alpha\in \mathbb{N}\) & Dimension of a cell in a regular cell
complex \\
\(0\)-cells & Vertices of a CC \\
\(1\)-cells & Edges of a CC \\
\(k\)-cells & Cells with rank \(k\) \\
\(\mathcal{X}^{(k)}\) & \(k\)-skeleton of \(\mathcal{X}\), formed by
\(i\)-cells in \(\mathcal{X}\) with \(i\leq k\) \\
\(\mathcal{X}^k\) & Set of k-cells of \(\mathcal{X}\) \\
\(|\mathcal{X}^k|\) & Cardinality of \(\mathcal{X}^k\), that is number
of \(k\)-cells of \(\mathcal{X}\) \\
\(\mathcal{X}_{n-hop}(G)\) & \(n\)-hop CC of a graph \(G\) \\
\(\mathcal{X}_p(G)\) & Path-based CC of a graph \(G\) \\
\(\mathcal{X}_{loop}(G)\) & Loop-based CC of a graph \(G\) \\
\(\mathcal{X}_{SC}(\mathcal{Y})\) & Coface CC of a simplicial complex/CC
\(\mathcal{Y}\) \\
& \\
\(B_{r,k}\) & Incidence matrices between \(r\)-cells and \(k\)-cells \\
\(A_{r,k}\) & Adjacency matrices among the cells of \(\mbox{X}^{r}\)
with respect to the cells of \(\mbox{X}^{k}\) \\
\(coA_{r,k}\) & Coadjacency matrices among the cells of \(\mbox{X}^{r}\)
with respect to the cells of \(\mbox{X}^{k}\) \\
& \\
\(\mathbf{W}\) & Trainable parameter \\
\(\mathcal{C}^k(\mathcal{X},\mathbb{R}^d)\) & \(k\)-cochain space with
features in \(\mathbb{R}^d\) \\
\(\mathcal{C}^k\) & \(k\)-cochain space with features in some Euclidean
space \\
\(\mathbf{G}= \{G_1,\ldots,G_m\}\) & Set of cochain maps \(G_i\) defined
on a complex \\
\(\mathcal{M}_{ \mathbf{G};\mathbf{W}}\) & Merge node \\
\(G:C^{s}(\mathcal{X})\to C^{t}(\mathcal{X})\) & Cochain map \\
\((\mathbf{x}_{i_1},\ldots, \mathbf{x}_{i_m})\) & Vector of cochains \\
\(att^{l}: C^{s}(\mathcal{X})\to C^{s}(\mathcal{X})\) & Higher-order
attention matrix \\
\(\mathcal{N}_{\mathcal{Y}_0}=\{\mathcal{Y}_1,\ldots,\sigma_{|\mathcal{N}_{\mathcal{Y}_0}|}\}\)
& Set of a complex object in the vicinity of \(\mathcal{Y}_0\) \\
\(a: {\mathcal{Y}_0}\times \mathcal{N}_{\mathcal{Y}_0}\to [0,1]\) &
Higher-order attention function \\
\(\mbox{CCNN}_{\mathbf{G};\mathbf{W}}\) & CCNN or its tensor diagram
representation \\
\(\mathcal{H}_{\mathcal{X}}= (V (\mathcal{H}_{\mathcal{X}}), E(\mathcal{H}_{\mathcal{X}}) )\)
& Hasse graph with vertices \(V (\mathcal{H}_{\mathcal{X}})\) and edges
\(E(\mathcal{H}_{\mathcal{X}})\); see Definition @ref(def:hg) \\
\end{longtable}

The table below summarizes the paper's acronyms.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Acronym & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
AGD & Average geodesic distance \\
CC & Combinatorial complex \\
CCANN & Combinatorial complex attention neural network \\
CCCNN & Combinatorial complex convolutional neural network \\
CCNN & Combinatorial complex neural network \\
CNN & Convolutional neural network \\
DEC & Discrete exterior calculus \\
GDL & Geometric deep learning \\
GNN & Graph neural network \\
MOG & Mapper on graphs \\
RNN & Recurrent neural network \\
SCoNe & Simplicial complex network \\
Sub-CC & sub-combinatorial complex \\
TDA & Topological data analysis \\
TDL & Topological deep learning \\
TQFT & Topological quantum field theory \\
\end{longtable}

\section{Lifting maps}\label{lifting-maps}

Lifting refers to the process of mapping a featured domain to another
featured domain via a well-defined procedure. This section shows how we
can lift a given domain to a CC or cell complex. Such lifting is useful
as it allows CCNNs to be applied to common topological domains,
including graphs and cell/simplicial complexes. This section only
scratches the surface, as there remain many lifting constructions to be
explored. We refer the reader to
(\citeproc{ref-ferri2018simplicial}{Ferri, Bergomi, and Zu 2018}) for
examples of lifting graphs to simplicial complexes.

\subsection{n-hop CC of a graph}\label{n-hop-cc-of-a-graph}

Let \(\mathcal{G}=(V(\mathcal{G}),E(\mathcal{G}))\) be a graph and
\(n\geq 2\) an integer. The \(n\)\emph{-hop CC} of \(\mathcal{G}\),
denoted by \(\mbox{CC}_{n-\mbox{hop}}(\mathcal{G})\), is the CC whose
\(0\)-cells, \(1\)-cells, and \(n\)-cells are the nodes of
\(\mathcal{G}\), edges of \(\mathcal{G}\), and set nodes in \(n\)-hop
neighborhoods of the nodes in \(\mathcal{G}\), respectively. It is easy
to verify that \(\mbox{CC}_{n-\mbox{hop}}(\mathcal{G})\) is a CC of
dimension \(n\). Figure @ref(fig:liftings)(a) visualizes the \(1\)-hop
CC of a graph.

\begin{figure}

{\centering \includegraphics{figures/lifting_maps} 

}

\caption{Examples of lifting domains to CCs and cell complexes. (a): The $1$-hop neighborhood of the red node can be considered as a 2-cell that we can augment to the graph. Adding such 2-cells to a graph yields a CC called the 1-hop neighborhood of the graph. (b): A path on a graph of length more than two can be considered as a 2-cell that we can augment to the graph. Adding such 2-cells to a graph yields a CC called a path-based CC of the graph. (c): A loop in a graph (i.e., a closed path with no repeating edges) can be considered as a 2-cell that we can augment to the graph. Adding such 2-cells to a graph yields a CC called a loop-based CC of the graph. (d): For every blue 2-cell of a simplicial complex, we introduce a green 3-cell obtained by considering the 1-coface of the 2-cell. Adding such 3-cells to a simplicial complex yields a CC of dimension three called the coface CC of the simplicial complex.}\label{fig:liftings}
\end{figure}

\subsection{Path-based and subgraph-based CC of a
graph}\label{path-based-and-subgraph-based-cc-of-a-graph}

Let \(\mathcal{G}=(V(\mathcal{G}),E(\mathcal{G}))\) be a graph. A
natural CC structure on \(\mathcal{G}\) considers paths of
\(\mathcal{G}\). We define a \emph{path-based CC} of \(\mathcal{G}\),
denoted by \(\mbox{CC}_p(\mathcal{G})\), to be a CC consisting of
\(0\)-cells, \(1\)-cells and \(2\)-cells specified as follows. First,
\(\mathcal{X}^0\) and \(\mathcal{X}^1\) in \(\mbox{CC}_p(\mathcal{G})\)
are the sets of nodes and edges of \(\mathcal{G}\), respectively. We now
explain how to construct a \(2\)-cell in \(\mbox{CC}_p(\mathcal{G})\).
Let \(P\) be a path in \(\mathcal{G}\) with length larger than or equal
to two (i.e., with two or more edges). An element \(x_P\) in
\(\mathcal{X}^2\) induced by \(P\) is defined to be
\(x_P=\cup_{v\in P }\{v\}\). The set \(\mathcal{X}^2\) in
\(\mbox{CC}_p(\mathcal{G})\) is a non-empty collection of elements
\(x_P\). It is easy to verify that \(\mbox{CC}_p(\mathcal{G})\) is a CC
with \(\dim(\mbox{CC}_p(\mathcal{G}))=2\). Note that we may replace the
path \(P\) by a tree/subgraph of graph \(\mathcal{G}\) and obtain a
similar CC structure induced by the tree/subgraph of \(\mathcal{G}\).
Figure @ref(fig:liftings)(b) shows an example of a path-based CC of a
graph.

\subsection{Loop-based CC of a graph}\label{loop-based-cc-of-a-graph}

Let \(\mathcal{G}=(V(\mathcal{G}),E(\mathcal{G}))\) be a graph. We
associate a CC structure with \(\mathcal{G}\) that considers loops in
\(\mathcal{G}\). We define a \emph{loop-based CC} of \(\mathcal{G}\),
denoted by \(\mbox{CC}_{loop}(\mathcal{G})\), to be a CC consisting of
\(0\)-cells, \(1\)-cells and \(2\)-cells specified as follows. First, we
set \(\mathcal{X}^0\) and \(\mathcal{X}^1\) in
\(\mbox{CC}_{loop}(\mathcal{G})\) to be the nodes and edges of
\(\mathcal{G}\), respectively. We now explain how to construct a
\(2\)-cell in \(\mbox{CC}_{loop}(\mathcal{G})\). A 2-cell in
\(\mbox{CC}_{loop}(\mathcal{G})\) is a set
\(C=\{x^0_1, \ldots , x^0_k\} \subset \mathcal{X}^0\) such that
\(\{x^0_i,x^0_{i+1}\}\), \(1 \leq  i \leq k - 1\), and
\(\{x^0_k, x^0_1\}\) are the only edges in \(\mathcal{X}^1 \cap C\). The
set \(\mathcal{X}^2\) in \(\mbox{CC}_{loop}(\mathcal{G})\) is a nonempty
collection of elements \(C\). It is easy to verify that
\(\mbox{CC}_{loop}(\mathcal{G})\) is a CC with
\(\dim(\mbox{CC}_{loop}(\mathcal{G}))=2\). Note that the sequence
\((x^0_1, \ldots , x^0_k)\) defines a loop in \(\mathcal{G}\). This loop
is called the loop that characterizes the 2-cell
\(C=\{x^0_1, \ldots , x^0_k\}\). Similar constructions are suggested in
(\citeproc{ref-aschbacher1996combinatorial}{Aschbacher 1996};
\citeproc{ref-basak2010combinatorial}{Basak 2010};
\citeproc{ref-savoy2022combinatorial}{Savoy 2021};
\citeproc{ref-roddenberry2021signal}{T. Mitchell Roddenberry, Schaub,
and Hajij 2022}). In fact, it is easy to confirm that every
2-dimensional regular cell complex can be constructed in this manner
(\citeproc{ref-roddenberry2021signal}{T. Mitchell Roddenberry, Schaub,
and Hajij 2022}). Figure @ref(fig:liftings)(c) shows an example of a
loop-based CC of a graph.

\subsection{Coface CC of a simplicial complex or of a
CC}\label{coface-cc-of-a-simplicial-complex-or-of-a-cc}

Here, we describe a method to lift a simplicial complex of dimension two
to a CC of dimension three. This method can be easily generalized to
other dimensions. For a simplicial complex \(\mathcal{Y}\) of dimension
two, the \emph{coface CC} of \(\mathcal{Y}\), denoted by
\(\mbox{CC}_{SC}( \mathcal{Y})\), is defined as follows.
\(\mathcal{Y}^0\), \(\mathcal{Y}^1\), and \(\mathcal{Y}^2\) in
\(\mbox{CC}_{SC}( \mathcal{Y})\) are the nodes, the edges, and the
triangles in \(\mathcal{Y}\), respectively. We now explain how to
construct a \(3\)-cell in \(\mbox{CC}_{SC}( \mathcal{Y})\). Let \(x^2\)
be a 2-cell in \(\mathcal{Y}\). The 3-cell in
\(\mbox{CC}_{SC}( \mathcal{Y})\) associated with \(x^2\) is the union of
all 0-cells in \(\mathcal{N}_{co,1}(x^2) \cup x^2\). The set
\(\mathcal{Y}^3\) in \(\mbox{CC}_{SC}( \mathcal{Y})\) is defined as the
set of all 3-cells associated with all 2-cells \(x^2\) in
\(\mathcal{Y}\). It is easy to verify that
\(\mbox{CC}_{SC}(\mathcal{Y})\) is a CC with
\(\dim(\mbox{CC}_{SC}(\mathcal{Y}) )=3\). A similar lifting construction
can be defined to augment any CC of dimension \(n\) with \((n+1)\)-cells
in order to obtain a CC of dimension \(n+1\).

\subsection{Augmentation of CCs by higher-rank
cells}\label{augmentation-of-ccs-by-higher-rank-cells}

The lifting methods proposed in Sections
@ref(path-based-and-subgraph-based-cc-of-a-graph),
@ref(loop-based-cc-of-a-graph), and
@ref(coface-cc-of-a-simplicial-complex-or-of-a-cc) can be described
abstractly under a single general lifting construction. Specifically,
the essence of all these lifting methods is to \emph{augment the
underlying CC \(\mathcal{X}\) with new cells that have a rank of
\(\dim(\mathcal{X})+1\)}. Proposition @ref(prp:augmented-cc) formalizes
the general lifting construction.

\phantomsection\label{augmented-cc}
Let \(S\) be a nonempty set and \((S,\mathcal{X},\mbox{rk})\) a CC of
dimension \(n\) defined on \(S\). Consider a set
\(\mathcal{X}^{n+1} \subset \mathcal{P}(S)\) such that if
\(x\in\mathcal{X}\) and \(y \in \mathcal{X}^{n+1}\) with
\(x\subseteq y\), then \(x\subsetneq y\). Further, consider a map
\(\hat{\mbox{rk}}\colon \mathcal{X}\cup \mathcal{X}^{n+1}\to \mathbb{Z}_{\ge 0}\)
that satisfies \(\hat{\mbox{rk}}(x)= \mbox{rk}(x)\) for all
\(x\in\mathcal{X}\) and \(\hat{\mbox{rk}}(x)=n+1\) for all
\(x \in \mathcal{X}^{n+1}\). For \(\mathcal{X}^{n+1}\) and
\(\hat{\mbox{rk}}\) satisfying such conditions,
\((S,\mathcal{X}\cup \mathcal{X}^{n+1},\hat{\mbox{rk}} )\) is a CC of
dimension \(n+1\).

The proof follows directly from Definition @ref(def:cc).

Given a CC \(\mathcal{X}\), we call a CC of the form
\((S,\mathcal{X}\cup \mathcal{X}^{n+1},\hat{\mbox{rk}} )\), as
constructed in Proposition @ref(prp:augmented-cc), a \emph{highest-rank
augmented CC of \(\mathcal{X}\)}. Note that Proposition
@ref(prp:augmented-cc) provides a constructive and iterative method to
build a CC of arbitrary dimension from a nonempty set \(S\) of abstract
points.

\section{CCNN architecture search and topological quantum field
theories}\label{ccnn-architecture-search-and-topological-quantum-field-theories}

The problem of CCNN architecture search for a given TDL task can be cast
as a hyperparameter optimization problem over the space of CCNNs. More
precisely, consider the query of searching for an optimal CCNN between
two fixed Cartesian products
\(\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times  \mathcal{C}^{i_m}\)
and
\(\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times \mathcal{C}^{j_n}\)
of cochain spaces. In practice, this query poses a challenging problem.
Rather than performing a computationally expensive CCNN search directly,
an alternative approach is to conduct a search in the simpler space of
\emph{marked trivalent graphs} between marked points
\(\{i_1,\ldots,i_m\}\) and \(\{j_1,\ldots,j_n\}\), and then map the
resulting marked trivalent graph to a corresponding CCNN architecture.
In the present section, we briefly sketch such a graph-based search
method using tools from \emph{topological quantum field theory (TQFT)}
(\citeproc{ref-turaev2016quantum}{Turaev 2016}), and accordingly we
assume some familiarity with the basics of \emph{category theory}.

Tensor diagrams draw their inspiration from TQFT, in which arbitrary
maps between topological spaces are constructed from simpler and more
manageable building blocks. The main workflow in TQFT constructions
involves breaking down the topological spaces under consideration into
simpler subspaces, and subsequently utilizing maps between the subspaces
to construct maps between the initial topological spaces; see Figure
@ref(fig:functor)(a). Thus, the topological properties of maps between
topological spaces are better understood via the topological properties
of simpler constituent maps between respective subspaces. This can be
especially useful in applications such as knot theory or the study of
three-dimensional manifolds, where understanding the topological
properties of involved maps is the main interest
(\citeproc{ref-turaev2016quantum}{Turaev 2016}).

\begin{figure}

{\centering \includegraphics{figures/functor} 

}

\caption{A sketch of the main idea of a TQFT construction in (a) and a functor $F_{tri}$ for constructing tensor diagrams in (b). (a): In the depicted example, the goal is to construct a map $f\colon A\to B$ between two topological spaces $A$ and $B$. We first decompose $A$ and $B$ into simpler sub-spaces, say $A = A_1 \cup A_2$ and $B = B_1 \cup B_2$, so that ${A_1,A_2}$ and ${B_1,B_2}$ are `more elementary spaces' than $A$ and $B$, respectively. We then construct two maps $f_1 \colon A_1 \to B_1$ and $f_2 \colon A_2 \to B_2$, and use them to construct $f$. (b): A visual example of a functor $F_{tri}$ that pairs each marked trivalent graph with a corresponding tensor diagram. This example sends a marked trivalent graph with marked points $\{0,1,2\}$ at the bottom and marked points $\{1,2\}$ at the top to a tensor diagram with domain $\mathcal{C}^{0} \times \mathcal{C}^{1}\times \mathcal{C}^{2}$ and codomain $\mathcal{C}^{1}\times \mathcal{C}^{2}$. Once the push-forward, the merge and the split nodes are defined, the functor $F_{tri}$ attaches a well-defined tensor diagram to each trivalent graph. A CCNN can be then constructed from the tensor diagram.}\label{fig:functor}
\end{figure}

Before we introduce the relation between tensor diagrams and TQFTs, we
sketch two required preliminaries, namely marked trivalent graphs and
TQFTs. First, we define the objects and morphisms of the category of
\emph{marked trivalent graphs} \(Tri\). A marked trivalent graph is
intuitively described via its layout. Consider the 2d-disk
\([0,1]\times [0,1]\) with a collection of marked points
\(\{i_1,\ldots,i_m\}\) at the bottom and a collection of marked points
\(\{j_1,\ldots,j_n\}\) at the top. A marked trivalent graph is a graph
that is drawn inside the disk in such a way that all edges flow within
the disk, connecting the marked points at the top with the marked points
at the bottom, and meeting at each vertex with exactly three edges. In
the category \(Tri\), a trivalent graph with marked points
\(\{i_1,\ldots,i_m\}\) and \(\{j_1,\ldots,j_n\}\) at the bottom and top,
respectively, plays the role of a morphism between the object
\(\{i_1,\ldots,i_m\}\) and the object \(\{j_1,\ldots,j_n\}\). The
composition of two such morphisms, when admissible, is defined to be the
vertical concatenation of their corresponding marked trivalent graphs.
The vertical concatenation of marked trivalent graphs yields a marked
trivalent graph. Finally, any trivalent graph can be built by
concatenating horizontally and vertically three elementary trivalent
graphs similar to the graph shown in Figure @ref(fig:functor)(b),
ignoring the labels in the figure.

At a high level, a TQFT is a functor \(F\colon Bord_{n}\to Vec_{K}\)
that assigns a \(K\)-vector space \(F(x)\in Vec_{K}\) to each
\(n\)-manifold \(x \in Bord_{n}\), and a linear map \(F(f)\) to each
\((n+1)\)-cobordism \(f \in Bord_{n}\). It is noted that a
\((n+1)\)-cobordism is an \((n+1)\)-manifold representing a morphism
between two \(n\)-manifolds. The functor \(F\) is typically defined on a
few elementary morphisms in \(Bord_{n}\), but can be extended to
arbitrary morphisms in \(Bord_{n}\) by preserving the structure of the
elementary maps when mapped via \(F\). Such a functor enables the study
of \(n\)-manifolds and \((n+1)\)-cobordisms defined between them by
mapping \(n\)-manifolds to their corresponding simpler vector spaces in
\(Vec_{K}\) and by mapping \((n+1)\)-cobordisms to linear maps between
the corresponding vector spaces.

To establish the relation between marked trivalent graphs and tensor
diagrams, we introduce a new TQFT \(F_{tri}\) that sends each morphism
(marked trivalent graph) in \(Tri\) to a corresponding tensor diagram by
labeling marked trivalent graphs with appropriate cochain maps. Figure
@ref(fig:functor)(b) shows an illustration of such a functor
\(F_{tri}\). Through the lens of \(F_{tri}\), the relation between
tensor diagrams and marked trivalent graphs becomes clear; a tensor
diagram is a marked trivalent graph whose edges are labeled via cochain
maps. The functor \(F_{tri}\) allows us to search the space of marked
trivalent graphs as an equivalent way of conducting CCNN architecture
search, since CCNNs are represented by tensor diagrams.

\section{Learning discrete exterior calculus operators with
CCANNs}\label{learning-discrete-exterior-calculus-operators-with-ccanns}

The operator \(G_{tr}=G\odot att\) of Equations @ref(eq:attention1) and
@ref(eq:attention2) has an advantageous cross-cutting interpretation.
First, recall that \(G_{tr}\) has the same shape as the original
operator \(G\). More importantly, \(G_{tr}\) can be viewed as a learnt
version of \(G\). For instance, if \(G\) is the \(k\)-Hodge Laplacian
\(\mathbf{L}_k\), then the learnt attention version \(G_{tr}\) of it
represents a \(k\)-Hodge Laplacian that is adapted to the domain
\(\mathcal{X}\) for the learning task at hand. This perspective converts
our attention framework to a tool for learning \emph{discrete exterior
calculus (DEC) operators} (\citeproc{ref-desbrun2008discrete}{Desbrun,
Kanso, and Tong 2008}). We refer the interested reader to recent works
along these lines (\citeproc{ref-smirnov2021hodgenet}{Smirnov and
Solomon 2021}; \citeproc{ref-trask2022enforcing}{Trask, Huang, and Hu
2022}), where neural networks are used to learn Laplacian operators in
various shape analysis tasks.

Concretely, one of the main building blocks of DEC is a collection of
linear operators of the form
\(\mathcal{A} \colon \mathcal{C}^i(\mathcal{X}) \to \mathcal{C}^j(\mathcal{X})\)
that act on a cochain \(\mathbf{H}\) to produce another cochain
\(\mathcal{A}(\mathbf{H})\). An example of an operator \(\mathcal{A}\)
is the graph Laplacian. There are seven primitive DEC operators,
including the discrete exterior derivative, the hodge star and the wedge
product. These seven primitive operators can be combined together to
form other operators. In our setting, the discrete exterior derivatives
are precisely a signed version of the incidence matrices defined in the
context of cell/simplicial complexes. We denote the \(k\)-signed
incidence matrix defined on a cell/simplicial complex by
\(\mathbf{B}_k\). It is common in the context of discrete exterior
calculus (\citeproc{ref-desbrun2008discrete}{Desbrun, Kanso, and Tong
2008}) to refer to \(\mathbf{B}_k^T\) as the \(k^{th}\) \emph{discrete
exterior derivative} \(d^k\). So, from a DEC point of view, the matrices
\(\mathbf{B}_0^T, \mathbf{B}_1^T\) and \(\mathbf{B}_2^T\) are regarded
as the discrete exterior derivatives \(d^0(\mathbf{H})\),
\(d^1 (\mathbf{H})\), and \(d^2 (\mathbf{H})\) of some 0-, 1-, and
2-cochains defined on \(\mathcal{\mathcal{X}}\), which in turn are the
discrete analogs of the gradient \(\nabla \mathbf{H}\), curl
\(\nabla\times \mathbf{H}\) and divergence \(\nabla \cdot \mathbf{H}\)
of a smooth function defined on a smooth surface. We refer the reader to
(\citeproc{ref-desbrun2008discrete}{Desbrun, Kanso, and Tong 2008}) for
a coherent list of DEC operators and their interpretation. Together,
cochains and the operators that act on them provide a concrete framework
that facilitates computing a cochain of interest, such as a cochain
obtained by solving a partial differential equation on a discrete
surface.

Our attention framework can be viewed as a non-linear version of the DEC
based on linear operators \(\mathcal{A}\), and can be used to learn the
DEC operators on a domain \(\mathcal{X}\) for a particular learning
task. Specifically, a linear operator \(\mathcal{A}\), as it appears in
classical DEC, can be considered as a special case of Equations
@ref(eq:attention1) and @ref(eq:attention2). Unlike existing work
(\citeproc{ref-smirnov2021hodgenet}{Smirnov and Solomon 2021};
\citeproc{ref-trask2022enforcing}{Trask, Huang, and Hu 2022}), our DEC
learning approach based on CCANNs generalizes and applies to all domains
in which DEC is typically applicable; examples of such domains include
triangular and polygonal meshes (\citeproc{ref-crane2013digital}{Crane
et al. 2013}). In contrast, existing operator learning methods are
defined only for particular types of DEC operators, and therefore cannot
be used to learn arbitrary types of DEC operators.

\section{A mapper-induced topology-preserving CC-pooling
operation}\label{a-mapper-induced-topology-preserving-cc-pooling-operation}

In this section, we give an example of constructing a shape-preserving
pooling operation on a CC \(\mathcal{X}\). Specifically, we demonstrate
the case in which \(\mathcal{X}\) is a graph, and utilize the
\emph{mapper on graphs (MOG)} construction
(\citeproc{ref-hajij2018mog}{Hajij, Wang, and Rosen 2018}), which is a
graph skeletonization algorithm that can be used to augment
\(\mathcal{X}\) with topology-preserving higher-rank cells, as
demonstrated in Figure @ref(fig:hs). Although we only demonstrate the
shape-preserving pooling construction on graphs, the method suggested
herein can be easily extended to CCs.

Let \(\mathcal{X}\) be a connected graph and
\(g\colon\mathcal{X}^{0} \to [0,1]\) a scalar function. Let
\(\mathcal{U}=\{U_\alpha\}_{\alpha \in I}\) be a finite collection of
open sets that covers the interval \([0,1]\). The MOG construction of a
graph \(MOG(V_{MOG},E_{MOG})\) based on the triplet (\(\mathcal{X}\),
\(g\), \(\mathcal{U}\)) consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first use the cover \(\mathcal{U}\) to construct the pull-back
  cover
  \(g^{\star}(\mathcal{U})=\{g^{-1}(U_{\alpha})\}_{\alpha \in I}\).
\item
  The vertex set \(V_{MOG}\) is formed by considering the connected
  components (i.e., maximal connected subgraphs) induced by
  \(g^{-1}(U_\alpha)\) for each \(\alpha\).
\item
  The edge set \(E_{MOG}\) is formed by considering the intersection
  among the connected components computed in step 2.
\end{enumerate}

Figure @ref(fig:mog) shows an illustrative example of applying the MOG
algorithm to a graph. Figure @ref(fig:mog)(a) shows the graph on which
the MOG algorithm is applied, while Figure @ref(fig:mog)(b) visualizes
the scalar function \(g\) and the covering \(\mathcal{U}\), which
consists of four covering elements depicted in red, orange, yellow and
blue colors.

\begin{figure}

{\centering \includegraphics{figures/graph_tda_appendix} 

}

\caption{An illustration of the MOG algorithm. The input to the MOG algorithm is a triplet $(\mathcal{X}, g, \mathcal{U})$, where $\mathcal{X}$ is a graph, $g\colon\mathcal{X}^0\to [0,1]$ is a scalar function defined on the vertex set of $\mathcal{X}$, and  $\mathcal{U}$ is a cover of $[0,1]$. (a): An input graph $\mathcal{X}$. (b): The scalar function $g\colon\mathcal{X}\to [0,1]$ is visualized by color-mapping its scalar values according to the displayed color bar. Figure (b) also shows the covering $\mathcal{U}= \{U_1,U_2, U_3, U_4\}$ depicted in red, orange, yellow and blue colors. (c): We pull back via $g$ each cover element $U_i$ in $\mathcal{U}$, and we compute the connected components in $g^{-1}(U_i)$. (d): The vertex set in the graph generated by the MOG algorithm consists of the connected components induced by $g^{-1}(U_i)$, while the edge set is formed by considering the intersection among the connected components. Note that the graph generated by the MOG algorithm approximates the shape of the input graph.}\label{fig:mog}
\end{figure}

The connected components obtained from the MOG algorithm can be used to
augment a graph \(\mathcal{X}\) with a skeleton \(\mathcal{X}^2\) of
dimension 2, thus constructing a CC, as described in Proposition
@ref(prp:augmented-cc). We denote the resulting CC by
\(\mathcal{X}_{g,\mathcal{U}}\). Figure
@ref(fig:mapper-as-pooling-operator) demonstrates the construction of a
CC using this augmentation process based on the MOG algorithm.

\begin{figure}

{\centering \includegraphics{figures/augmented_mapper_cc} 

}

\caption{A visual example of obtaining a CC from a graph via the MOG algorithm. (a): An input graph $\mathcal{X}$. (b): The graph $\mathcal{X}$ is augmented by the $2$-cells formed via the connected components obtained from applying the MOG algorithm to $\mathcal{X}$. (c): The CC $X_{g,\mathcal{U}}$ obtained by augmenting $\mathcal{X}$ with these $2$-cells.}\label{fig:mapper-as-pooling-operator}
\end{figure}

MOG is a topology-preserving graph skeletonization algorithm, which can
be used to coarsen the size of an input graph \(\mathcal{X}\). Such
coarsening typically occurs when constructing a pooling layer. So, the
skeleton \(\mathcal{X}^2\) generated by the MOG algorithm can be
utilized to obtain a feature-preserving pooling operation. Figure
@ref(fig:read-data) illustrates that the MOG algorithm coarsens an input
graph to produce a graph that preserves topological features of the
input graph.

\begin{figure}

{\centering \includegraphics{figures/mapper_and_pooling} 

}

\caption{Illustration of coarsening a graph via the MOG algorithm. (a): Visualization of a graph and of a scalar function defined on it, which are used as inputs to the MOG algorithm. The scalar function is visualized by color-mapping its scalar values. (b): The resulting MOG graph. Notice how the MOG construction preserves the overall shape of the original graph.}\label{fig:read-data}
\end{figure}

To recap, the MOG algorithm receives an input graph, and outputs a
coarsened graph along with connected components. The connected
components yield in turn a CC. The coarsened graph and the adjacency
relation between the \(2\)-cells of the associated CC are related, as
elaborated in Proposition @ref(prp:connection-to-tda). Figure
@ref(fig:adjadj) conveys visually Proposition
@ref(prp:connection-to-tda).

\phantomsection\label{connection-to-tda}
Let \((\mathcal{X}, g, \mathcal{U})\) be a triplet consisting of a graph
\(\mathcal{X}\), a scalar function \(g\colon\mathcal{X}^0\to [0,1]\)
defined on the vertex set of \(\mathcal{X}\), and a cover
\(\mathcal{U}\) of \([0,1]\). Let \(MOG(\mathcal{X}\), \(g\),
\(\mathcal{U})\) be the graph generated by the MOG algorithm upon
receiving the triplet \((\mathcal{X}, g, \mathcal{U})\) as input.
Furthermore, let \(\mathcal{X}_{g,\mathcal{U}}\) be the CC constructed
from the connected components that are generated by the MOG algorithm.
The adjacency matrix of the graph \(MOG(\mathcal{X}\), \(g\),
\(\mathcal{U})\) is equivalent to the adjacency matrix \(A_{2,2}\) of
the CC \(\mathcal{X}_{g,\mathcal{U}}\).

The proof follows by observing that the \(2\)-cells, which are augmented
to \(\mathcal{X}\) to generate the CC \(\mathcal{X}_{g,\mathcal{U}}\),
are 2-adjacent if and only if they intersect on a vertex. Moreover, two
cells intersect on a vertex if and only if there is an edge between them
in the graph \(MOG(\mathcal{X}, g, \mathcal{U})\) outputted by the MOG
algorithm.

\begin{figure}

{\centering \includegraphics{figures/adjmog_adjcc_scaled} 

}

\caption{Visual demonstration of how a MOG can be viewed as a CC. (a): Visualization of a CC $\mathcal{X}_{g,\mathcal{U}},$ which highlights the intersection between blue 2-cells. (b): The corresponding $MOG(\mathcal{X}, g, \mathcal{U})$ graph. There is a one-to-one correspondence between the blue 2-cells of $\mathcal{X}_{g,\mathcal{U}}$ on the left-hand side and the blue vertices of $MOG(\mathcal{X}, g, \mathcal{U})$ on the right-hand side. Further, two blue 2-cells of $\mathcal{X}_{g,\mathcal{U}}$ (left) intersect on an orange vertex if and only if there is an orange edge (right) that connects the corresponding two vertices of $MOG(\mathcal{X}, g, \mathcal{U})$.}\label{fig:adjadj}
\end{figure}

Given a CC \(X_{g,\mathcal{U}}\) obtained from a graph \(\mathcal{X}\)
via the MOG algorithm, the map
\(B_{0,2}^T\colon\mathcal{C}^0(X_{g,\mathcal{U}})\to \mathcal{C}^2(X_{g,\mathcal{U}})\)
can be used to induce a shape-preserving CC-pooling operation
(Definition @ref(def:pooling-exact-definition)). More specifically, a
signal \(\mathbf{H}_0\) supported on the vertices of \(\mathcal{X}\) can
be push-forwarded and pooled to a signal \(\mathbf{H}_2\) supported on
the 2-cells of \(X_{g,\mathcal{U}}\).

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abramenko2008buildings}
Abramenko, Peter, and Kenneth S. Brown. 2008. \emph{Buildings: Theory
and Applications}. Vol. 248. Springer Science \& Business Media.

\bibitem[\citeproctext]{ref-adams2017persistence}
Adams, Henry, Tegan Emerson, Michael Kirby, Rachel Neville, Chris
Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis
Motta, and Lori Ziegelmeier. 2017. {``Persistence Images: A Stable
Vector Representation of Persistent Homology.''} \emph{Jmlr} 18 (1):
218--52.

\bibitem[\citeproctext]{ref-adesso2023}
Adesso, Gerardo. 2023. {``Towards the Ultimate Brain: Exploring
Scientific Discovery with {C}hat{GPT} {AI}.''} \emph{AI Magazine} 44
(3): 328--42.
\url{https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12113}.

\bibitem[\citeproctext]{ref-amar2014constructing}
Amar, David, and Ron Shamir. 2014. {``Constructing Module Maps for
Integrated Analysis of Heterogeneous Biological Networks.''}
\emph{Nucleic Acids Research} 42 (7): 4208--19.

\bibitem[\citeproctext]{ref-anand2023hodge}
Anand, D. V., and Moo K. Chung. 2023. {``Hodge {L}aplacian of Brain
Networks.''} \emph{IEEE Transactions on Medical Imaging}.

\bibitem[\citeproctext]{ref-anwar2022synchronization}
Anwar, Md Sayeed, and Dibakar Ghosh. 2022. {``Stability of
Synchronization in Simplicial Complexes with Multiple Interaction
Layers.''} \emph{Phys. Rev. E} 106 (September): 034314.

\bibitem[\citeproctext]{ref-arvind2020weisfeiler}
Arvind, Vikraman, Frank Fuhlbrück, Johannes Köbler, and Oleg Verbitsky.
2020. {``On {W}eisfeiler-{L}eman Invariance: Subgraph Counts and Related
Graph Properties.''} \emph{Journal of Computer and System Sciences} 113:
42--59.

\bibitem[\citeproctext]{ref-arya2018exploiting}
Arya, Devanshu, and Marcel Worring. 2018. {``Exploiting Relational
Information in Social Networks Using Geometric Deep Learning on
Hypergraphs.''} In \emph{Proceedings of the 2018 {ACM} on
{I}nternational {C}onference on {M}ultimedia {R}etrieval}, 117--25.

\bibitem[\citeproctext]{ref-aschbacher1996combinatorial}
Aschbacher, Michael. 1996. {``Combinatorial Cell Complexes.''} In
\emph{Progress in Algebraic Combinatorics}, 1--80. Mathematical Society
of Japan.

\bibitem[\citeproctext]{ref-asif2021graph}
Asif, Nurul A., Yeahia Sarker, Ripon K. Chakrabortty, Michael J. Ryan,
Md. Hafiz Ahamed, Dip K. Saha, Faisal R. Badal, et al. 2021. {``Graph
Neural Network: A Comprehensive Review on Non-{E}uclidean Space.''}
\emph{IEEE Access} 9: 60588--606.
\url{https://doi.org/10.1109/ACCESS.2021.3071274}.

\bibitem[\citeproctext]{ref-attene2003shape}
Attene, Marco, Silvia Biasotti, and Michela Spagnuolo. 2003. {``Shape
Understanding by Contour-Driven Retiling.''} \emph{The Visual Computer}
19 (2): 127--38.

\bibitem[\citeproctext]{ref-atzmon2018point}
Atzmon, Matan, Haggai Maron, and Yaron Lipman. 2018. {``Point
Convolutional Neural Networks by Extension Operators.''} \emph{ACM
Transactions on Graphics} 37 (4).

\bibitem[\citeproctext]{ref-ausiello2017directed}
Ausiello, Giorgio, and Luigi Laura. 2017. {``Directed Hypergraphs:
Introduction and Fundamental Algorithms-a Survey.''} \emph{Theoretical
Computer Science} 658: 293--306.

\bibitem[\citeproctext]{ref-bahdanau2014}
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. {``Neural
Machine Translation by Jointly Learning to Align and Translate.''}
\emph{arXiv Preprint arXiv:1409.0473}.
\url{https://arxiv.org/abs/1409.0473}.

\bibitem[\citeproctext]{ref-bai2021multi}
Bai, Junjie, Biao Gong, Yining Zhao, Fuqiang Lei, Chenggang Yan, and Yue
Gao. 2021. {``Multi-Scale Representation Learning on Hypergraph for 3{D}
Shape Retrieval and Recognition.''} \emph{IEEE Transactions on Image
Processing} 30: 5327--38.

\bibitem[\citeproctext]{ref-bai2021hypergraph}
Bai, Song, Feihu Zhang, and Philip H. S. Torr. 2021. {``Hypergraph
Convolution and Hypergraph Attention.''} \emph{Pattern Recognition} 110:
107637.

\bibitem[\citeproctext]{ref-bailoni2022gasp}
Bailoni, Alberto, Constantin Pape, Nathan Hütsch, Steffen Wolf, Thorsten
Beier, Anna Kreshuk, and Fred A Hamprecht. 2022. {``{GASP}, a
Generalized Framework for Agglomerative Clustering of Signed Graphs and
Its Application to Instance Segmentation.''} In \emph{Cvpr}, 11645--55.

\bibitem[\citeproctext]{ref-bajaj1997contour}
Bajaj, Chandrajit L., Valerio Pascucci, and Daniel R. Schikore. 1997.
{``The Contour Spectrum.''} In \emph{{Proceedings of the 8th Conference
on Visualization '97}}, 167--ff. IEEE Computer Society Press.

\bibitem[\citeproctext]{ref-BampasidouGentimis2014}
Bampasidou, Maria, and Thanos Gentimis. 2014. {``Modeling Collaborations
with Persistent Homology.''} \emph{arXiv Preprint arXiv:1403.5346}
abs/1403.5346.

\bibitem[\citeproctext]{ref-bao2022impact}
Bao, Xiaoge, Qitong Hu, Peng Ji, Wei Lin, Jürgen Kurths, and Jan Nagler.
2022. {``Impact of Basic Network Motifs on the Collective Response to
Perturbations.''} \emph{Nature Communications} 13 (1): 5301.

\bibitem[\citeproctext]{ref-barabasi2013network}
Barabási, Albert-László. 2013. {``Network Science.''}
\emph{Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences} 371 (1987): 20120375.

\bibitem[\citeproctext]{ref-barbarossa2020topological}
Barbarossa, Sergio, and Stefania Sardellitti. 2020a. {``Topological
Signal Processing over Simplicial Complexes.''} \emph{IEEE Transactions
on Signal Processing} 68: 2992--3007.

\bibitem[\citeproctext]{ref-barbarossa2020topologicalmag}
---------. 2020b. {``Topological Signal Processing: Making Sense of Data
Building on Multiway Relations.''} \emph{IEEE Signal Processing
Magazine} 37 (6): 174--83.

\bibitem[\citeproctext]{ref-barbarossa2018learning}
Barbarossa, Sergio, Stefania Sardellitti, and Elena Ceci. 2018.
{``Learning from Signals Defined over Simplicial Complexes.''} In
\emph{2018 IEEE Data Science Workshop (DSW)}, 51--55. IEEE.

\bibitem[\citeproctext]{ref-barbarossa2016introduction}
Barbarossa, Sergio, and Mikhail Tsitsvero. 2016. {``An Introduction to
Hypergraph Signal Processing.''} In \emph{2016 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)},
6425--29. IEEE.

\bibitem[\citeproctext]{ref-basak2010combinatorial}
Basak, Tathagata. 2010. {``Combinatorial Cell Complexes and Poincar{é}
Duality.''} \emph{Geometriae Dedicata} 147 (1): 357--87.

\bibitem[\citeproctext]{ref-battaglia2018relational}
Battaglia, Peter W., Jessica B. Hamrick, Victor Bapst, Alvaro
Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea
Tacchetti, et al. 2018. {``Relational Inductive Biases, Deep Learning,
and Graph Networks.''} \emph{arXiv Preprint arXiv:1806.01261}.

\bibitem[\citeproctext]{ref-battaglia2016interaction}
Battaglia, Peter, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende,
and Koray kavukcuoglu. 2016. {``Interaction Networks for Learning about
Objects, Relations and Physics.''} In \emph{Proceedings of the 30th
International Conference on Neural Information Processing Systems},
4509--17. NIPS'16. Red Hook, NY, USA: Curran Associates Inc.

\bibitem[\citeproctext]{ref-battiloro2023topological}
Battiloro, Claudio, Stefania Sardellitti, Sergio Barbarossa, and Paolo
Di Lorenzo. 2023. {``Topological Signal Processing over Weighted
Simplicial Complexes.''} \emph{arXiv Preprint arXiv:2302.08561}.

\bibitem[\citeproctext]{ref-battiston2021physics}
Battiston, Federico, Enrico Amico, Alain Barrat, Ginestra Bianconi,
Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini,
et al. 2021. {``The Physics of Higher-Order Interactions in Complex
Systems.''} \emph{Nature Physics} 17 (10): 1093--98.

\bibitem[\citeproctext]{ref-battiston2020networks}
Battiston, Federico, Giulia Cencetti, Iacopo Iacopini, Vito Latora,
Maxime Lucas, Alice Patania, Jean-Gabriel Young, and Giovanni Petri.
2020. {``Networks Beyond Pairwise Interactions: Structure and
Dynamics.''} \emph{Physics Reports} 874: 1--92.

\bibitem[\citeproctext]{ref-beaini2021directional}
Beaini, Dominique, Saro Passaro, Vincent Létourneau, Will Hamilton,
Gabriele Corso, and Pietro Liò. 2021. {``Directional Graph Networks.''}
In \emph{International Conference on Machine Learning}.

\bibitem[\citeproctext]{ref-benson2018simplicial}
Benson, Austin R., Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and
Jon Kleinberg. 2018. {``Simplicial Closure and Higher-Order Link
Prediction.''} \emph{Proceedings of the National Academy of Sciences}
115 (48): E11221--30.

\bibitem[\citeproctext]{ref-benson2021higher}
Benson, Austin R., David F. Gleich, and Desmond J. Higham. 2021.
{``Higher-Order Network Analysis Takes Off, Fueled by Classical Ideas
and New Data.''} \emph{arXiv Preprint arXiv:2103.05031}.

\bibitem[\citeproctext]{ref-benson2016higher}
Benson, Austin R., David F. Gleich, and Jure Leskovec. 2016.
{``Higher-Order Organization of Complex Networks.''} \emph{Science} 353
(6295): 163--66.

\bibitem[\citeproctext]{ref-bentaieb2016topology}
BenTaieb, Aicha, and Ghassan Hamarneh. 2016. {``Topology Aware Fully
Convolutional Networks for Histology Gland Segmentation.''} In
\emph{Medical Image Computing and Computer-Assisted Intervention--MICCAI
2016: 19th International Conference, Athens, Greece, October 17-21,
2016, Proceedings, Part II 19}, 460--68. Springer.

\bibitem[\citeproctext]{ref-berry2020functional}
Berry, Eric, Yen-Chi Chen, Jessi Cisewski-Kehe, and Brittany Terese
Fasy. 2020. {``Functional Summaries of Persistence Diagrams.''} \emph{J.
Appl. Comput. Topol.} 4 (2): 211--62.

\bibitem[\citeproctext]{ref-bhattacharya2020step}
Bhattacharya, Uttaran, Trisha Mittal, Rohan Chandra, Tanmay Randhavane,
Aniket Bera, and Dinesh Manocha. 2020. {``{STEP}: Spatial Temporal Graph
Convolutional Networks for Emotion Perception from {G}aits.''}
\emph{Proceedings of the AAAI Conference on Artificial Intelligence} 34
(02): 1342--50. \url{https://doi.org/10.1609/aaai.v34i02.5490}.

\bibitem[\citeproctext]{ref-bianchi2020mincutpool}
Bianchi, Filippo Maria, Claudio Gallicchio, and Alessio Micheli. 2022.
{``Pyramidal Reservoir Graph Neural Network.''} \emph{Neurocomputing}
470: 389--404.

\bibitem[\citeproctext]{ref-bianchi2020spectral}
Bianchi, Filippo Maria, Daniele Grattarola, and Cesare Alippi. 2020.
{``Spectral Clustering with Graph Neural Networks for Graph Pooling.''}
In \emph{Icml}, 874--83. PMLR.

\bibitem[\citeproctext]{ref-bianconi2021higher}
Bianconi, Ginestra. 2021. \emph{Higher-Order Networks}. Cambridge
University Press.

\bibitem[\citeproctext]{ref-biasotti2008describing}
Biasotti, Silvia, Leila De Floriani, Bianca Falcidieno, Patrizio
Frosini, Daniela Giorgi, Claudia Landi, Laura Papaleo, and Michela
Spagnuolo. 2008. {``Describing Shapes by Geometrical-Topological
Properties of Real Functions.''} \emph{ACM Computing Surveys (CSUR)} 40
(4): 12.

\bibitem[\citeproctext]{ref-bick2021higher}
Bick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T
Schaub. 2021. {``What Are Higher-Order Networks?''} \emph{arXiv Preprint
arXiv:2104.11329}.

\bibitem[\citeproctext]{ref-birdal2021intrinsic}
Birdal, Tolga, Aaron Lou, Leonidas J Guibas, and Umut Simsekli. 2021.
{``Intrinsic Dimension, Persistent Homology and Generalization in Neural
Networks.''} \emph{Advances in Neural Information Processing Systems}.

\bibitem[\citeproctext]{ref-bodnar2021weisfeiler}
Bodnar, Cristian, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio,
Guido F Montufar, and Michael Bronstein. 2021. {``Weisfeiler and
{L}ehman Go Cellular: {CW} Networks.''} In \emph{Advances in Neural
Information Processing Systems}.

\bibitem[\citeproctext]{ref-boscaini2015learning}
Boscaini, Davide, Jonathan Masci, Simone Melzi, Michael M Bronstein,
Umberto Castellani, and Pierre Vandergheynst. 2015. {``Learning
Class-Specific Descriptors for Deformable Shapes Using Localized
Spectral Convolutional Networks.''} \emph{Computer Graphics Forum} 34
(5): 13--23.

\bibitem[\citeproctext]{ref-boscaini2016learning}
Boscaini, Davide, Jonathan Masci, Emanuele Rodolà, and Michael
Bronstein. 2016. {``Learning Shape Correspondence with Anisotropic
Convolutional Neural Networks.''} In \emph{Advances in Neural
Information Processing Systems}, 3189--97.

\bibitem[\citeproctext]{ref-bouritsas2020improving}
Bouritsas, Giorgos, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M.
Bronstein. 2023. {``Improving Graph Neural Network Expressivity via
Subgraph Isomorphism Counting.''} \emph{IEEE Transactions on Pattern
Analysis and Machine Intelligence} 45 (1): 657--68.

\bibitem[\citeproctext]{ref-boyell1963hybrid}
Boyell, Roger L., and Henry Ruston. 1963. {``Hybrid Techniques for
Real-Time Radar Simulation.''} In \emph{Proceedings of the November
12-14, 1963, Fall Joint Computer Conference}, 445--58. ACM.

\bibitem[\citeproctext]{ref-bronstein2021geometric}
Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković.
2021. {``Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and
Gauges.''} \emph{arXiv Preprint arXiv:2104.13478}.

\bibitem[\citeproctext]{ref-bronstein2017geometric}
Bronstein, Michael M., Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre
Vandergheynst. 2017. {``Geometric Deep Learning: Going Beyond
{E}uclidean Data.''} \emph{IEEE Signal Processing Magazine} 34 (4):
18--42.

\bibitem[\citeproctext]{ref-brown2006}
Brown, Ronald. 2006. \emph{Topology and Groupoids}. BookSurge
Publishing.

\bibitem[\citeproctext]{ref-bruna2013spectral}
Bruna, Joan, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014.
{``Spectral Networks and Locally Connected Networks on Graphs.''} In
\emph{Proceedings of the 2nd International Conference on Learning
Representations}, edited by Yoshua Bengio and Yann LeCun. {ICLR} 2014.
Banff, AB, Canada.

\bibitem[\citeproctext]{ref-bubenik2015statistical}
Bubenik, Peter. 2015. {``Statistical Topological Data Analysis Using
Persistence Landscapes.''} \emph{Jmlr} 16 (1): 77--102.

\bibitem[\citeproctext]{ref-bunch2020simplicial}
Bunch, Eric, Qian You, Glenn Fung, and Vikas Singh. 2020. {``Simplicial
2-Complex Convolutional Neural Nets.''} \emph{NeurIPS Workshop on
Topological Data Analysis and Beyond}.

\bibitem[\citeproctext]{ref-burnssimplicial}
Burns, Thomas F., and Tomoki Fukai. 2023. {``Simplicial {H}opfield
Networks.''} In \emph{The Eleventh International Conference on Learning
Representations}.

\bibitem[\citeproctext]{ref-calmon2022higher}
Calmon, Lucille, Michael T. Schaub, and Ginestra Bianconi. 2022.
{``Higher-Order Signal Processing with the {D}irac Operator.''} In
\emph{Asilomar Conference on Signals, Systems, and Computers}.

\bibitem[\citeproctext]{ref-cao2020comprehensive}
Cao, Wenming, Zhiyue Yan, Zhiquan He, and Zhihai He. 2020. {``A
Comprehensive Survey on Geometric Deep Learning.''} \emph{IEEE Access}
8: 35929--49.

\bibitem[\citeproctext]{ref-carlsson2006algebraic}
Carlsson, Erik, Gunnar Carlsson, and Vin De Silva. 2006. {``An Algebraic
Topological Method for Feature Identification.''} \emph{International
Journal of Computational Geometry \& Applications} 16 (04): 291--314.

\bibitem[\citeproctext]{ref-carlsson2009topology}
Carlsson, Gunnar. 2009. {``Topology and Data.''} \emph{Bulletin of the
American Mathematical Society} 46 (2): 255--308.

\bibitem[\citeproctext]{ref-carlsson2020topological}
Carlsson, Gunnar, and Rickard Brüel Gabrielsson. 2020. {``Topological
Approaches to Deep Learning.''} In \emph{Topological Data Analysis: The
Abel Symposium 2018}, 119--46. Springer; Springer.

\bibitem[\citeproctext]{ref-carlsson2008local}
Carlsson, Gunnar, Tigran Ishkhanov, Vin De Silva, and Afra Zomorodian.
2008. {``On the Local Behavior of Spaces of Natural Images.''}
\emph{Ijcv} 76 (1): 1--12.

\bibitem[\citeproctext]{ref-carlsson2008persistent}
Carlsson, Gunnar, and Facundo Mémoli. 2008. {``Persistent Clustering and
a Theorem of {J. K}leinberg.''} \emph{arXiv Preprint arXiv:0808.2241}.

\bibitem[\citeproctext]{ref-carlsson2009theory}
Carlsson, Gunnar, and Afra Zomorodian. 2009. {``The Theory of
Multidimensional Persistence.''} \emph{Discrete \& Computational
Geometry} 42 (1): 71--93.

\bibitem[\citeproctext]{ref-carlsson2005persistence}
Carlsson, Gunnar, Afra Zomorodian, Anne Collins, and Leonidas J Guibas.
2005. {``Persistence Barcodes for Shapes.''} \emph{International Journal
of Shape Modeling} 11 (02): 149--87.

\bibitem[\citeproctext]{ref-carr2004simplifying}
Carr, Hamish, Jack Snoeyink, and Michiel van de Panne. 2004.
{``Simplifying Flexible Isosurfaces Using Local Geometric Measures.''}
In \emph{{IEEE} Visualization}, 497--504. IEEE.

\bibitem[\citeproctext]{ref-carriere_perslay_2020}
Carriere, Mathieu, Frederic Chazal, Yuichi Ike, Theo Lacombe, Martin
Royer, and Yuhei Umeda. 2020. {``{PersLay}: A Neural Network Layer for
Persistence Diagrams and New Graph Topological Signatures.''} In
\emph{Proceedings of the {Twenty} {Third} {International} {Conference}
on {Artificial} {Intelligence} and {Statistics}}, 2786--96. PMLR.

\bibitem[\citeproctext]{ref-CarstensHoradam2013}
Carstens, C. J., and K. J. Horadam. 2013. {``Persistent Homology of
Collaboration Networks.''} \emph{Mathematical Problems in Engineering}
2013.

\bibitem[\citeproctext]{ref-chan2013topology}
Chan, Joseph Minhow, Gunnar Carlsson, and Raul Rabadan. 2013.
{``Topology of Viral Evolution.''} \emph{Proceedings of the National
Academy of Sciences} 110 (46): 18566--71.

\bibitem[\citeproctext]{ref-chang2022graphrr}
Chang, Yaomin, Lin Shu, Erxin Du, Chuan Chen, Ziyang Zhang, Zibin Zheng,
Yuzhao Huang, and Xingxing Xing. 2022. {``Graph{RR}: A Multiplex Graph
Based Reciprocal Friend Recommender System with Applications on Online
Gaming Service.''} \emph{Knowledge-Based Systems} 251: 109187.

\bibitem[\citeproctext]{ref-chaudhari2021attentive}
Chaudhari, Sneha, Varun Mithal, Gungor Polatkan, and Rohan Ramanath.
2021. {``An Attentive Survey of Attention Models.''} \emph{ACM
Transactions on Intelligent Systems and Technology (TIST)} 12 (5):
1--32.

\bibitem[\citeproctext]{ref-chen2015statistical}
Chen, Yen-Chi, Daren Wang, Alessandro Rinaldo, and Larry Wasserman.
2015. {``Statistical Analysis of Persistence Intensity Functions.''}
\emph{arXiv Preprint arXiv:1510.02502}.

\bibitem[\citeproctext]{ref-chen2019graph}
Chen, Yunpeng, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng, Jiashi
Feng, and Yannis Kalantidis. 2019. {``Graph-Based Global Reasoning
Networks.''} In \emph{Conference on Computer Vision and Pattern
Recognition}.

\bibitem[\citeproctext]{ref-chen2021bscnets}
Chen, Yuzhou, Yulia R. Gel, and H. Vincent Poor. 2022. {``{BS}c{N}ets:
Block Simplicial Complex Neural Networks.''} \emph{Proceedings of the
AAAI Conference on Artificial Intelligence} 36 (6): 6333--41.
\url{https://doi.org/10.1609/aaai.v36i6.20583}.

\bibitem[\citeproctext]{ref-choi2017gram}
Choi, Edward, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and
Jimeng Sun. 2017. {``{GRAM}: Graph-Based Attention Model for Healthcare
Representation Learning.''} In \emph{Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining}.

\bibitem[\citeproctext]{ref-cinque2022pooling}
Cinque, Domenico Mattia, Claudio Battiloro, and Paolo Di Lorenzo. 2022.
{``Pooling Strategies for Simplicial Convolutional Networks.''}
\emph{arXiv Preprint arXiv:2210.05490}.

\bibitem[\citeproctext]{ref-clough2019explicit}
Clough, James R., Ilkay Oksuz, Nicholas Byrne, Julia A. Schnabel, and
Andrew P. King. 2019. {``Explicit Topological Priors for Deep-Learning
Based Image Segmentation Using Persistent Homology.''} In
\emph{Information Processing in Medical Imaging: 26th International
Conference, IPMI 2019, Hong Kong, China, June 2--7, 2019, Proceedings
26}, 16--28. Springer.

\bibitem[\citeproctext]{ref-collins2004barcode}
Collins, Anne, Afra Zomorodian, Gunnar Carlsson, and Leonidas J Guibas.
2004. {``A Barcode Shape Descriptor for Curve Point Cloud Data.''}
\emph{Computers \& Graphics} 28 (6): 881--94.

\bibitem[\citeproctext]{ref-crane2013digital}
Crane, Keenan, Fernando De Goes, Mathieu Desbrun, and Peter Schröder.
2013. {``Digital Geometry Processing with Discrete Exterior Calculus.''}
In \emph{ACM SIGGRAPH 2013 Courses}, 1--126. Association for Computing
Machinery.

\bibitem[\citeproctext]{ref-curto2017can}
Curto, Carina. 2017. {``What Can Topology Tell Us about the Neural
Code?''} \emph{Bulletin of the American Mathematical Society} 54 (1):
63--78.

\bibitem[\citeproctext]{ref-DabaghianMemoliFrank2012}
Dabaghian, Y., F. Mémoli, L. Frank, and G. Carlsson. 2012. {``A
Topological Paradigm for Hippocampal Spatial Map Formation Using
Persistent Homology.''} \emph{PLoS Computational Biology} 8 (8):
e1002581.

\bibitem[\citeproctext]{ref-dai2021nrgnn}
Dai, Enyan, Charu Aggarwal, and Suhang Wang. 2021. {``{NRGNN}: Learning
a Label Noise Resistant Graph Neural Network on Sparsely and Noisily
Labeled Graphs.''} In \emph{Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery \& Data Mining}, 227--36.

\bibitem[\citeproctext]{ref-de2017multilayer}
De Domenico, Manlio. 2017. {``{Multilayer modeling and analysis of human
brain networks}.''} \emph{GigaScience} 6 (5).
\url{https://doi.org/10.1093/gigascience/gix004}.

\bibitem[\citeproctext]{ref-de2016physics}
De Domenico, Manlio, Clara Granell, Mason A Porter, and Alex Arenas.
2016. {``The Physics of Spreading Processes in Multilayer Networks.''}
\emph{Nature Physics} 12 (10): 901--6.

\bibitem[\citeproctext]{ref-deng2018ppfnet}
Deng, Haowen, Tolga Birdal, and Slobodan Ilic. 2018. {``{PPFNet}: Global
Context Aware Local Features for Robust 3{D} Point Matching.''} In
\emph{Cvpr}, 195--205.

\bibitem[\citeproctext]{ref-deng2020cola}
Deng, Songgaojun, Shusen Wang, Huzefa Rangwala, Lijing Wang, and Yue
Ning. 2020. {``Cola-{GNN}: Cross-Location Attention Based Graph Neural
Networks for Long-Term {ILI} Prediction.''} In \emph{Proceedings of the
29th ACM International Conference on Information \& Knowledge
Management}, 245--54.

\bibitem[\citeproctext]{ref-desbrun2008discrete}
Desbrun, Mathieu, Eva Kanso, and Yiying Tong. 2008. {``Discrete
Differential Forms for Computational Modeling.''} In \emph{Discrete
Differential Geometry}, 287--324. Springer.

\bibitem[\citeproctext]{ref-dewoskin2010applications}
DeWoskin, D., J. Climent, I. Cruz-White, M. Vazquez, C. Park, and J.
Arsuaga. 2010. {``Applications of Computational Homology to the Analysis
of Treatment Response in Breast Cancer Patients.''} \emph{Topology and
Its Applications} 157 (1): 157--64.

\bibitem[\citeproctext]{ref-dey2010persistent}
Dey, Tamal K., Kuiyu Li, Chuanjiang Luo, Pawas Ranjan, Issam Safa, and
Yusu Wang. 2010. {``Persistent Heat Signature for Pose-Oblivious
Matching of Incomplete Models.''} \emph{Computer Graphics Forum} 29 (5):
1545--54.

\bibitem[\citeproctext]{ref-dey2016multiscale}
Dey, Tamal K., Facundo Mémoli, and Yusu Wang. 2016. {``Multiscale
Mapper: Topological Summarization via Codomain Covers.''} In
\emph{Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on
Discrete Algorithms}, 997--1013. SIAM.

\bibitem[\citeproctext]{ref-dey22}
Dey, Tamal K., and Yusu Wang. 2022a. \emph{Computational Topology for
Data Analysis}. Cambridge University Press.

\bibitem[\citeproctext]{ref-DW22}
---------. 2022b. \emph{Computational Topology for Data Analysis}.
Cambridge University Press.

\bibitem[\citeproctext]{ref-dhillon2007weighted}
Dhillon, Inderjit S., Yuqiang Guan, and Brian Kulis. 2007. {``Weighted
Graph Cuts Without Eigenvectors a Multilevel Approach.''} \emph{Pami} 29
(11): 1944--57.

\bibitem[\citeproctext]{ref-dodziuk1976finite}
Dodziuk, Jozef. 1976. {``Finite-Difference Approach to the {H}odge
Theory of Harmonic Forms.''} \emph{American Journal of Mathematics} 98
(1): 79--104.

\bibitem[\citeproctext]{ref-dupuis2023generalization}
Dupuis, Benjamin, George Deligiannidis, and Umut Şimşekli. 2023.
{``Generalization Bounds with Data-Dependent Fractal Dimensions.''}
\emph{arXiv Preprint arXiv:2302.02766}.

\bibitem[\citeproctext]{ref-ebli2020simplicial}
Ebli, Stefania, Michaël Defferrard, and Gard Spreemann. 2020.
{``Simplicial Neural Networks.''} \emph{NeurIPS Workshop on Topological
Data Analysis and Beyond}.

\bibitem[\citeproctext]{ref-eckmann1944harmonische}
Eckmann, Beno. 1944. {``Harmonische Funktionen Und Randwertaufgaben in
Einem Komplex.''} \emph{Commentarii Mathematici Helvetici} 17 (1):
240--55.

\bibitem[\citeproctext]{ref-edelsbrunner2010computational}
Edelsbrunner, Herbert, and John Harer. 2010. \emph{Computational
Topology: An Introduction}. American Mathematical Soc.

\bibitem[\citeproctext]{ref-edelsbrunner2004time}
Edelsbrunner, Herbert, John Harer, Ajith Mascarenhas, and Valerio
Pascucci. 2004. {``Time-Varying {R}eeb Graphs for Continuous Space-Time
Data.''} In \emph{Proceedings of the Twentieth Annual Symposium on
Computational Geometry}, 366--72. ACM.

\bibitem[\citeproctext]{ref-efthymiou2021graph}
Efthymiou, Athanasios, Stevan Rudinac, Monika Kackovic, Marcel Worring,
and Nachoem Wijnberg. 2021. {``Graph Neural Networks for Knowledge
Enhanced Visual Representation of Paintings.''} \emph{arXiv Preprint
arXiv:2105.08190}.

\bibitem[\citeproctext]{ref-elhamdadi2021affectivetda}
Elhamdadi, Hamza, Shaun Canavan, and Paul Rosen. 2021.
{``Affective{TDA}: Using Topological Data Analysis to Improve Analysis
and Explainability in Affective Computing.''} \emph{IEEE Transactions on
Visualization and Computer Graphics} 28 (1): 769--79.

\bibitem[\citeproctext]{ref-feng2019hypergraph}
Feng, Yifan, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019.
{``Hypergraph Neural Networks.''} \emph{Proceedings of the AAAI
Conference on Artificial Intelligence} 33 (01): 3558--65.

\bibitem[\citeproctext]{ref-ferri2018simplicial}
Ferri, Massimo, Dott Mattia G. Bergomi, and Lorenzo Zu. 2018.
{``Simplicial Complexes from Graphs Towards Graph Persistence.''}
\emph{arXiv Preprint arXiv:1805.10716}.

\bibitem[\citeproctext]{ref-fey2019fast}
Fey, Matthias, and Jan Eric Lenssen. 2019. {``Fast Graph Representation
Learning with {P}y{T}orch {G}eometric.''} \emph{arXiv Preprint
arXiv:1903.02428}.

\bibitem[\citeproctext]{ref-bruel2019topology}
Gabrielsson, Rickard Brüel, Bradley J. Nelson, Anjan Dwaraknath, and
Primoz Skraba. 2020. {``A Topology Layer for Machine Learning.''} In
\emph{Proceedings of the Twenty Third International Conference on
Artificial Intelligence and Statistics}, edited by Silvia Chiappa and
Roberto Calandra, 108:1553--63. \#PMLR\#. PMLR.

\bibitem[\citeproctext]{ref-gallicchio2010graph}
Gallicchio, Claudio, and Alessio Micheli. 2010. {``Graph Echo State
Networks.''} In \emph{The 2010 International Joint Conference on Neural
Networks (IJCNN)}, 1--8. IEEE.

\bibitem[\citeproctext]{ref-gallier2016spectral}
Gallier, Jean. 2016. {``Spectral Theory of Unsigned and Signed Graphs.
Applications to Graph Clustering: A Survey.''} \emph{arXiv Preprint
arXiv:1601.04692}.

\bibitem[\citeproctext]{ref-gao2019graph}
Gao, Hongyang, and Shuiwang Ji. 2019. {``Graph {U}-{N}ets.''} In
\emph{Icml}, 2083--92. PMLR.

\bibitem[\citeproctext]{ref-gao2021topology}
Gao, Hongyang, Yi Liu, and Shuiwang Ji. 2021. {``Topology-Aware Graph
Pooling Networks.''} \emph{Pami} 43 (12): 4512--18.

\bibitem[\citeproctext]{ref-gao2022hgnn}
Gao, Yue, Yifan Feng, Shuyi Ji, and Rongrong Ji. 2022. {``{HGNN}+:
General Hypergraph Neural Networks.''} \emph{IEEE Transactions on
Pattern Analysis and Machine Intelligence}.

\bibitem[\citeproctext]{ref-gao2020hypergraph}
Gao, Yue, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing
Zou. 2020. {``Hypergraph Learning: Methods and Practices.''} \emph{IEEE
Transactions on Pattern Analysis and Machine Intelligence}.

\bibitem[\citeproctext]{ref-georgiev2022heat}
Georgiev, Dobrik, Marc Brockschmidt, and Miltiadis Allamanis. 2022.
{``{HEAT}: Hyperedge Attention Networks.''} \emph{Transactions on
Machine Learning Research}.

\bibitem[\citeproctext]{ref-ghrist2014elementary}
Ghrist, Robert W. 2014. \emph{Elementary Applied Topology}. Vol. 1.
Createspace Seattle.

\bibitem[\citeproctext]{ref-gilmer2017neural}
Gilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and
George E Dahl. 2017. {``Neural Message Passing for Quantum Chemistry.''}
In \emph{International Conference on Machine Learning}.

\bibitem[\citeproctext]{ref-girault2017towards}
Girault, Benjamin, Shrikanth S. Narayanan, and Antonio Ortega. 2017.
{``Towards a Definition of Local Stationarity for Graph Signals.''} In
\emph{IEEE International Conference on Acoustics, Speech and Signal
Processing}.

\bibitem[\citeproctext]{ref-giusti2016two}
Giusti, Chad, Robert Ghrist, and Danielle S. Bassett. 2016. {``Two's
Company, Three (or More) Is a Simplex: Algebraic-Topological Tools for
Understanding Higher-Order Structure in Neural Data.''} \emph{Journal of
Computational Neuroscience} 41: 1.

\bibitem[\citeproctext]{ref-giusti2022simplicial}
Giusti, Lorenzo, Claudio Battiloro, Paolo Di Lorenzo, Stefania
Sardellitti, and Sergio Barbarossa. 2022. {``Simplicial Attention
Networks.''} \emph{arXiv Preprint arXiv:2203.07485}.

\bibitem[\citeproctext]{ref-giusti2022cell}
Giusti, Lorenzo, Claudio Battiloro, Lucia Testa, Paolo Di Lorenzo,
Stefania Sardellitti, and Sergio Barbarossa. 2022. {``Cell Attention
Networks.''} \emph{arXiv Preprint arXiv:2209.08179}.

\bibitem[\citeproctext]{ref-de2016vector}
Goes, Fernando de, Mathieu Desbrun, and Yiying Tong. 2016. {``Vector
Field Processing on Triangle Meshes.''} In \emph{ACM SIGGRAPH 2016
Courses}, 1--49. Association for Computing Machinery.

\bibitem[\citeproctext]{ref-goh2022simplicial}
Goh, Christopher Wei Jin, Cristian Bodnar, and Pietro Lio. 2022.
{``Simplicial Attention Networks.''} In \emph{ICLR 2022 Workshop on
Geometrical and Topological Representation Learning}.

\bibitem[\citeproctext]{ref-gong2023generative}
Gong, Xue, Desmond J. Higham, and Konstantinos Zygalakis. 2023.
{``Generative Hypergraph Models and Spectral Embedding.''}
\emph{Scientific Reports} 13 (1): 540.

\bibitem[\citeproctext]{ref-goodfellow2016}
Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
2016. \emph{Deep Learning}. Vol. 1. MIT Press Cambridge.
\url{https://mitpress.mit.edu/9780262035613/deep-learning/}.

\bibitem[\citeproctext]{ref-goyal2018graph}
Goyal, Palash, and Emilio Ferrara. 2018. {``Graph Embedding Techniques,
Applications, and Performance: A Survey.''} \emph{Knowledge-Based
Systems} 151: 78--94.

\bibitem[\citeproctext]{ref-grady2010discrete}
Grady, Leo J., and Jonathan R. Polimeni. 2010. \emph{Discrete Calculus:
Applied Analysis on Graphs for Computational Science}. Vol. 3. Springer.

\bibitem[\citeproctext]{ref-grattarola2022understanding}
Grattarola, Daniele, Daniele Zambon, Filippo Maria Bianchi, and Cesare
Alippi. 2022. {``Understanding Pooling in Graph Neural Networks.''}
\emph{IEEE Transactions on Neural Networks and Learning Systems}.

\bibitem[\citeproctext]{ref-hagberg2008exploring}
Hagberg, Aric, Pieter Swart, and Daniel S Chult. 2008. {``Exploring
Network Structure, Dynamics, and Function Using {N}etwork{X}.''} Los
Alamos National Lab (LANL), Los Alamos, NM (United States).

\bibitem[\citeproctext]{ref-hajijcell}
Hajij, Mustafa, Kyle Istvan, and Ghada Zamzmi. 2020. {``Cell Complex
Neural Networks.''} In \emph{NeurIPS 2020 Workshop TDA and Beyond}.

\bibitem[\citeproctext]{ref-hajij2022high}
Hajij, Mustafa, Karthikeyan Natesan Ramamurthy, Aldo Saenz, and Ghada
Zamzmi. 2022. {``High Skip Networks: A Higher Order Generalization of
Skip Connections.''} In \emph{ICLR 2022 Workshop on Geometrical and
Topological Representation Learning}.

\bibitem[\citeproctext]{ref-hajij2020efficient}
Hajij, Mustafa, and Paul Rosen. 2020. {``An Efficient Data Retrieval
Parallel {R}eeb Graph Algorithm.''} \emph{Algorithms} 13 (10): 258.

\bibitem[\citeproctext]{ref-hajij2018mog}
Hajij, Mustafa, Bei Wang, and Paul Rosen. 2018. {``{MOG}: Mapper on
Graphs for Relationship Preserving Clustering.''} \emph{arXiv Preprint
arXiv:1804.11242}.

\bibitem[\citeproctext]{ref-hajij2018visual}
Hajij, Mustafa, Bei Wang, Carlos Scheidegger, and Paul Rosen. 2018.
{``Visual Detection of Structural Changes in Time-Varying Graphs Using
Persistent Homology.''} In \emph{2018 IEEE Pacific Visualization
Symposium (PacificVis)}, 125--34. IEEE.

\bibitem[\citeproctext]{ref-hajij2021simplicial}
Hajij, Mustafa, Ghada Zamzmi, Theodore Papamarkou, Vasileios Maroulas,
and Xuanting Cai. 2022. {``Simplicial Complex Representation
Learning.''} In \emph{Machine Learning on Graphs (MLoG) Workshop at ACM
International WSD Conference}.

\bibitem[\citeproctext]{ref-halaoui2010smart}
Halaoui, Hatem F. 2010. {``Smart Traffic Online System ({STOS}):
Presenting Road Networks with Time-Weighted Graphs.''} In \emph{2010
International Conference on Information Society}, 349--56. IEEE.

\bibitem[\citeproctext]{ref-hamilton2017representation}
Hamilton, William L., Rex Ying, and Jure Leskovec. 2017.
{``Representation Learning on Graphs: Methods and Applications.''}
\emph{IEEE Data Engineering Bulletin} 40 (3): 52--74.

\bibitem[\citeproctext]{ref-hanocka2019meshcnn}
Hanocka, Rana, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and
Daniel Cohen-Or. 2019. {``Mesh{CNN}: A Network with an Edge.''}
\emph{ACM Transactions on Graphics} 38 (4): 1--12.

\bibitem[\citeproctext]{ref-hansen2019toward}
Hansen, Jakob, and Robert Ghrist. 2019. {``Toward a Spectral Theory of
Cellular Sheaves.''} \emph{Journal of Applied and Computational
Topology} 3 (4): 315--58.

\bibitem[\citeproctext]{ref-hatcher2005algebraic}
Hatcher, Allen. 2005. \emph{Algebraic Topology}. Cambridge University
Press.

\bibitem[\citeproctext]{ref-hayhoe2022stable}
Hayhoe, Mikhail, Hans Riess, Victor M Preciado, and Alejandro Ribeiro.
2022. {``Stable and Transferable Hyper-Graph Neural Networks.''}
\emph{arXiv Preprint arXiv:2211.06513}.

\bibitem[\citeproctext]{ref-he2016deep}
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. {``Deep
Residual Learning for Image Recognition.''} In \emph{2016 {IEEE}
{C}onference on {C}omputer {V}ision and {P}attern {R}ecognition
({CVPR})}, 770--78. \url{https://doi.org/10.1109/CVPR.2016.90}.

\bibitem[\citeproctext]{ref-hensel2021survey}
Hensel, Felix, Michael Moor, and Bastian Rieck. 2021. {``A Survey of
Topological Machine Learning Methods.''} \emph{Frontiers in Artificial
Intelligence} 4: 681108.

\bibitem[\citeproctext]{ref-hofer2020graph}
Hofer, Christoph, Florian Graf, Bastian Rieck, Marc Niethammer, and
Roland Kwitt. 2020. {``Graph Filtration Learning.''} In
\emph{International Conference on Machine Learning}, 4314--23. PMLR.

\bibitem[\citeproctext]{ref-hofer2017deep}
Hofer, Christoph, Roland Kwitt, Marc Niethammer, and Andreas Uhl. 2017.
{``Deep Learning with Topological Signatures.''} In \emph{Neurips},
1634--44.

\bibitem[\citeproctext]{ref-HorakMaleticRajkovic2009}
Horak, Danijela, Slobodan Maletić, and Milan Rajković. 2009.
{``Persistent Homology of Complex Networks.''} \emph{Journal of
Statistical Mechanics: Theory and Experiment}, P03034.

\bibitem[\citeproctext]{ref-hu_topology-preserving_2019}
Hu, Xiaoling, Fuxin Li, Dimitris Samaras, and Chao Chen. 2019.
{``Topology-Preserving Deep Image Segmentation.''} In \emph{Advances in
{Neural} {Information} {Processing} {Systems}}. Vol. 32. Curran
Associates, Inc.

\bibitem[\citeproctext]{ref-hu_trigger_2022}
Hu, Xiaoling, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao
Chen. 2022. {``Trigger Hunting with a Topological Prior for {T}rojan
Detection.''} In \emph{International Conference on Learning
Representations}.

\bibitem[\citeproctext]{ref-huang2022multiway}
Huang, Jiahui, Tolga Birdal, Zan Gojcic, Leonidas J Guibas, and Shi-Min
Hu. 2022. {``Multiway Non-Rigid Point Cloud Registration via Learned
Functional Map Synchronization.''} \emph{IEEE Transactions on Pattern
Analysis and Machine Intelligence}.

\bibitem[\citeproctext]{ref-huang2019attpool}
Huang, Jingjia, Zhangheng Li, Nannan Li, Shan Liu, and Ge Li. 2019.
{``Att{P}ool: Towards Hierarchical Feature Representation in Graph
Convolutional Networks via Attention Mechanism.''} In \emph{Iccv},
6480--89.

\bibitem[\citeproctext]{ref-huang2021unignn}
Huang, Jing, and Jie Yang. 2021. {``Uni{GNN}: A Unified Framework for
Graph and Hypergraph Neural Networks.''} In \emph{Proceedings of the
Thirtieth International Joint Conference on Artificial Intelligence,
{IJCAI}}.

\bibitem[\citeproctext]{ref-itoh2022multi}
Itoh, Takeshi D., Takatomi Kubo, and Kazushi Ikeda. 2022. {``Multi-Level
Attention Pooling for Graph Neural Networks: Unifying Graph
Representations with Multiple Localities.''} \emph{Neural Networks} 145:
356--73.

\bibitem[\citeproctext]{ref-jha2022prediction}
Jha, Kanchan, Sriparna Saha, and Hiteshi Singh. 2022. {``Prediction of
Protein--Protein Interaction Using Graph Neural Networks.''}
\emph{Scientific Reports} 12 (1): 1--12.

\bibitem[\citeproctext]{ref-jiang2019dynamic}
Jiang, Jianwen, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. 2019.
{``Dynamic Hypergraph Neural Networks.''} In \emph{IJCAI}, 2635--41.

\bibitem[\citeproctext]{ref-jiang2022graph}
Jiang, Weiwei, and Jiayun Luo. 2022. {``Graph Neural Network for Traffic
Forecasting: A Survey.''} \emph{Expert Systems with Applications},
117921.

\bibitem[\citeproctext]{ref-joglwe2022}
Jogl, Fabian. 2022. {``Do We Need to Improve Message Passing? Improving
Graph Neural Networks with Graph Transformations.''} PhD thesis, Vienna
University of Technology.

\bibitem[\citeproctext]{ref-joslyn2021hypernetwork}
Joslyn, Cliff A, Sinan G Aksoy, Tiffany J Callahan, Lawrence E Hunter,
Brett Jefferson, Brenda Praggastis, Emilie Purvine, and Ignacio J
Tripodi. 2021. {``Hypernetwork Science: From Multidimensional Networks
to Computational Topology.''} In \emph{Unifying Themes in Complex
Systems x: Proceedings of the Tenth International Conference on Complex
Systems}, 377--92. Springer.

\bibitem[\citeproctext]{ref-keros2021dist2cycle}
Keros, Alexandros D., Vidit Nanda, and Kartic Subr. 2022. {``Dist2Cycle:
A Simplicial Neural Network for Homology Localization.''}
\emph{Proceedings of the AAAI Conference on Artificial Intelligence} 36
(7): 7133--42. \url{https://doi.org/10.1609/aaai.v36i7.20673}.

\bibitem[\citeproctext]{ref-kim2020hypergraph}
Kim, Eun-Sol, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and
Byoung-Tak Zhang. 2020. {``Hypergraph Attention Networks for Multimodal
Learning.''} In \emph{Cvpr}, 14581--90.

\bibitem[\citeproctext]{ref-kim2020pllay}
Kim, Kwangho, Jisu Kim, Manzil Zaheer, Joon Kim, Frédéric Chazal, and
Larry Wasserman. 2020. {``Pllay: Efficient Topological Layer Based on
Persistent Landscapes.''} \emph{Advances in Neural Information
Processing Systems} 33: 15965--77.

\bibitem[\citeproctext]{ref-KimLipmanChen2010}
Kim, Vladimir G, Yaron Lipman, Xiaobai Chen, and Thomas Funkhouser.
2010. {``Möbius Transformations for Global Intrinsic Symmetry
Analysis.''} \emph{Computer Graphics Forum} 29 (5): 1689--1700.

\bibitem[\citeproctext]{ref-kipf2016semi}
Kipf, Thomas N., and Max Welling. 2016. {``Semi-Supervised
Classification with Graph Convolutional Networks.''} \emph{arXiv
Preprint arXiv:1609.02907}.

\bibitem[\citeproctext]{ref-kivela2014multilayer}
Kivelä, Mikko, Alex Arenas, Marc Barthelemy, James P Gleeson, Yamir
Moreno, and Mason A Porter. 2014. {``Multilayer Networks.''}
\emph{Journal of Complex Networks} 2 (3): 203--71.

\bibitem[\citeproctext]{ref-klette2000cell}
Klette, Reinhard. 2000. {``Cell Complexes Through Time.''} In
\emph{Vision Geometry IX}, 4117:134--45. SPIE.

\bibitem[\citeproctext]{ref-knoke2019social}
Knoke, David, and Song Yang. 2019. \emph{Social Network Analysis}. SAGE
publications.

\bibitem[\citeproctext]{ref-kokkinos2012intrinsic}
Kokkinos, Iasonas, Michael M Bronstein, Roee Litman, and Alex M
Bronstein. 2012. {``Intrinsic Shape Context Descriptors for Deformable
Shapes.''} In \emph{Cvpr}, 159--66. IEEE.

\bibitem[\citeproctext]{ref-krizhevsky2017imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2017.
{``Imagenet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.

\bibitem[\citeproctext]{ref-krizhevsky2012}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012.
{``Image{N}et Classification with Deep Convolutional Neural Networks.''}
In \emph{Advances in Neural Information Processing Systems}.
\url{https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html}.

\bibitem[\citeproctext]{ref-kunegis2010spectral}
Kunegis, Jérôme, Stephan Schmidt, Andreas Lommatzsch, Jürgen Lerner,
Ernesto W De Luca, and Sahin Albayrak. 2010. {``Spectral Analysis of
Signed Graphs for Clustering, Prediction and Visualization.''} In
\emph{Proceedings of the 2010 SIAM International Conference on Data
Mining}, 559--70. SIAM.

\bibitem[\citeproctext]{ref-kusano2016persistence}
Kusano, Genki, Yasuaki Hiraoka, and Kenji Fukumizu. 2016. {``Persistence
Weighted {G}aussian Kernel for Topological Data Analysis.''} In
\emph{Icml}, 2004--13.

\bibitem[\citeproctext]{ref-kushnir2006fast}
Kushnir, Dan, Meirav Galun, and Achi Brandt. 2006. {``Fast Multiscale
Clustering and Manifold Identification.''} \emph{Pattern Recognition} 39
(10): 1876--91.

\bibitem[\citeproctext]{ref-kweon1994extracting}
Kweon, In So, and Takeo Kanade. 1994. {``Extracting Topographic Terrain
Features from Elevation Maps.''} \emph{CVGIP: Image Understanding} 59
(2): 171--82.

\bibitem[\citeproctext]{ref-la2022music}
La Gatta, Valerio, Vincenzo Moscato, Mirko Pennone, Marco Postiglione,
and Giancarlo Sperlı́. 2022. {``Music Recommendation via Hypergraph
Embedding.''} \emph{IEEE Transactions on Neural Networks and Learning
Systems}.

\bibitem[\citeproctext]{ref-lambiotte2019networks}
Lambiotte, Renaud, Martin Rosvall, and Ingo Scholtes. 2019. {``From
Networks to Optimal Higher-Order Models of Complex Systems.''}
\emph{Nature Physics} 15 (4): 313--20.

\bibitem[\citeproctext]{ref-lecun1998}
LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998.
{``Gradient-Based Learning Applied to Document Recognition.''}
\emph{Proceedings of the IEEE} 86 (11): 2278--2324.
\url{https://ieeexplore.ieee.org/document/726791}.

\bibitem[\citeproctext]{ref-LeeChungKang2011b}
Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Boong-Nyun Kim, and Dong Soo
Lee. 2011a. {``Computing the Shape of Brain Networks Using Graph
Filtration and Gromov-Hausdorff Metric.''} \emph{International
Conference on Medical Image Computing and Computer Assisted
Intervention}, 302--9.

\bibitem[\citeproctext]{ref-LeeChungKang2011}
Lee, Hyekyoung, Moo K. Chung, Hyejin Kang, Bung-Nyun Kim, and Dong Soo
Lee. 2011b. {``Discriminative Persistent Homology of Brain Networks.''}
\emph{{IEEE} International Symposium on Biomedical Imaging: From Nano to
Macro}, 841--44.

\bibitem[\citeproctext]{ref-LeeKangChung2012}
Lee, Hyekyoung, Hyejin Kang, Moo K. Chung, Bung-Nyun Kim, and Dong Soo
Lee. 2012a. {``Persistent Brain Network Homology from the Perspective of
Dendrogram.''} \emph{{IEEE} Transactions on Medical Imaging} 31 (12):
2267--77.

\bibitem[\citeproctext]{ref-LeeKangChung2012b}
---------. 2012b. {``Weighted Functional Brain Network Modeling via
Network Filtration.''} \emph{NIPS Workshop on Algebraic Topology and
Machine Learning}.

\bibitem[\citeproctext]{ref-boaz2019}
Lee, John Boaz, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee
Koh. 2019. {``Attention Models in Graphs: A Survey.''} \emph{ACM
Transactions on Knowledge Discovery from Data} 13 (6): 1--25.

\bibitem[\citeproctext]{ref-lee2018graph}
Lee, John Boaz, Ryan Rossi, and Xiangnan Kong. 2018. {``Graph
Classification Using Structural Attention.''} In \emph{Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery \&
Data Mining}.

\bibitem[\citeproctext]{ref-lee2019self}
Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. {``Self-Attention Graph
Pooling.''} In \emph{Icml}, 3734--43. PMLR.

\bibitem[\citeproctext]{ref-leventhal2023exploring}
Leventhal, Samuel, Attila Gyulassy, Mark Heimann, and Valerio Pascucci.
2023. {``Exploring Classification of Topological Priors with Machine
Learning for Feature Extraction.''} \emph{IEEE Transactions on
Visualization and Computer Graphics}.

\bibitem[\citeproctext]{ref-li2020graph}
Li, Juanhui, Yao Ma, Yiqi Wang, Charu Aggarwal, Chang-Dong Wang, and
Jiliang Tang. 2020. {``Graph Pooling with Representativeness.''} In
\emph{2020 IEEE International Conference on Data Mining (ICDM)},
302--11. IEEE.

\bibitem[\citeproctext]{ref-li2021learning}
Li, Zhifei, Hai Liu, Zhaoli Zhang, Tingting Liu, and Neal N Xiong. 2021.
{``Learning Knowledge Graph Embedding with Heterogeneous Relation
Attention Networks.''} \emph{IEEE Transactions on Neural Networks and
Learning Systems}.

\bibitem[\citeproctext]{ref-lian2011shape}
Lian, Z., A. Godil, B. Bustos, M Daoudi, J. Hermans, S. Kawamura, Y.
Kurita, G. Lavoua, P. Dp Suetens, et al. 2011. {``Shape Retrieval on
Non-Rigid 3{D} Watertight Meshes.''} In \emph{Eurographics Workshop on
3d Object Retrieval (3DOR)}. Citeseer.

\bibitem[\citeproctext]{ref-lim2020hodge}
Lim, Lek-Heng. 2020. {``{H}odge {L}aplacians on Graphs.''} \emph{{SIAM}
Review} 62 (3): 685--715.

\bibitem[\citeproctext]{ref-linka2020outbreak}
Linka, Kevin, Mathias Peirlinck, Francisco Sahli Costabal, and Ellen
Kuhl. 2020. {``Outbreak Dynamics of {COVID}-19 in {E}urope and the
Effect of Travel Restrictions.''} \emph{Computer Methods in Biomechanics
and Biomedical Engineering} 23 (11): 710--17.

\bibitem[\citeproctext]{ref-lo2016modeling}
Lo, Derek, and Briton Park. 2016. {``Modeling the Spread of the {Z}ika
Virus Using Topological Data Analysis.''} \emph{arXiv Preprint
arXiv:1612.03554}.

\bibitem[\citeproctext]{ref-loukas2019graph}
Loukas, Andreas. 2019. {``What Graph Neural Networks Cannot Learn: Depth
Vs Width.''} \emph{arXiv Preprint arXiv:1907.03199}.

\bibitem[\citeproctext]{ref-love2023topological}
Love, Ephy R., Benjamin Filippenko, Vasileios Maroulas, and Gunnar
Carlsson. 2023a. {``Topological Convolutional Layers for Deep
Learning.''} \emph{Jmlr} 24 (59): 1--35.

\bibitem[\citeproctext]{ref-love2020topological}
---------. 2023b. {``Topological Convolutional Layers for Deep
Learning.''} \emph{Jmlr} 24 (59): 1--35.

\bibitem[\citeproctext]{ref-lum2013extracting}
Lum, P. Y., G. Singh, A. Lehman, T. Ishkanov, Mikael Vejdemo-Johansson,
M. Alagappan, J. Carlsson, and G. Carlsson. 2013. {``Extracting Insights
from the Shape of Complex Data Using Topology.''} \emph{Scientific
Reports} 3: 1236.

\bibitem[\citeproctext]{ref-ma2019graph}
Ma, Yao, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019. {``Graph
Convolutional Networks with Eigenpooling.''} In \emph{Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery \& Data
Mining}, 723--31.

\bibitem[\citeproctext]{ref-majhi2022dynamics}
Majhi, Soumen, Matjaž Perc, and Dibakar Ghosh. 2022. {``Dynamics on
Higher-Order Networks: A Review.''} \emph{Journal of the Royal Society
Interface} 19 (188): 20220043.

\bibitem[\citeproctext]{ref-maletic2016persistent}
Maletić, Slobodan, Yi Zhao, and Milan Rajković. 2016. {``Persistent
Topological Features of Dynamical Systems.''} \emph{Chaos: An
Interdisciplinary Journal of Nonlinear Science} 26 (5): 053105.

\bibitem[\citeproctext]{ref-manriquez2021protection}
Manrı́quez, Ronald, Camilo Guerrero-Nancuante, and Carla Taramasco. 2021.
{``Protection Strategy Against an Epidemic Disease on Edge-Weighted
Graphs Applied to a {COVID}-19 Case.''} \emph{Biology} 10 (7): 667.

\bibitem[\citeproctext]{ref-maron2019provably}
Maron, Haggai, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. 2019.
{``Provably Powerful Graph Networks.''} \emph{arXiv Preprint
arXiv:1905.11136}.

\bibitem[\citeproctext]{ref-maron2017convolutional}
Maron, Haggai, Meirav Galun, Noam Aigerman, Miri Trope, Nadav Dym, Ersin
Yumer, Vladimir G Kim, and Yaron Lipman. 2017. {``Convolutional Neural
Networks on Surfaces via Seamless Toric Covers.''} \emph{ACM
Transactions on Graphics} 36 (4): 71--71.

\bibitem[\citeproctext]{ref-masci2015geodesic}
Masci, Jonathan, Davide Boscaini, Michael Bronstein, and Pierre
Vandergheynst. 2015. {``Geodesic Convolutional Neural Networks on
{R}iemannian Manifolds.''} In \emph{Conference on Computer Vision and
Pattern Recognition}.

\bibitem[\citeproctext]{ref-mejia2017spectral}
Mejia, Daniel, Oscar Ruiz-Salguero, and Carlos A. Cadavid. 2017.
{``Spectral-Based Mesh Segmentation.''} \emph{International Journal on
Interactive Design and Manufacturing} 11 (3): 503--14.

\bibitem[\citeproctext]{ref-mendel1991tutorial}
Mendel, Jerry M. 1991. {``Tutorial on Higher-Order Statistics (Spectra)
in Signal Processing and System Theory: Theoretical Results and Some
Applications.''} \emph{Proceedings of the IEEE} 79 (3): 278--305.

\bibitem[\citeproctext]{ref-menichetti2016control}
Menichetti, Giulia, Luca Dall'Asta, and Ginestra Bianconi. 2016.
{``Control of Multilayer Networks.''} \emph{Scientific Reports} 6 (1):
1--8.

\bibitem[\citeproctext]{ref-mesquita2020rethinking}
Mesquita, Diego, Amauri Souza, and Samuel Kaski. 2020. {``Rethinking
Pooling in Graph Neural Networks.''} \emph{Neurips} 33: 2220--31.

\bibitem[\citeproctext]{ref-milano2020primal}
Milano, Francesco, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza,
and Luca Carlone. 2020. {``Primal-Dual Mesh Convolutional Neural
Networks.''} \emph{Conference on Neural Information Processing Systems}
33: 952--63.

\bibitem[\citeproctext]{ref-mitchell2022topological}
Mitchell, Edward C., Brittany Story, David Boothe, Piotr J. Franaszczuk,
and Vasileios Maroulas. 2022. {``A Topological Deep Learning Framework
for Neural Spike Decoding.''} \emph{arXiv Preprint arXiv:2212.05037}.

\bibitem[\citeproctext]{ref-mitchell1980need}
Mitchell, Tom M. 1980. {``The Need for Biases in Learning
Generalizations.''}

\bibitem[\citeproctext]{ref-mnih2014recurrent}
Mnih, Volodymyr, Nicolas Heess, Alex Graves, et al. 2014. {``Recurrent
Models of Visual Attention.''} In \emph{Advances in Neural Information
Processing Systems}. Vol. 27.

\bibitem[\citeproctext]{ref-monti2017geometric}
Monti, Federico, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan
Svoboda, and Michael M. Bronstein. 2017. {``Geometric Deep Learning on
Graphs and Manifolds Using Mixture Model {CNNs}.''} In \emph{Cvpr},
5115--24.

\bibitem[\citeproctext]{ref-moor2020topological}
Moor, Michael, Max Horn, Bastian Rieck, and Karsten Borgwardt. 2020.
{``Topological Autoencoders.''} In \emph{International Conference on
Machine Learning}, 7045--54. PMLR.

\bibitem[\citeproctext]{ref-morris2019weisfeiler}
Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton,
Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. {``Weisfeiler
and {L}eman Go Neural: Higher-Order Graph Neural Networks.''} In
\emph{Proceedings of the AAAI Conference on Artificial Intelligence}.

\bibitem[\citeproctext]{ref-munkres1974}
Munkres, James R. 1974. \emph{Topology; a First Course}. Prentice-Hall.

\bibitem[\citeproctext]{ref-murgas2022hypergraph}
Murgas, Kevin A., Emil Saucan, and Romeil Sandhu. 2022. {``Hypergraph
Geometry Reflects Higher-Order Dynamics in Protein Interaction
Networks.''} \emph{Scientific Reports} 12 (1): 20879.

\bibitem[\citeproctext]{ref-neyshabur2019role}
Neyshabur, Behnam, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and
Nathan Srebro. 2019. {``The Role of over-Parametrization in
Generalization of Neural Networks.''} In \emph{International Conference
on Learning Representations}.

\bibitem[\citeproctext]{ref-nicolau2011topology}
Nicolau, Monica, Arnold J. Levine, and Gunnar Carlsson. 2011.
{``Topology Based Data Analysis Identifies a Subgroup of Breast Cancers
with a Unique Mutational Profile and Excellent Survival.''}
\emph{Proceedings of the National Academy of Sciences} 108 (17):
7265--70.

\bibitem[\citeproctext]{ref-oballe2021bayesian}
Oballe, Christopher, Alan Cherne, Dave Boothe, Scott Kerick, Piotr J
Franaszczuk, and Vasileios Maroulas. 2021. {``Bayesian Topological
Signal Processing.''} \emph{Discrete \& Continuous Dynamical Systems-S}.

\bibitem[\citeproctext]{ref-ortega2018graph}
Ortega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and
Pierre Vandergheynst. 2018. {``Graph Signal Processing: Overview,
Challenges, and Applications.''} \emph{Proceedings of the IEEE} 106 (5):
808--28.

\bibitem[\citeproctext]{ref-pang2021graph}
Pang, Yunsheng, Yunxiang Zhao, and Dongsheng Li. 2021. {``Graph Pooling
via Coarsened Graph Infomax.''} In \emph{Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in
Information Retrieval}, 2177--81.

\bibitem[\citeproctext]{ref-mathilde2023}
Papillon, Mathilde, Sophia Sanborn, Mustafa Hajij, and Nina Miolane.
2023. {``Architectures of Topological Deep Learning: A Survey on
Topological Neural Networks.''} \emph{arXiv Preprint arXiv:2304.10031}.

\bibitem[\citeproctext]{ref-paszke2017automatic}
Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. {``Automatic Differentiation in {P}y{T}orch.''} In
\emph{NIPS Workshop}.

\bibitem[\citeproctext]{ref-scikit-learn}
Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.
Grisel, M. Blondel, et al. 2011. {``Scikit-Learn: Machine Learning in
{P}ython.''} \emph{Jmlr} 12: 2825--30.

\bibitem[\citeproctext]{ref-perea2015sw1pers}
Perea, Jose A., Anastasia Deckard, Steve B. Haase, and John Harer. 2015.
{``{SW}1{P}er{S}: Sliding Windows and 1-Persistence Scoring; Discovering
Periodicity in Gene Expression Time Series Data.''} \emph{BMC
Bioinformatics} 16 (1): 257.

\bibitem[\citeproctext]{ref-PetriScolamieroDonato2013}
Petri, Giovanni, Martina Scolamiero, Irene Donato, and Francesco
Vaccarino. 2013a. {``Networks and Cycles: A Persistent Homology Approach
to Complex Networks.''} \emph{Proceedings European Conference on Complex
Systems 2012, Springer Proceedings in Complexity}, 93--99.

\bibitem[\citeproctext]{ref-PetriScolamieroDonato2013b}
---------. 2013b. {``Topological Strata of Weighted Complex Networks.''}
\emph{PLoS ONE} 8 (6).

\bibitem[\citeproctext]{ref-piaggesi2022effective}
Piaggesi, Simone, André Panisson, and Giovanni Petri. 2022. {``Effective
Higher-Order Link Prediction and Reconstruction from Simplicial Complex
Embeddings.''} In \emph{Learning on Graphs Conference}, 55--51. PMLR.

\bibitem[\citeproctext]{ref-plizzari2021spatial}
Plizzari, Chiara, Marco Cannici, and Matteo Matteucci. 2021. {``Spatial
Temporal Transformer Network for Skeleton-Based Action Recognition.''}
In \emph{Icpr}, 694--701. Springer.

\bibitem[\citeproctext]{ref-pun2018persistent}
Pun, Chi Seng, Kelin Xia, and Si Xian Lee. 2018.
{``Persistent-Homology-Based Machine Learning and Its Applications--a
Survey.''} \emph{arXiv Preprint arXiv:1811.00252}.

\bibitem[\citeproctext]{ref-qi2017pointnet}
Qi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017.
{``Point{N}et: Deep Learning on Point Sets for 3{D} Classification and
Segmentation.''} In \emph{Cvpr}, 652--60.

\bibitem[\citeproctext]{ref-reddy2023clustering}
Reddy, Thummaluru Siddartha, Sundeep Prabhakar Chepuri, and Pierre
Borgnat. 2023. {``Clustering with Simplicial Complexes.''} \emph{arXiv
Preprint arXiv:2303.07646}.

\bibitem[\citeproctext]{ref-rempe2020caspr}
Rempe, Davis, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar,
and Leonidas J Guibas. 2020. {``{CASPR}: Learning Canonical
Spatiotemporal Point Cloud Representations.''} \emph{Neurips} 33:
13688--701.

\bibitem[\citeproctext]{ref-rieck2019persistent}
Rieck, Bastian, Christian Bock, and Karsten Borgwardt. 2019. {``A
Persistent {W}eisfeiler-{L}ehman Procedure for Graph Classification.''}
In \emph{International Conference on Machine Learning}, 5448--58. PMLR.

\bibitem[\citeproctext]{ref-rieck2015persistent}
Rieck, Bastian, and Heike Leitte. 2015. {``Persistent Homology for the
Evaluation of Dimensionality Reduction Schemes.''} \emph{Computer
Graphics Forum} 34 (3): 431--40.

\bibitem[\citeproctext]{ref-rieck_uncovering_2020}
Rieck, Bastian, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy
Wolf, Nick Turk-Browne, and Smita Krishnaswamy. 2020. {``Uncovering the
Topology of Time-Varying {fMRI} Data Using Cubical Persistence.''} In
\emph{Advances in {Neural} {Information} {Processing} {Systems}
({NeurIPS})}, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, 33:6900--6912. Curran Associates, Inc.

\bibitem[\citeproctext]{ref-robinson2014topological}
Robinson, Michael. 2014. \emph{Topological Signal Processing}. Vol. 81.
Springer.

\bibitem[\citeproctext]{ref-roddenberry2019hodgenet}
Roddenberry, T Mitchell, and Santiago Segarra. 2019. {``Hodge{N}et:
Graph Neural Networks for Edge Data.''} In \emph{2019 53rd Asilomar
Conference on Signals, Systems, and Computers}, 220--24. IEEE.

\bibitem[\citeproctext]{ref-roddenberry2021principled}
Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. 2021.
{``Principled Simplicial Neural Networks for Trajectory Prediction.''}
In \emph{International Conference on Machine Learning}.

\bibitem[\citeproctext]{ref-roddenberry2021signal}
Roddenberry, T. Mitchell, Michael T. Schaub, and Mustafa Hajij. 2022.
{``Signal Processing on Cell Complexes.''} In \emph{IEEE International
Conference on Acoustics, Speech and Signal Processing}.

\bibitem[\citeproctext]{ref-rombach2022}
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and
Björn Ommer. 2022. {``High-Resolution Image Synthesis with Latent
Diffusion Models.''} In \emph{Computer Vision and Pattern Recognition}.
\url{https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600k0674/1H1iFsO7Zuw}.

\bibitem[\citeproctext]{ref-ronneberger2015u}
Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. {``U-{N}et:
Convolutional Networks for Biomedical Image Segmentation.''} In
\emph{International Conference on Medical Image Computing and
Computer-Assisted Intervention}, 234--41. Springer.

\bibitem[\citeproctext]{ref-rosen2017using}
Rosen, Paul, Bei Wang, Anil Seth, Betsy Mills, Adam Ginsburg, Julia
Kamenetzky, Jeff Kern, and Chris R Johnson. 2017. {``Using Contour Trees
in the Analysis and Visualization of Radio Astronomy Data Cubes.''}
\emph{arXiv Preprint arXiv:1704.04561}, 1--7.

\bibitem[\citeproctext]{ref-sanchez2020learning}
Sanchez-Gonzalez, Alvaro, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure
Leskovec, and Peter Battaglia. 2020. {``Learning to Simulate Complex
Physics with Graph Networks.''} In \emph{International Conference on
Machine Learning}.

\bibitem[\citeproctext]{ref-santoro2017simple}
Santoro, Adam, David Raposo, David G Barrett, Mateusz Malinowski, Razvan
Pascanu, Peter Battaglia, and Timothy Lillicrap. 2017. {``A Simple
Neural Network Module for Relational Reasoning.''} In \emph{Advances in
Neural Information Processing Systems}.

\bibitem[\citeproctext]{ref-santoro2023higher}
Santoro, Andrea, Federico Battiston, Giovanni Petri, and Enrico Amico.
2023. {``Higher-Order Organization of Multivariate Time Series.''}
\emph{Nature Physics}, 1--9.

\bibitem[\citeproctext]{ref-sardellitti2022topological}
Sardellitti, Stefania, and Sergio Barbarossa. 2022. {``Topological
Signal Representation and Processing over Cell Complexes.''} \emph{arXiv
Preprint arXiv:2201.08993}.

\bibitem[\citeproctext]{ref-sardellitti2021topological}
Sardellitti, Stefania, Sergio Barbarossa, and Lucia Testa. 2021.
{``Topological Signal Processing over Cell Complexes.''}
\emph{Proceeding IEEE Asilomar Conference. Signals, Systems and
Computers}.

\bibitem[\citeproctext]{ref-savoy2022combinatorial}
Savoy, Maxime. 2021. {``Combinatorial Cell Complexes: Duality,
Reconstruction and Causal Cobordisms.''} PhD thesis, École Polytechnique
Fédérale de Lausanne.

\bibitem[\citeproctext]{ref-scarselli2008graph}
Scarselli, Franco, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
Gabriele Monfardini. 2008. {``The Graph Neural Network Model.''}
\emph{IEEE Transactions on Neural Networks} 20 (1): 61--80.

\bibitem[\citeproctext]{ref-schaub2020random}
Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali
Jadbabaie. 2020. {``Random Walks on Simplicial Complexes and the
Normalized {H}odge 1-{L}aplacian.''} \emph{SIAM Review} 62 (2): 353--91.

\bibitem[\citeproctext]{ref-schaub2022signal}
Schaub, Michael T., Jean-Baptiste Seby, Florian Frantzen, T. Mitchell
Roddenberry, Yu Zhu, and Santiago Segarra. 2022. {``Signal Processing on
Simplicial Complexes.''} In \emph{Higher-Order Systems}, 301--28.
Springer.

\bibitem[\citeproctext]{ref-schaub2018denoising}
Schaub, Michael T., and Santiago Segarra. 2018. {``Flow Smoothing and
Denoising: Graph Signal Processing in the Edge-Space.''} In \emph{2018
IEEE Global Conference on Signal and Information Processing
(GlobalSIP)}, 735--39.

\bibitem[\citeproctext]{ref-schaub2021signal}
Schaub, Michael T., Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry,
and Santiago Segarra. 2021. {``Signal Processing on Higher-Order
Networks: Livin'on the Edge... And Beyond.''} \emph{Signal Processing}
187: 108149.

\bibitem[\citeproctext]{ref-schiff2020characterizing}
Schiff, Yair, Vijil Chenthamarakshan, Karthikeyan Natesan Ramamurthy,
and Payel Das. 2020. {``Characterizing the Latent Space of Molecular
Deep Generative Models with Persistent Homology Metrics.''} \emph{arXiv
Preprint arXiv:2010.08548}.

\bibitem[\citeproctext]{ref-schlichtkrull2018modeling}
Schlichtkrull, Michael, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg,
Ivan Titov, and Max Welling. 2018. {``Modeling Relational Data with
Graph Convolutional Networks.''} In \emph{European Semantic Web
Conference}.

\bibitem[\citeproctext]{ref-sharp2022diffusionnet}
Sharp, Nicholas, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov.
2022. {``Diffusion{N}et: Discretization Agnostic Learning on
Surfaces.''} \emph{Tog} 41 (3): 1--16.

\bibitem[\citeproctext]{ref-shi2018hypergraph}
Shi, Heyuan, Yubo Zhang, Zizhao Zhang, Nan Ma, Xibin Zhao, Yue Gao, and
Jiaguang Sun. 2018. {``Hypergraph-Induced Convolutional Networks for
Visual Classification.''} \emph{IEEE Transactions on Neural Networks and
Learning Systems} 30 (10): 2963--72.

\bibitem[\citeproctext]{ref-shlomi2020graph}
Shlomi, Jonathan, Peter Battaglia, and Jean-Roch Vlimant. 2020. {``Graph
Neural Networks in Particle Physics.''} \emph{Machine Learning: Science
and Technology} 2 (2): 021001.

\bibitem[\citeproctext]{ref-shuman2016vertex}
Shuman, David I., Benjamin Ricaud, and Pierre Vandergheynst. 2016.
{``Vertex-Frequency Analysis on Graphs.''} \emph{Applied and
Computational Harmonic Analysis} 40 (2): 260--91.

\bibitem[\citeproctext]{ref-simonyan2014}
Simonyan, Karen, and Andrew Zisserman. 2014. {``Very Deep Convolutional
Networks for Large-Scale Image Recognition.''} \emph{arXiv Preprint
arXiv:1409.1556}. \url{https://arxiv.org/abs/1409.1556}.

\bibitem[\citeproctext]{ref-singh2007topological}
Singh, Gurjeet, Facundo Mémoli, Gunnar E Carlsson, et al. 2007.
{``Topological Methods for the Analysis of High Dimensional Data Sets
and 3d Object Recognition.''} \emph{PBG@ Eurographics} 2: 091--100.

\bibitem[\citeproctext]{ref-skardal2021higher}
Skardal, Per Sebastian, Lluı́s Arola-Fernández, Dane Taylor, and Alex
Arenas. 2021. {``Higher-Order Interactions Improve Optimal Collective
Dynamics on Networks.''} \emph{arXiv Preprint arXiv:2108.08190}.

\bibitem[\citeproctext]{ref-smirnov2021hodgenet}
Smirnov, Dmitriy, and Justin Solomon. 2021. {``Hodge{N}et: Learning
Spectral Geometry on Triangle Meshes.''} \emph{ACM Transactions on
Graphics} 40 (4): 1--11.

\bibitem[\citeproctext]{ref-su2021hierarchical}
Su, Zidong, Zehui Hu, and Yangding Li. 2021. {``Hierarchical Graph
Representation Learning with Local Capsule Pooling.''} In \emph{ACM
International Conference on Multimedia in Asia}.

\bibitem[\citeproctext]{ref-sun2009rankclus}
Sun, Yizhou, Jiawei Han, Peixiang Zhao, Zhijun Yin, Hong Cheng, and
Tianyi Wu. 2009. {``Rank{C}lus: Integrating Clustering with Ranking for
Heterogeneous Information Network Analysis.''} In \emph{Proceedings of
the 12th International Conference on Extending Database Technology:
Advances in Database Technology}.

\bibitem[\citeproctext]{ref-sutskever2014}
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. {``Sequence to
Sequence Learning with Neural Networks.''} In \emph{Advances in Neural
Information Processing Systems}. \url{https://arxiv.org/abs/1409.3215}.

\bibitem[\citeproctext]{ref-tabassum2018social}
Tabassum, Shazia, Fabiola SF Pereira, Sofia Fernandes, and João Gama.
2018. {``Social Network Analysis: An Overview.''} \emph{Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery} 8 (5):
e1256.

\bibitem[\citeproctext]{ref-taylor2015topological}
Taylor, Dane, Florian Klimm, Heather A Harrington, Miroslav Kramár,
Konstantin Mischaikow, Mason A Porter, and Peter J Mucha. 2015.
{``Topological Data Analysis of Contagion Maps for Examining Spreading
Processes on Networks.''} \emph{Nature Communications} 6: 7723.

\bibitem[\citeproctext]{ref-topaz2015topological}
Topaz, Chad M, Lori Ziegelmeier, and Tom Halverson. 2015. {``Topological
Data Analysis of Biological Aggregation Models.''} \emph{PloS One} 10
(5): e0126383.

\bibitem[\citeproctext]{ref-torres2021and}
Torres, Leo, Ann S Blevins, Danielle Bassett, and Tina Eliassi-Rad.
2021. {``The Why, How, and When of Representations for Complex
Systems.''} \emph{SIAM Review} 63 (3): 435--85.

\bibitem[\citeproctext]{ref-trask2022enforcing}
Trask, Nathaniel, Andy Huang, and Xiaozhe Hu. 2022. {``Enforcing Exact
Physics in Scientific Machine Learning: A Data-Driven Exterior Calculus
on Graphs.''} \emph{Journal of Computational Physics}.

\bibitem[\citeproctext]{ref-turaev2016quantum}
Turaev, Vladimir G. 2016. \emph{Quantum Invariants of Knots and
3-Manifolds}. Vol. 18. Walter de Gruyter GmbH \& Co KG.

\bibitem[\citeproctext]{ref-umeda2017time}
Umeda, Yuhei. 2017. {``Time Series Classification via Topological Data
Analysis.''} \emph{Information and Media Technologies} 12: 228--39.

\bibitem[\citeproctext]{ref-vaswani2017}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.
{``Attention Is All You Need.''}
\url{https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[\citeproctext]{ref-velivckovic2022message}
Veličković, Petar. 2022. {``Message Passing All the Way Up.''}
\emph{ICLR 2022 Workshop on Geometrical and Topological Representation
Learning}.

\bibitem[\citeproctext]{ref-velickovic2017graph}
Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Lio, and Yoshua Bengio. 2018. {``Graph Attention Networks.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-wachs2006poset}
Wachs, Michelle L. 2006. {``Poset Topology: Tools and Applications.''}
\emph{arXiv Preprint Math/0602226}.

\bibitem[\citeproctext]{ref-waibel_capturing_2022}
Waibel, Dominik J. E., Scott Atwell, Matthias Meier, Carsten Marr, and
Bastian Rieck. 2022. {``Capturing {Shape} {Information}
with~{Multi}-{Scale} {Topological} {Loss} {Terms} for~{3D}
{Reconstruction}.''} In \emph{Medical {Image} {Computing} and {Computer}
{Assisted} {Intervention} -- {MICCAI} 2022}, edited by Linwei Wang, Qi
Dou, P. Thomas Fletcher, Stefanie Speidel, and Shuo Li, 150--59. Lecture
{Notes} in {Computer} {Science}. Cham: Springer Nature Switzerland.
\url{https://doi.org/10.1007/978-3-031-16440-8_15}.

\bibitem[\citeproctext]{ref-wang2023survey}
Wang, Cheng, Nan Ma, Zhixuan Wu, Jin Zhang, and Yongqiang Yao. 2023.
{``Survey of Hypergraph Neural Networks and Its Application to Action
Recognition.''} In \emph{Artificial Intelligence: Second CAAI
International Conference, CICAI 2022, Beijing, China, August 27--28,
2022, Revised Selected Papers, Part II}, 387--98. Springer.

\bibitem[\citeproctext]{ref-wangtopogan}
Wang, Fan, Huidong Liu, Dimitris Samaras, and Chao Chen. 2020.
{``Topogan: A Topology-Aware Generative Adversarial Network.''} In
\emph{Eccv}, 118--36. Springer.

\bibitem[\citeproctext]{ref-wang2012active}
Wang, Yunhai, Shmulik Asafi, Oliver Van Kaick, Hao Zhang, Daniel
Cohen-Or, and Baoquan Chen. 2012. {``Active Co-Analysis of a Set of
Shapes.''} \emph{ACM Transactions on Graphics} 31 (6): 1--10.

\bibitem[\citeproctext]{ref-ELuYao2012}
Weinan, E., Luan Jianfeng, and Yao Yuan. 2013. {``The Landscape of
Complex Networks: Critical Nodes and a Hierarchical Decomposition.''}
\emph{Methods and Applications of Analysis} 20: 383--404.

\bibitem[\citeproctext]{ref-weisfeiler1968reduction}
Weisfeiler, Boris, and Andrei Leman. 1968. {``The Reduction of a Graph
to Canonical Form and the Algebra Which Appears Therein.''} \emph{NTI,
Series} 2 (9): 12--16.

\bibitem[\citeproctext]{ref-point-cloud-utils}
Williams, Francis. 2022. {``Point {C}loud {U}tils.''}

\bibitem[\citeproctext]{ref-wu2022hypergraph}
Wu, Hanrui, and Michael K. Ng. 2022. {``Hypergraph Convolution on
Nodes-Hyperedges Network for Semi-Supervised Node Classification.''}
\emph{ACM Transactions on Knowledge Discovery from Data (TKDD)} 16 (4):
1--19.

\bibitem[\citeproctext]{ref-wu20153d}
Wu, Zhirong, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang,
Xiaoou Tang, and Jianxiong Xiao. 2015. {``3{D} {S}hape{N}ets: A Deep
Representation for Volumetric Shapes.''} In \emph{Cvpr}, 1912--20.

\bibitem[\citeproctext]{ref-wu2020comprehensive}
Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. {``A Comprehensive Survey on Graph Neural
Networks.''} \emph{IEEE Transactions on Neural Networks and Learning
Systems} 32 (1): 4--24.

\bibitem[\citeproctext]{ref-xie2015}
Xie, Yihui. 2015. \emph{Dynamic Documents with {R} and Knitr}. 2nd ed.
Boca Raton, Florida: Chapman; Hall/CRC. \url{http://yihui.name/knitr/}.

\bibitem[\citeproctext]{ref-xie2024}
---------. 2024. \emph{Bookdown: Authoring Books and Technical Documents
with {R} {M}arkdown}. \url{https://github.com/rstudio/bookdown}.

\bibitem[\citeproctext]{ref-xu2022groupnet}
Xu, Chenxin, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. 2022.
{``Group{N}et: Multiscale Hypergraph Neural Networks for Trajectory
Prediction with Relational Reasoning.''} In \emph{Conference on Computer
Vision and Pattern Recognition}.

\bibitem[\citeproctext]{ref-xu2018powerful}
Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. {``How
Powerful Are Graph Neural Networks?''} \emph{arXiv Preprint
arXiv:1810.00826}.

\bibitem[\citeproctext]{ref-yan2018spatial}
Yan, Sijie, Yuanjun Xiong, and Dahua Lin. 2018. {``Spatial Temporal
Graph Convolutional Networks for Skeleton-Based Action Recognition.''}
In \emph{Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[\citeproctext]{ref-yang2023convolutional}
Yang, Maosheng, and Elvin Isufi. 2023. {``Convolutional Learning on
Simplicial Complexes.''} \emph{arXiv Preprint arXiv:2301.11163}.

\bibitem[\citeproctext]{ref-yang2021finite}
Yang, Maosheng, Elvin Isufi, Michael T Schaub, and Geert Leus. 2021.
{``Finite Impulse Response Filters for Simplicial Complexes.''} In
\emph{2021 29th European Signal Processing Conference (EUSIPCO)},
2005--9. IEEE.

\bibitem[\citeproctext]{ref-yin2022dynamic}
Yin, Nan, Fuli Feng, Zhigang Luo, Xiang Zhang, Wenjie Wang, Xiao Luo,
Chong Chen, and Xian-Sheng Hua. 2022. {``Dynamic Hypergraph
Convolutional Network.''} In \emph{2022 IEEE 38th International
Conference on Data Engineering (ICDE)}, 1621--34. IEEE.

\bibitem[\citeproctext]{ref-ying2018hierarchical}
Ying, Zhitao, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton,
and Jure Leskovec. 2018. {``Hierarchical Graph Representation Learning
with Differentiable Pooling.''} \emph{Neurips} 31.

\bibitem[\citeproctext]{ref-zaheer2017deep}
Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ
R Salakhutdinov, and Alexander J Smola. 2017. {``Deep Sets.''} In
\emph{Advances in Neural Information Processing Systems}.

\bibitem[\citeproctext]{ref-zeng_topological_2021}
Zeng, Sebastian, Florian Graf, Christoph Hofer, and Roland Kwitt. 2021.
{``Topological Attention for Time Series Forecasting.''} In
\emph{Advances in {Neural} {Information} {Processing} {Systems}},
34:24871--82. Curran Associates, Inc.

\bibitem[\citeproctext]{ref-zhang2018kernel}
Zhang, Qi, Qizhao Jin, Jianlong Chang, Shiming Xiang, and Chunhong Pan.
2018. {``Kernel-Weighted Graph Convolutional Network: A Deep Learning
Approach for Traffic Forecasting.''} In \emph{2018 24th International
Conference on Pattern Recognition (ICPR)}, 1018--23. IEEE.

\bibitem[\citeproctext]{ref-zhang2020deep}
Zhang, Shi-Xue, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa
Wang, and Xu-Cheng Yin. 2020. {``Deep Relational Reasoning Graph Network
for Arbitrary Shape Text Detection.''} In \emph{Cvpr}, 9699--9708.

\bibitem[\citeproctext]{ref-zhang2020multiplex}
Zhang, Weifeng, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. {``Multiplex
Graph Neural Networks for Multi-Behavior Recommendation.''} In
\emph{Proceedings of the 29th ACM International Conference on
Information \& Knowledge Management}, 2313--16.

\bibitem[\citeproctext]{ref-zhang2021hierarchical}
Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Zhao Li, Chengwei
Yao, Huifen Dai, Zhi Yu, and Can Wang. 2021. {``Hierarchical Multi-View
Graph Pooling with Structure Learning.''} \emph{IEEE Transactions on
Knowledge and Data Engineering} 35 (1): 545--59.

\bibitem[\citeproctext]{ref-zhang2019hierarchical}
Zhang, Zhen, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi
Yu, and Can Wang. 2019. {``Hierarchical Graph Pooling with Structure
Learning.''} \emph{arXiv Preprint arXiv:1911.05954}.

\bibitem[\citeproctext]{ref-zhao2022from}
Zhao, Lingxiao, Wei Jin, Leman Akoglu, and Neil Shah. 2022. {``From
Stars to Subgraphs: Uplifting Any {GNN} with Local Structure
Awareness.''} In \emph{International Conference on Learning
Representations}.

\bibitem[\citeproctext]{ref-zhao20223dpointcaps}
Zhao, Yongheng, Guangchi Fang, Yulan Guo, Leonidas Guibas, Federico
Tombari, and Tolga Birdal. 2022. {``3{DP}oint{C}aps++: Learning 3{D}
Representations with Capsule Networks.''} \emph{Ijcv} 130 (9): 2321--36.

\bibitem[\citeproctext]{ref-zhou2020graph}
Zhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan
Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. {``Graph Neural
Networks: A Review of Methods and Applications.''} \emph{AI Open} 1:
57--81.

\bibitem[\citeproctext]{ref-zhu2018social}
Zhu, Jianming, Junlei Zhu, Smita Ghosh, Weili Wu, and Jing Yuan. 2018.
{``Social Influence Maximization in Hypergraph in Social Networks.''}
\emph{IEEE Transactions on Network Science and Engineering} 6 (4):
801--11.

\end{CSLReferences}

\end{document}
