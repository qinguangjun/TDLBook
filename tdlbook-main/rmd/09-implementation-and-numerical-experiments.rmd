---
output:
  pdf_document: default
  html_document: default
---
# (PART\*) 第四部分：应用，文献和结论 {-}

# 实现与实验{#implementation-and-numerical-results}

所提出的 CCNNs 可用于为不同的学习任务构建不同的神经网络架构。在本节中，我们将通过评估 CCNNs 在形状分析和图形学习任务中的预测性能来证明其通用性和有效性。在几何处理实验中，我们将 CCNNs 与最先进的方法进行了比较，这些方法针对特定任务进行了高度设计和训练。此外，我们还对几何数据处理中常用的各种数据模式（即点云和三维网格）进行了实验。我们还对图形数据进行了实验。在实验中，我们调整了三个主要部分：CCNN 架构的选择、学习率和数据增强中的副本数量。我们为每项学习任务选择的 CCNN 架构进行了论证。我们在 PyTorch 中实现了我们的流程，并在使用 Microsoft Windows 后端的单 GPU NVIDIA GeForce RTX 3060 Ti 上运行了实验。

## 软件：TopoNetX, TopoEmbedX, and TopoModelX{#software-toponetx-topoembedx-and-topomodelx}

我们所有的软件开发和实验分析都是使用 Python 进行的。我们也开发了三个 Python 软件包，并用它们来运行我们的实验: 

+ [TopoNetX](https://github.com/pyt-team/TopoNetX)，支持构建多种拓扑结构，包括胞腔复形、单纯复形和组合复形类。这些类分别提供了计算胞腔复形、单纯复形和组合复形上的边界算子（ boundary operators）、霍奇拉普拉斯（Hodge Laplacians）和高阶邻接算子的方法；

+ [TopoEmbedX](https://github.com/pyt-team/TopoEmbedX) ，支持对胞腔复形、单纯复形和组合复形的高阶关系进行表征学习（representation learning ）；

+ [TopoModelX](https://github.com/pyt-team/TopoModelX)， 支持计算定义在这些拓扑域上的深度学习模型。

除了所实现的软件包，我们还使用了 PyTorch [@paszke2017automatic] 来训练本节中报告的神经网络。此外，我们还利用 Scikit-learn [@scikit-learn] 计算了 1-Hodge Laplacians的特征向量。点云的法向量是使用点云工具包（Point Cloud Utils package）[@point-cloud-utils]计算的。最后，在软件包的开发和计算过程中，我们使用了 NetworkX [@hagberg2008exploring] 和 HyperNetX [@joslyn2021hypernetwork]。

## 数据集{#datasets}

在CCNNs的评估实验中，我们使用了四种数据集：Human Body, COSEG, SHREC11, 以及一个用于图分类的标准数据集 [@bianchi2020mincutpool]。数据集的摘要如下：

**人体分割数据集，Human Body segmentation dataset**. 文献[@atzmon2018point] 中提出的原始人体分割数据集包含相对较大的网格，网格顶点最多可达 12000 个。该数据集中提供的分割标签是按面(per-face)设置的，分割准确率被定义为正确分类的面数与整个数据集中面总数的比率。在本文项工作中，我们使用了 [@hanocka2019meshcnn] 提供的原始人体数据集的简化版本，其中网格的节点数少于 1,000 个，分割标签被重新映射到边上。我们在第 \@ref(mesh-segmentation)节的形状分析（例如，网格分割）任务中使用了这个简化版的人体数据集。

**COSEG分割数据集，COSEG segmentation dataset**. 原始 COSEG 数据集[@wang2012active]包含 11 组带有基准真值（ground-true）分割的形状。在本文工作中，我们使用了原始 COSEG 数据集的一个子集，其中包含相对较大的外星人、花瓶和椅子集。这三个数据集分别包含 200、300 和 400 个形状。我们使用这个自定义的 COSEG 数据集子集来完成第 \@ref(mesh-segmentation)节中的形状分析（例如，网格分割）任务。

**SHREC11分类数据集，SHREC11 classification dataset**. SHREC 2011 [@lian2011shape], 简写为SHREC11, 是一个大型数据集，其中包含来自 30 个类别的 600 个非刚性变形形状（水密三角形网格，watertight triangel meshes^[水密（watertight）网格通常描述由一个封闭曲面组成的网格，水密网格不包含孔洞并且内部定义明确]），每个类别包含相同数量的物体。这些类别包括手、灯、女人、男人、火烈鸟和兔子。该数据集分为训练集和测试集，分别包含 480 个和 120 个形状。我们使用 SHREC11 数据集来完成 \@ref(mesh-and-point-cloud-classification) 和  \@ref(pooling-with-mapper-on-graphs-and-data-classification)章节中的形状分析任务。

>译者注：水密（watertight）网格通常描述由一个封闭曲面组成的网格，水密网格不包含孔洞并且内部定义明确

**图分类基准数据集**. 该数据集包含属于三个不同类别的图 [@bianchi2020mincutpool]。对于每个图，每个顶点（0-cochain）上的特征向量都是大小为 5 的独热向量，它存储了图上顶点的相对位置。该数据集分为简易版和困难版，简易版包含高度连接的图，而困难版包含稀疏的图。我们在第 \@ref(graph-classification)节的图分类任务中使用了这个数据集。

## 形状分析：网格分割与分类{#shape-analysis-mesh-segmentation-and-classification}

用于形状分析实验（网格分割和分类）的 CC 结构是由网格的三角剖分简单诱导出来的。具体来说，0-、1-和 2-cells分别是网格的顶点、边和面。用于 CCNN 的矩阵是 $B_{0,1},~B_{0,2}$、它们的转置矩阵以及（共）邻接矩阵 $A_{1,1}$、$coA_{1,1}$ 和 $coA_{2,1}$。

CCNN 将共链向量作为输入特征。对于形状分析任务，我们考虑直接从底层网格的顶点坐标建立特征的共链，我们也注意到还有其他选择（例如 文献[@mejia2017spectral] 中基于光谱的共链）也可以包括在内。我们的形状分析任务有三个输入共链：顶点共链、边共链和面共链，每个顶点共链有两个输入特征：与顶点相关的位置和法向量（normal vector）。与[@hanocka2019meshcnn]类似，每个边共链由五个特征组成：每个面的边长、二面角（dihedral angle）、两个内角和两个边长比。最后，每个输入面共链由三个输入特征组成：面面积、面法线和三个面角度。

### 网格分割{#mesh-segmentation}

对于人体数据集 [@maron2017convolutional]，我们构建了一个 CCNN，它能产生一个边类。架构的张量图如图\@ref(fig:mesh-net)(a)所示。对于 COSEG 数据集[@wang2012active]，我们构建了一个 CCNN，结合我们提出的定义在顶点、边和面上的特征向量来学习最终的面类。如图 \@ref(fig:mesh-net)(b)所示，该架构使用关联矩阵以及（共）邻接矩阵来构建信号流。具体来说，张量图显示了三个非平方注意块（non-squared attention-blocks）和三个平方注意块（squared attention blocks）。如图 \@ref(fig:mesh-net)(b)所示，模型的深度选择为2。

```{r mesh-net, echo=FALSE, fig.align="center", fig.cap="实验中所用的CCNNs的张量图 (a): 用于网格分割任务的 CCNNs. 尤其, $\\mbox{CCNN}_{HB}$ 和 $\\mbox{CCNN}_{COSEG}$分别是 是人体数据集[@atzmon2018point]和COSEG dataset [@wang2012active]使用的架构。 (b): SHREC11 数据集 [@lian2011shape]使用网格分类CCNN. (c): 数据集[@bianchi2020mincutpool]使用图分类CCNN。 (d): 在 SHREC11 数据集上网格/点云分类 CCNNs与 MOG 算法结合使用."}
knitr::include_graphics('figures/experiment.png', dpi=NA)
```

请注意，为 COSEG 和人体数据集选择的架构具有相同数量和类型的构建模块；比较图 \@ref(fig:mesh-net)(a) 和 (b). 实验采用 85%-15% 的随机训练-测试比例。对于这两种架构，输出张量都采用了软最大激活（softmax activation），使用 0.0001 的学习率和标准交叉熵损失对所有分割模型进行了 600 次训练，在人体和形状 COSEG 数据集上都采用这些设置。

我们使用Human Body [@maron2017convolutional] 和 Shape COSEG（花瓶、椅子和外星人）[@wang2012active] 数据集对所提出的 CCNN 进行了网格分割测试。对于这些数据集中的每个网格，所使用的 CC 结构是由网格的三角剖分建立的，尽管 CC 结构的其他变化也会产生类似的结果。此外，还为 $0\leq k \leq 2$  构建了三个 $k$-cochains，并在 CCNN 训练中使用。如表 \@ref(tab:shape-xp)所示，在四个数据集中的两个数据集上，CCNN 优于为网格分析量身定制的三个神经网络（HodgeNet [@smirnov2021hodgenet], PD-MeshNet [@milano2020primal] 和 MeshCCN [@hanocka2019meshcnn]），并且是所有四个数据集上最好的两个神经网络之一。


```{r shape-xp, echo=FALSE}
methods <- c('HodgeNet', 'PD-MeshNet', 'MeshCNN', 'CCNN')
hb <- c('85.03', '85.61', '85.39', '87.30')
cosegv <- c('90.30', '95.36', '92.36', '93.40')
cosegc <- c('95.68', '97.23', '92.99', '98.30')
cosega <- c('96.03', '98.18', '96.26', '93.70')
domains <- data.frame(methods, hb, cosegv, cosegc, cosega)
colnames(domains) <- c('方法', 'Human Body', 'COSEG vase', 'COSEG chair', 'COSEG alien')
knitr::kable(domains, align=c('l', 'c', 'c', 'c', 'c'), booktabs=TRUE, caption="与形状分析相关的测试集，即Human Body和 COSEG（花瓶、椅子、外星人）数据集的预测准确率。本文报告的结果基于  $\\mbox{CCNN}_{COSEG}$ 和$\\mbox{CCNN}_{HB}$  架构。其中，第一列报告的是 $\\mbox{CCNN}_{HB}$ 的结果，而第二、第三和第四列报告的是 $\\mbox{CCNN}_{COSEG}$的结果。")
```

**$\mbox{CCNN}_{COSEG}$ 和  $\mbox{CCNN}_{HB}$的架构**. 在 $\mbox{CCNN}_{COSEG}$中, 如图\@ref(fig:mesh-net)(a)所示, 我们选择定义 \@ref(def:general-pooling-hoan)中给出的 带池化操作的CCNN 的架构，该架构会推送来自顶点、边和面的信号，并将它们的信息汇总到最终的面预测类中。我们同样选择 $\mbox{CCNN}_{HB}$，只不过预测的信号是边类。之所以这样选择，是因为Human Body 数据集[@atzmon2018point]对边缘的分割信息进行了编码。

### 网格和点云分类{#mesh-and-point-cloud-classification}

我们使用 SHREC11 数据集[@lian2011shape]对我们的方法针对网格分类进行了评估，该数据集采用和第\@ref(mesh-segmentation)节的分割实验中的相同共链和 CC 结构。用于网格分类任务的 CCNN 架构（表示为$\mbox{CCNN}_{SHREC}$ ）如图 \@ref(fig:mesh-net)(b)所示。$\mbox{CCNN}_{SHREC}$的最后一层在图\@ref(fig:mesh-net)(b)中以灰色节点表示，这是一个简单的池化操作，将CC的所有嵌入映射到相同的欧是空间后求和。$\mbox{CCNN}_{SHREC}$ 使用 $tanh$ 和 $identity$ 激活函数进行了 40 次训练，学习率为 0.005，损失为标准交叉熵。我们使用各向异向扩展和随机旋转来增强数据。每个网格增强 30 次，以质量顶点中心为中心，并重新缩放以适应单位立方体。


带有$identity$ 激活和$\tanh$激活的$\mbox{CCNN}_{SHREC}$的预测准确率分别达到96.67%和99.17%。表\@ref(tab:shrec)显示，CCNNs 的表现优于两个专门用于网格分析的神经网络（HodgeNet 和 MeshCCN），在网格和点云分类方面是仅次于 PD-MeshNet 的最佳模型。值得一提的是，网格分类 CCNN 所需的训练历时（40 个epochs）大大少于网格分割 CCNN（600 个epochs）。

> 译者注：Identity激活 是一种输入和输出相等的激活函数，比较适合底层函数是线性的，比如线性回归问题，当存在非线性问题是，作用就大打折扣

```{r shrec, echo=FALSE}
methods <- c('HodgeNet', 'PD-MeshNet', 'MeshCNN', 'CCNN')
mesh <- c('99.10', '99.70', '98.60', '99.17')
pointcloud <- c('94.70', '99.10', '91.00', '95.20')
domains <- data.frame(methods, mesh, pointcloud)
colnames(domains) <- c('方法', 'Mesh', 'Point cloud')
knitr::kable(domains, align=c('l', 'c', 'c'), booktabs=TRUE, caption="在数据集SHREC11 test 上的预测精度。 左列和右列分别报告了网格和点云分类结果。网格分类的 CCNN 为 $\\mbox{CCNN}_{SHREC}$ ，点云分类的 CCNN 为 $\\mbox{CCNN}_{MOG2}$ 。")
```

**$\mbox{CCNN}_{SHREC}$的架构**. $mbox{CCNN}_{SHREC}$有两层，用作定义\@ref(def:general-pooling-hoan)中描述的池化CCNN，类似于$\mbox{CCNN}_{COSEG}$和$\mbox{CCNN}_{HB}$。主要区别在于，$\mbox{CCNN}_{SHREC}$ 的最后一层（图 \@ref(fig:mesh-net)(b)中的灰色点表示）是一个全局池化函数，它将底层 CC 的所有维度（0、1、2）的嵌入映射到相同的欧氏空间后求和。

### 图分类{#graph-classification}

对于图分类任务，使用文献[@bianchi2020mincutpool]中提供的图分类测试基准数据，该数据集由三类不同标签的数据组成。对于每个图， 每个顶点(0-cochain)的特征向量都是尺寸为5的独热向量，存储了图中顶点的相对位置。 为了构建 CC 结构，我们使用了输入图的 2-clique 复形。 然后，我们继续构建用于图分类的 CCNN，表示为 $\mbox{CCNN}_{Graph}$ 表示，如图 \@ref(fig:mesh-net)(c)所示。 构造 $\mbox{CCNN}_{Graph}$的矩阵式 $B_{0,1},~B_{1,2},~B_{0,2}$，转置矩阵和（共）邻矩阵是$A_{0,1},A_{1,1},~coA_{2,1}$。 $\mbox{CCNN}_{Graph}$ 的共链构建方式如下对于数据集中的每个图，我们将 0-Cochain 设置为数据集提供的大小为 5的独热向量。这个独热向量存储了顶点在图中的相对位置。我们还通过考虑连接到每个胞腔顶点的独热向量的坐标最大值，在图的 2-clique复形上构建 1-cochain和2-cochain。$\mbox{CCNN}_{graoh}$ 的输入包括作为数据集一部分提供的 0-cochain，以及构建的 1-cochain和 2-cochain。图 \@ref(fig:mesh-net)(c) 中的灰色节点表示简单的均值池化操作。我们以 0.005 的学习率训练这个网络，并且没有增强数据。

表 \@ref(tab:wrap-tab)报告了*简易*和*困难*两个版本数据集的结果^[这些数据集的难度受图簇紧凑程度的控制；“简易 ”数据中的簇具有更多的簇间连接，而 “困难 ”数据中的簇更加孤立[@bianchi2020mincutpool]]，并将它们与六个最先进的GNN进行了比较。如表\@ref(tab:wrap-tab)所示，在 “困难 ”数据集上，CCNN优于所有六种GNN，在 “简易 ”数据集上，CCNN优于五种GNN。我们提出的 CCNN 在困难数据集上的表现优于 MinCutPool，而在建议数据集上的表现与 MinCutPool 相当。

```{r wrap-tab, echo=FALSE}
dsets <- c('Easy', 'Hard')
graclus <- c('97.81', '69.08')
ndp <- c('97.93', '72.67')
diffpool <- c('98.64', '69.98')
topk <- c('82.47', '42.80')
sagpool <- c('84.23', '37.71')
mincutpool <- c('99.02', '73.80')
ccnn <- c('98.90', '75.59')
domains <- data.frame(
  dsets, graclus, ndp, diffpool, topk, sagpool, mincutpool, ccnn
)
colnames(domains) <- c(
  '数据集',
  'Graclus',
  'NDP',
  'DiffPool',
  'Top-K',
  'SAGPool',
  'MinCutPool',
  'CCNN'
)
knitr::kable(domains, align=c('l', 'c', 'c', 'c', 'c'), booktabs=TRUE, caption="在[@bianchi2020mincutpool]测试集上与图分类相关的预测准确率。所有结果均使用 $\\mbox{CCNN}_{Graph}$架构报告。")
```

**$\mbox{CCNN}_{Graph}$的架构**. 对于图\@ref(fig:mesh-net)(c)中给出的$\mbox{CCNN}_{Graph}$， 我们选择了定义\@ref(def:general-pooling-hoan)中给出的 CCNN 池架构，该架构推送来自顶点、边和面的信号，并在做出最终预测之前将它们的信息聚合到高阶胞腔。对于[@bianchi2020mincutpool]中的数据集，我们用两种架构进行了实验：第一种架构与图\@ref(fig:mesh-net)(b)中所示的$\mbox{CCNN}_{SHREC}$相同，第二种架构是图\@ref(fig:mesh-net)(c)中所示的$\mbox{CCNN}_{Graph}$。我们报告的是 $\mbox{CCNN}_{Graph}$的实验结果，因为它的性能更优。需要注意的是，当这种神经网络在底层单纯复形上执行时，通常不会考虑关联矩阵 $B_{0,1}$ and $B_{1,3}$，但是配备了这些额外关联矩阵的 CC 结构有助于提高 $\mbox{CCNN}_{Graph}$ 的泛化性能。

## 在图上用映射器（mapper）算法池化和数据分类{#pooling-with-mapper-on-graphs-and-data-classification}

我们通过实验来评估第 \@ref(mapper-and-the-cc-pooling-operation)节中讨论的MOG池化操作策略的有效性。回顾一下，MOG 算法需要两部分输入：CC $\mathcal{X}$ 的 1-skeleton和 $\mathcal{X}$ 顶点上的标量函数。我们选择的输入标量函数是平均测地距离（Average geodesic distance，AGD）[@KimLipmanChen2010]，它与反射和旋转无关，因此适用于形状检测。对于图上的两个实体 $u$ 和 $v$，使用 Dijkstra 的最短路径算法计算出 $u$ 和 $v$ 之间的测地距离，用 $d(v,u)$ 表示。AGD 的计算公式如下：
\begin{equation}
AGD(v)=\frac{1}{|V|}\sum_{u\in V}d(v,u).
(\#eq:agd)
\end{equation}

从公式 \@ref(eq:agd)可以看出，靠近图中心的顶点可能具有较低的函数值，而位于外围的点可能具有较高的函数值。这一观察结果已被用于研究图的对称性[@KimLipmanChen2010]，并为 MOG 池化策略选择 AGD 提供了理由。图 \@ref(fig:pooling-examples) 展示了在 SHREC11 数据集上使用 带AGD的MOG 池化策略的几个示例。

```{r pooling-examples, echo=FALSE, fig.align="center", fig.cap="在数据集SHREC11上[@lian2011shape]使用MOG算法的示例。 在每幅图中，我们左边显示的是原始网格图，右边显示的是映射图。MOG 算法选择的标量函数是平均测地距离（AGD）。我们观察到，池化后的映射图与原始图的整体形状相似."}
knitr::include_graphics('figures/pooling_examples.png', dpi=NA)
```

为了证明我们的 MOG 池化方法的有效性，我们在 SHREC11 数据集上进行了三项实验： 基于输入顶点和边缘特征的 CC-pooing网格分类（参见第 \@ref(mesh-classification-cc-pooling-with-input-vertex-and-edge-features)节）、基于仅输入顶点特征的 CC-pooing的网格分类（参见第\@ref(mesh-classification-cc-pooling-with-input-vertex-features-only)节）和基于仅输入顶点特征的 CC-pooling点云分类（第 \@ref(mesh-classification-cc-pooling-with-input-vertex-features-only)节）、 以及基于仅输入顶点特征的 CC 汇集的点云分类（参见第\@ref(point-cloud-classification-cc-pooling-with-input-vertex-features-only）节）。在\@ref(mesh-classification-cc-pooling-with-input-vertex-and-edge-features)和 \@ref(mesh-classification-cc-pooling-with-input-vertex-features-only) 节中的实验利用了SHREC11数据集中的网格结构，而 \@ref(point-cloud-classification-cc-pooling-with-input-vertex-features-only) 节中的实验则利用了自己的点云版本。特别是，我们选择了两个简单的 CCNN 架构，如图 \@ref(fig:mesh-net)(d)所示，分别用 $\mbox{CCNN}_{MOG1}$ 和 $\mbox{CCNN}_{MOG2}$ 表示，而不是图  Figure \@ref(fig:mesh-net)(b)中更为复杂的 $\mbox{CCNN}_{SHREC}$ 架构。$\mbox{CCNN}_{MOG1}$ 和 $\mbox{CCNN}_{MOG2}$ 之间的主要区别在于输入特征向量的选择，这将在下文中介绍。

### 网格分类{#mesh-classification-cc-pooling-with-input-vertex-and-edge-features}

在本实验中，我们将顶点特征向量视为底层网格中每个顶点的位置与法向量的连接。对于边缘特征，我们计算了 1-Hodge Laplacian [@dodziuk1976finite; @eckmann1944harmonische]的前十个特征向量，并为底层网格的边缘附加了一个 10 维特征向量。我们在此考虑的 CC 是三维的，因为它由三角形网格（顶点、边和面）和 3-cells组成。3-cells通过 MOG 算法获得，用于增强每个网格。 我们使用 AGD 标量函数作为输入，通过 MOG 算法计算 3-cells。 我们使用通过张量图$\mbox{CCNN}_{MOG1}$定义的 CCNN 进行了这项实验，张量图见图 \@ref(fig:mesh-net)(d)。在训练过程中，我们为每个网格添加了 10 个附加网格，每个附加网格都是通过随机旋转以及对顶点位置进行 0.1% 的噪声扰动获得的。我们使用 0.0002 的学习率和标准交叉熵损失对 $\mbox{CCNN}_{MOG1}$ 进行了 100 次训练，获得了 98.1% 的准确率。虽然 $\mbox{CCNN}_{MOG1}$ 的准确率低于我们在表\@ref(tab:shrec)中报告的 $\mbox{CCNN}_{SHREC}$ 的准确率（99. 17%），但我们注意到，$\mbox{CCNN}_{MOG1}$ 要达到类似的准确率，网格增强所需的重复次数要少得多（$\mbox{CCNN}_{MOG1}$ 需要 10 次重复，而 $\mbox{CCNN}_{SHREC}$ 需要 30 次重复）。


**$\mbox{CCNN}_{MOG1}$架构**. 图\@ref(fig:mesh-net)(d)中的张量图$\mbox{CCNN}_{MOG1}$对应于池化CCNN。 特别是，$\mbox{CCNN}_{MOG1}$ 将信号推向两个不同的高阶胞腔：网格面以及从 MOG 算法中获得的 3-cells。

### 网格分类：仅带输入顶点特征的CC-pooling{#mesh-classification-cc-pooling-with-input-vertex-features-only}

在本实验中，我们考虑输入顶点的位置和范数向量。
我们所考虑的 CC 结构是从每个网格中获得的底层图结构；也就是说，我们只使用顶点和边，而忽略面。我们使用 AGD 标量函数作为输入，通过 MOG 算法获得的 2-cells来增强这一结构。我们选择了比 $\mbox{CCNN}_{MOG1}$ 相对简单的网络结构，并在图 \@ref(fig:mesh-net)(d)中报告为 $\mbox{CCNN}_{MOG2}$。在训练过程中，我们为每个网格添加了 10 个额外的网格，每个额外的网格都是通过随机旋转以及对顶点位置进行 0.05% 的噪声扰动获得的。我们使用 0.0003 的学习率和标准交叉熵损失对 $\mbox{CCNN}_{MOG2}$ 进行了 100 次训练，并获得了 97.1% 的准确率。

**用于网格分类的$\mbox{CCNN}_{MOG2}$架构**. 图 \@ref(fig:mesh-net)(d) 中的张量图 $\mbox{CCNN}_{MOG2}$ 对应的是池化 CCNN。尤其，$\mbox{CCNN}_{MOG2}$ 将信号推向从 MOG 算法中获得的单个 2-cell。请注意，$\mbox{CCNN}_{MOG2}$ 的整体架构在原理上与 AlexNet [@krizhevsky2017imagenet] 相似，即卷积层之后是池化层。

### 点云分类：仅带输入顶点特征得CC-pooling{#point-cloud-classification-cc-pooling-with-input-vertex-features-only}

在本实验中，我们考虑在 SHREC11 数据集上进行点云分类。实验设置与第\@ref(mesh-classification-cc-pooling-with-input-vertex-features-only) 节中研究的原理类似，我们只将点云顶点上支持的特征作为输入。 具体来说，对于 SHREC11 数据集中的每个网格，我们从网格表面采样 1000 个点。此外，我们还使用点云工具包（Point Cloud Utils package）[@point-cloud-utils]估算了所得点云的法向量（normal vector）。 为了构建 CC 结构，我们首先考虑从每个点云中获取的 $k$ 最近邻居图（ 设置$k=7$）。然后，我们使用 AGD 标量函数作为输入，通过 MOG 算法获得的 2-cells对该图进行扩充。我们训练的 $\mbox{CCNN}_{MOG2}$ 如图 \@ref(fig:mesh-net)(d)所示在训练过程中，我们为每个点云添加了 12 个额外的实例，每个实例都是通过随机旋转获得的。我们使用 0.0003 的学习率和标准交叉熵损失对 $mbox{CCNN}_{MOG2}$ 进行了 100 次训练，并获得了 95.2% 的准确率（见表\@ref(tab:shrec)）。

## 消融实验{#ablation-studies}

在本节中，我们将进行两项消融研究。第一项消融研究表明，CCNN 中的池化策略对预测性能有至关重要的影响。第二项消融研究表明，CCNN 比 GNN 具有更好的预测能力；CCNN 的优势来自其拓扑池化操作以及从拓扑特征中学习的能力。

**CCNNs中得池化策略**. 为了评估池化策略的选择对预测性能的影响，我们使用 SHREC11 分类数据集试验了两种池化策略。 第一种池化策略是第\@ref(pooling-with-mapper-on-graphs-and-data-classification)节中描述的 MOG 算法；第 \@ref(mesh-classification-cc-pooling-with-input-vertex-features-only) 节讨论了基于 $\mbox{CCNN}_{MOG2}$ 的这种池化策略的结果（97.1%）。 第二种池化策略简述如下。对于每个网格，我们考虑将每个 1-hop邻域视为 CC 中的 1-cells，将每个 2-hop邻域视为 CC 中的 2-cells，从而得到 2 维 CC。我们训练了 $\mbox{CCNN}_{MOG2}$，得到了 89.2% 的准确率，但是低于 97.1%。这些实验表明，池化策略的选择对预测性能有着至关重要的影响。

**比较 CCNN 与 GNN 的预测性能**. 请注意，$\mbox{CCNN}_{SHREC}$ 的输入是一维和二维拓扑特征。 另一方面，$\mbox{CCNN}_{MOG2}$ 的输入只有顶点特征，但它通过使用前推操作，将信号从 0-cells推向从 MOG 算法中获得的 2-cells，从而学习高阶胞腔潜在特征。 在这两种情况下，使用高阶结构对提高预测性能都至关重要，尽管在利用高阶结构方面采用了两种不同的策略。 为了支持我们的说法，我们进行了一项实验，用$A_{0,1}$诱导的共链算子替换$\mbox{CCNN}_{MOG2}$中的池化操作层，从而有效地将神经网络转化为 GNN。在这种情况下，使用与实验 \@ref(mesh-classification-cc-pooling-with-input-vertex-features-only)相同的设置，我们获得了 84.56% 的准确率。这个实验揭示了采用高阶结构的性能优势，可以利用高阶胞腔支持的输入拓扑特征，也可以通过增强高阶胞腔的池化策略。
