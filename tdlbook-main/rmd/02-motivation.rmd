---
output:
  pdf_document: default
  html_document: default
---
# 研究动机{#motivation}

本文介绍的组合复形神经网络（CCNNs）通过拓扑结构推广了图神经网络（GNNs）及其高阶类似物。在本节中，我们将从三个角度介绍在机器学习，特别是深度学习中使用拓扑结构的动机。第一，*数据建模(* data modeling)*：拓扑抽象如何帮助我们推理和计算拓扑空间上支持的各种数据类型；第二，*实用性(*utility)*：怎了利用拓扑结构来提高性能。第三，*统一性（unification）*：拓扑如何用于综合和抽象不同的概念。

## 从拓扑空间数据中建模和学习

在机器学习上下文中，域上支持的典型数据类型是线性或者说向量空间。这种底层向量空间为许多计算提供了便利，并且经常被隐含地假定。然而，正如几何深度学习所认识到的那样，考虑不同领域支持的数据往往至关重要。事实上，出于各种原因，明确考虑域所支持的数据并对其进行建模至关重要。

首先，不同的域可能具有不同的特征和属性，这就需要不同类型的深度学习模型来有效处理和分析数据。例如，图域可能需要图卷积网络 [@kipf2016semi] 来考虑非欧结构，而点云域则可能需要类似 PointNet 的架构 [@qi2017pointnet; @zaheer2017deep]来处理无序的点集。

其次， 每个域中的具体数据在大小、复杂性和噪声方面都会有很大差异。通过了解特定域内的特定数据属性，研究人员可以开发出针对这些特定属性的模型。例如，针对高噪声点云设计的模型可能会采用离群点去除或局部特征聚合等技术。

第三，准确区分领域和数据对于开发能够很好地概括新的、未见过的数据的模型非常重要。通过识别域的底层结构，研究人员可以开发出既能适应数据变化，又能捕捉相关几何特征的模型。总之，通过同时考虑域和数据，研究人员可以开发出更适合处理不同几何学习任务的特定挑战和复杂性的模型。

**数据的不同视角：域（domains）和关系（relations）**。文献中提到拓扑数据或拓扑数据分析时，对于实际数据包括什么以及数据应该建模什么存在不同观点。在此，我们遵循[@bick2021higher]的观点，区分不同类型的数据和我们学习过程的目标，尽管这些不同类型数据之间的区别有时可能比这里介绍的更加模糊。

*关系数据（Relational data）*，用于描述不同实体或对象间的关系，这一基本概念有多种表现形式，例如社交网络中用户之间的联系，朋友关系或追随者关系，就是这种关系的例子。传统上，人们通过图来理解这些关系，图中的顶点和边分别代表实体和实体之间的成对连接（*pairwise connections*）。换句话说，图中的边被视为度量单位，每条关系数据都涉及两个顶点。

然而，现实世界中的许多现象自然涉及复杂的多方互动，涉及不止两个实体以错综复杂的方式相互影响。例如，社会中的个人群体、合著者集合、相互影响的基因或蛋白质都是这种高阶关系的例子。这种依赖关系超越了成对的相互作用，可以用高阶关系（*higher-order relations*）来建模，并恰当地抽象为超图、胞腔复形或 CCs。显然，拓扑学思想可以在理解这些数据方面发挥重要作用，关键是能让我们从模拟成对关系的图转向更丰富的表征。

*数据作为域的实例*，与TDA 采用的相关概念类似，即我们的观测数据应该代表什么。也就是说，所有观测到的数据都应该对应于拓扑对象本身的噪声实例。例如，我们的目标是学习数据的 "拓扑形状"。请注意，在这种情况下，我们通常不会直接观察相关数据，而是从观察到的数据中构建关系，然后利用这些关系对观察到的数据集进行分类，对点云数据进行的持久同源性分析就是这种观点的一个主流例子。

*拓扑域上支持的数据类型*，当在拓扑域（例如图）上考察数据时，拓扑化思想起着重要的作用。它可能会是某种特定的动态变化，例如流行病的传播，也可能是顶点上支持的任何类型的其他数据或信号(图上的边缘信号（edage-signals）的情况不经常被考虑，尽管存在例外情况[@schaub2018denoising; @schaub2021signal])。在图信号处理（graph signal processing）的语境下，在图域上可以定义多种函数或信号，并且称他们是*被该域支持*。重要的是，图本身可以任意复杂，典型情况下看作是固定的，观察到的数据不是关系的，而是图的顶点上支持所支持的。

在超越图结构方面更一般的拓扑结构，例如CCs，可支持的数据不仅在顶点和边上，也在其他的高阶实体上，正如图\@ref(fig:support)(a-b)上所展示的。例如，计算机图形学中定义在网格上的向量场（vector fields）就是边和面上通常支持的数据形式，可以很方便地建模为高阶域上支持的数据形式 [@de2016vector]。类似的，类别标签化的数据也可以在给定网格的边和面上提供支持，看图 \@ref(fig:support)(c)中的例子。为了处理这样的数据，就需要再去考虑底层拓扑域上的结构。

```{r support, echo=FALSE, fig.align="center", fig.cap="数据可以在高阶关系上很自然的得到支持. (a): 基于边的向量场（edge-based vector field）. (b): 基于面的向量场（face-based vector field）.（a）和（b）上的向量场都定义在胞腔复形torus结构上^[此URL提供（a）和（b）的交互式可视化[戳这里](https://app.vectary.com/p/5uLAflZj6U2kvACv2kk2tN).]。（c）：类别标签化的拓扑数据可以很自然的在高阶关系上被支持。例如，二维面（2-Face）上的网格分割标签（mesh segmentation labels）可以用不同的颜色来描述（蓝色，绿色，绿松石色，粉色，棕色），以表示马的不同部分（头部，颈部，身体，腿部，尾部）"}
knitr::include_graphics('figures/two_tori_horse.png', dpi=NA)
```

**建模和处理图之外的数据：示例。**图非常适合建模表现出成对相互作用的系统，例如，如图\@ref(fig:cloth)(a)所示，基于粒子的流体模拟可以用图有效地表示，消息传递用于更新流体分子的物理状态。采用这种方法，每个分子都表示为包含流体分子物理状态的顶点，边则将他们连接起来，以进行交互作用的计算[@sanchez2020learning; @shlomi2020graph]。

```{r cloth, echo=FALSE, fig.align="center", fig.cap="在图或高阶网络上处理数据的例子。 (a): 图可用于建模流体动力学中的分子相互作用，其中，顶点表示粒子， 粒子与粒子之间的相互作用通过顶点之间的信息传递来模拟  [@sanchez2020learning; @shlomi2020graph]. (b): 在对弹性（spring）和自碰撞(self-collision)建模时，自然要使用边而不是顶点，这是 这是因为布料的拉伸行为是由沿边缘作用的拉力和压力决定的，而不仅仅是由单个颗粒的位置决定的。 为了模拟多条边之间的相互作用，可以使用多边形面来表示布的局部几何形状,多边形面可提供计算边缘间高阶信息传递的方法"}
knitr::include_graphics('figures/cloth.png', dpi=NA)
```
图不适用于布料仿真等更复杂的系统建模，因为状态变量是和边或三角面等关联的，而非顶点，在这种情况下，就需要高阶消息传递来计算和更新物理状态，如图\@ref(fig:cloth)(b)所示。在自然语言处理中也有类似的挑战，语言表现出了多层次的语法、语义和语境。虽然，GNN可以捕获词汇间基本的语法和语义关系，但是否定、讽刺或挖苦等更复杂的关系可能难以表述 [@girault2017towards]。将高阶关系和层次关系纳入其中，可以为这些关系提供更好的模型，并能更细致、更准确地理解语言。

## 拓扑的有用性{#the-utility-of-topology}

除了为复杂系统建模提供多功能框架外，TDL 模型还具有广泛的用途：它们可以为模型学习推导出有意义的归纳偏差，促进底层域中更大范围内的高效信息传播，增强现有基于图的模型的表达能力，并有可能揭示深度网络本身的关键特征，下文将对此进行介绍。

**建立数据的分层表示**， 虽然利用高阶信息对学习表征很重要，但保留实体间复杂的高阶关系对实现强大而多变的表征也很关键。例如，人们可以通过分层建立关系之间的关系来形成抽象和类比。然而，常用于在各种实体间建立关系推理的基于图的模型[@santoro2017simple; @zhang2020deep; @chen2019graph; @schlichtkrull2018modeling]在建立实体间高阶关系的能力方面却是是有限的。

分层关系推理的需求[@xu2022groupnet]要求所用方法能够捕捉关系之间更深层次的抽象关系，而不仅仅是模拟原始实体之间的关系。高阶网络模型为应对高阶关系推理的挑战提供了一个很有前景的解决方案，如图所示\@ref(fig:hs)。

```{r hs, echo=FALSE, fig.align="center", fig.cap="用高阶网络表示分层数据的示例。黑色箭头表示通过高阶关系扩增图，橙色箭头表示粗略的图提取。(a): 表示抽象实体（黄色顶点）间二元关系（粉色边）的图编码。 (b): 蓝色胞腔表示的高阶关系可看作是原始图中顶点或边之间的关系。 (c): 提取原始图的粗略版本。在粗略图中，顶点代表原始图的高阶关系（蓝色胞腔），边代表这些蓝色胞腔的交集。 (d-e): 重复同样的过程，可以得到原始图的更粗略版本。整个过程对应于分层的高阶关系，即关系之间的关系，从而提取意义和内容 (包括数据形状),这是拓扑数据分析的常见任务[@carlsson2009topology; @dey22]."}
knitr::include_graphics('figures/hs.png', dpi=NA)
```

**通过归纳偏置提高性能**，
拓扑深度学习模型通过提供一个定义明确的过程，以将图形提升到指定的高阶网络，从而提高基于图形的学习任务的预测性能。通过这种方法纳入多节顶点关系可被视为一种归纳偏差，可用于提高预测性能。归纳偏置允许学习算法根据观察到的数据之外的因素，优先选择一种解决方案，而不是另一种解决方案[@mitchell1980need]。图 \@ref(fig:inductivebias) 展示了使用高阶胞腔对图进行增强的两种形式。


```{r inductivebias, echo=FALSE, fig.align="center", fig.cap="中间的图可以用高阶关系来增强，以改进学习任务.在(a)中,在缺失的面上添加了胞腔；[@bodnar2021weisfeiler]考虑了类似的归纳偏置，以改进分子数据的分类。在（b）中，图中的一些顶点添加了单跳邻域；[@feng2019hypergraph]，并使用一种基于将图提升到其相应单跳邻域超图的归纳偏置，以提高基于顶点任务的性能"}
knitr::include_graphics('figures/inductive_bias.png', dpi=NA)
```

**构建基于图的高效远程消息传递系统**，TDL 网络的分层结构有助于高效地构建远距离交互。利用这种结构，定义在域顶点上的信号可以在通过长边路径连接的顶点之间有效传播远距离依赖关系。图 \@ref(fig:ft)说明了通过增加拓扑关系对图进行扩充，并将信号反池化（unpooling）回顶点，从而提升定义在图顶点上的信号的过程。[@hajij2022high] 利用这种池化和反池化操作来构建能够捕捉远距离依赖关系的模型，从而更有效、更自适应地学习底层域上的局部和全局结构。关于基于图的池化的相关工作，我们推荐读者参阅[@su2021hierarchical; @itoh2022multi]。

```{r ft, echo=FALSE, fig.align="center", fig.cap="二元或高阶胞腔之间的消息传递示意图。(a): 使用基于图的消息传递方案时，一些从顶点$x$ 开始的信息需要经过很长的距离，即长边路径，才能到达顶点 $y$。(b): 使用蓝色胞腔表示的高阶胞腔结构，消息可以从顶点提升到高阶胞腔，从而以更少的步骤在顶点 $x$ 和 $y$ 之间来回传播。这种快捷方式可以在属于蓝色胞腔的所有顶点之间，以更少的计算步骤高效地共享信息[@hajij2022high]。"}
knitr::include_graphics('figures/factors_thru.png', dpi=NA)
```

**提高表达能力**，TDL 可以捕捉传统基于图的模型无法捕捉的复杂而微妙的依赖关系。通过允许更丰富的高阶交互，高阶网络可以提高表达能力，并在许多任务中带来卓越的性能[@morris2019weisfeiler; @bodnar2021weisfeiler]。

## 深度学习和结构化计算的统一视角

拓扑学提供了一个框架来泛化科学计算中通常会遇到的众多离散域。这些领域的例子包括图、单纯复形和胞腔复形；见图\@ref(fig:unifying)。在这些域上支持的计算模型可以被视为各种深度学习架构的泛化；也就是说，可以为比较和设计 TDL 架构提供一个统一的框架。此外，通过统一理论了解不同类型 TDL 之间的联系，可以为复杂系统的底层结构和行为提供有价值的见解，因为在复杂系统中自然会出现高阶关系[@hansen2019toward; @hajijcell; @bick2021higher; @majhi2022dynamics]。

```{r unifying, echo=FALSE, fig.align="center", fig.cap="我们的工作引入了组合复形，它是一种高阶网络，可泛化科学计算中通常遇到的大多数离散域。 包括 (a) 序列和图像, (b) 图, (c) 3D形状和单纯复形, (d) 立方体（cubical）和胞腔复形（cellular complexes）, (e) 离散流形（discrete manifolds）, (f) 超图."}
knitr::include_graphics('figures/unifying.png', dpi=NA)
```

**了解深度网络的工作原理**，当代深度神经网络的一个显著特点是，即使拥有大量参数，它们也能很好地泛化未见数据。这与我们以往对统计学习理论的理解相矛盾，促使研究人员寻求新的方法来理解深度神经网络的底层工作原理[@neyshabur2019role]。最近的研究发现，深度神经网络推导出的图拓扑或其相应的训练轨迹（建模为马尔科夫链）与泛化性能表现出很强的相关性 [@birdal2021intrinsic; @dupuis2023generalization]。一个悬而未决的问题是，在支持高阶域的学习架构中，拓扑结构、计算函数和泛化特性之间能在多大程度上存在这种关联。我们相信，通过使用我们提出的拓扑结构（如 CCs 和 CCNNs）来研究这些问题，不仅能深入了解 TDL，还能更广泛地了解深度学习。
