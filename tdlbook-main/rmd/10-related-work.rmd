# 相关工作{#related-work}

拓扑深度学习（TDL）是最近出现的一个新的研究前沿，它处于几何和拓扑机器学习以及网络科学等多个领域的交叉点。为了说明拓扑深度学习在现有文献中的位置，我们回顾了之前的大量工作，并将其分为基于图的模型、高阶深度学习模型、基于图的池化、基于注意力的模型和应用代数拓扑。

## 基于图的模型{#graph-based-models}

基于图的模型已被广泛用于不同系统元素（顶点）之间成对相互作用（边）的建模，包括社会系统（如社会网络分析）和生物系统（如蛋白质-蛋白质相互作用），更多内容参见[@knoke2019social; @jha2022prediction]。 根据图的边或顶点属性，图可以分为无权图（无权边）、有权图（有权边）、有符号图（有符号边）、无向图或有向图（无向边或有向边）以及时空图（时空顶点），详见[@goyal2018graph; @wu2020comprehensive]。这些图类型中的每一种都可以与神经网络相结合，形成图神经网络，并对各种系统中的不同交互作用进行建模 [@goyal2018graph; @wu2020comprehensive]。例如，基于非加权和非定向图的模型已被用于 omic 数据映射[@amar2014constructing]和社交网络中的相互友谊检测[@tabassum2018social]；基于加权图的模型已被广泛应用于交通预测[@halaoui2010smart；@zhang2018kernel]和流行病学建模/预测[@linka2020outbreak； @manriquez2021protection]；基于符号图的模型适用于分割[@bailoni2022gasp]和聚类[@kunegis2010spectral; @gallier2016spectral]等任务；基于时空图的模型可以描述具有时空性质的系统，如人类活动和不同类型的运动[@yan2018spatial; @bhattacharya2020step; @plizzari2021spatial]。

由于利用单层（single-layer 或 monolayer）图的基于图的方法无法模拟网络中顶点之间的多种类型关系[@goyal2018graph; @wu2020comprehensive]，因此有人提出了多层或多重网络[@kivela2014multilayer; @zhang2020multiplex; @chang2022graphrr]。与单层图类似，多重网络包含顶点和边，但边存在于不同的层中，每一层代表一种特定类型的交互或关系。多层网络已被用于各种应用，包括人脑的多层建模 [@de2017multilayer; @anand2023hodge] 和在线游戏 [@chang2022graphrr]。所有这些类型的网络都只能模拟顶点之间的成对关系，因此需要更高阶的网络，这将在第  \@ref(the-utility-of-topology)节中讨论。
## 高阶深度学习模型{#higher-order-deep-learning-models}

近年来，人们对高阶网络越来越感兴趣[@mendel1991tutorial; @battiston2020networks; @bick2021higher]，因为这些网络能够充分捕捉高阶交互。在信号处理和深度学习文献中，已经为高阶网络开发了霍奇理论（Hodge-theoretic）方法、信息传递方案和跳转连接。

单纯复形上的霍奇理论方法[@lim2020hodge]已由[@barbarossa2020topological; @schaub2021signal]引入。这项工作由[@barbarossa2016introduction; @schaub2021signal]扩展到超图，由[@roddenberry2021signal; @sardellitti2021topological]扩展到胞腔复形。@roddenberry2019hodgenet]的工作通过利用1-霍奇拉普拉斯算子（1-Hodge Laplacian operator）进行线性过滤，定义了一种基于边的卷积神经网络[@barbarossa2018learning; @schaub2018denoising; @barbarossa2020topological; @barbarossa2020topologicalmag; @schaub2021signal]。


针对高阶神经网络的卷积算子和消息传递算法已经被开发出来。例如，[@arya2018exploiting；@feng2019hypergraph；@jiang2019dynamic]提出了超图上的卷积算子，[@jiang2019dynamic；@gao2020hypergraph；@bai2021hypergraph；@bai2021multi；@giusti2022cell；@wu2022hypergraph；@gong2023generative]对其进行了进一步研究。 最近，文献[@huang2021unignn]
提出了在图和超图上学习的统一框架。@gao2022hgnn]中的作者介绍了所谓的通用超图神经网络，它构成了一个多模式/多类型数据关联建模框架。至于复形上的消息传递，文献[@hajijcell]的工作引入了一个高阶消息传递框架，其中包含了[@gilmer2017neural; @bunch2020simplicial; @ebli2020simplicial; @hayhoe2022stable]提出的框架，并利用了各种局部邻域聚合方案。  在文献[@mitchell2022topological]中，提出了递归单纯形神经网络，并将其应用于轨迹预测。在[@calmon2022higher]中，作者通过在高阶网络上引入一种利用狄拉克算子（Dirac operator.）的耦合多信号方法，解决了同时处理多个胞腔维度上支持的信号的难题。 最近还介绍了一些单纯形和胞腔神经网络，包括[@burnssimplicial; @bodnar2021weisfeiler; @roddenberry2021signal; @sardellitti2021topological; @sardellitti2022topological; @battiloro2023topological; @yang2023convolutional]。更多详情，请读者参阅 [@mathilde2023] 最近关于 TDL 的研究。

文献 [@hajij2022high]将跨层连接[@ronneberger2015u; @he2016deep]推广到了介绍了单纯复形上，使得可以训练高阶深度神经网络。文献[@morris2019weisfeiler]中提出了一种高阶图神经网络，它考虑到了多种尺度的高阶图结构。虽然这些方法允许多向分层耦合，但耦合是各向同性的，无法学习特定多向连接内的权重差异。但是，基于注意力的模型可以缓解这些限制。

高阶模型在一些现实世界的应用中取得了可喜的性能，包括连接预测（link prediction）[@hajij2022high; @piaggesi2022effective； @chen2021bscnets]、动作识别（action recognition）[@wang2023survey]、视觉分类（visual classification）[@shi2018hypergraph]、最优同源生成器检测（optimal homology generator detection）[@keros2021dist2cycle]、时间序列（time series）[@santoro2023higher]、动力学系统（dynamical systems ）[@majhi2022dynamics]、谱聚类（ spectral clustering）[@reddy2023clustering]、节点分类（node classification）[@hajij2022high]和轨迹预测（trajectory prediction ）[@benson2018simplicial； @roddenberry2021principled]。

## 基于注意力的模型{#attention-based-models}

现实世界的关系数据庞大、非结构化、稀疏且嘈杂。因此，图神经网络（GNN）可能会学习到次优的数据表示，从而表现出尚可接受的性能[@wu2020comprehensive; @asif2021graph; @dai2021nrgnn]。为了解决这些问题，各种注意力机制[@chaudhari2021attentive]已被纳入 GNN，它们允许学习神经架构，以检测给定图形中最相关的部分，同时忽略不相关的部分。根据所使用的注意机制，现有的图注意方法可分为基于权重的注意、基于相似性的注意和基于注意引导的游走 [@boaz2019]。

除了[@bai2021hypergraph；@kim2020hypergraph；@georgiev2022heat；@giusti2022cell；@giusti2022simplicial；@goh2022simplicial]之外，大多数基于注意力的机制都是为图数据而设计的。例如，[@goh2022simplicial]提出的注意力模型是对[@velickovic2017graph]的图注意力模型的推广。在[@giusti2022simplicial]中，作者利用基于霍奇分解（Hodge decomposition）的模型（类似于[@roddenberry2021principled]中提出的模型）提出了单纯复形上的注意力模型。文献[@kim2020hypergraph；@bai2021hypergraph]中引入的超图注意力模型为[@velickovic2017graph]的图注意力模型提供了另一种泛化选择。上述注意模型既不允许也不结合不同维度实体的高阶注意块。这限制了神经架构的空间和现有注意力模型的应用范围。

## 基于图的池化{#graph-based-pooling}

为了在图的背景下模仿基于图像的池化层的成功，已经进行了多次尝试。一些早期工作采用了流行的图聚类算法 [@kushnir2006fast; @dhillon2007weighted] 来实现基于图的池化架构 [@bruna2013spectral]。粗化操作被应用于图，以获得学习任务所需的不变性[@ying2018hierarchical; @mesquita2020rethinking; @gao2021topology]。目前最先进的基于图的池化方法大多依赖于学习任务所需的动态池化学习[@grattarola2022understanding]，这包括谱方法（spectral methods ） [@ma2019graph]、聚类方法（如 DiffPool [@ying2018hierarchical] 和 MinCut [@bianchi2020spectral]）、top-K 方法 [@gao2019graph； @lee2019self; @zhang2021hierarchical] 和分层图池[@huang2019attpool; @lee2019self; @zhang2019hierarchical; @li2020graph; @pang2021graph; @zhang2021hierarchical]。除了文献[@cinque2022pooling]按照[@grattarola2022understanding]提出的思路开发了一种单纯复形池化策略外，对高阶网络上的池化仍未进行研究。

## 代数拓扑的应用{#applied-algebraic-topology}

虽然代数拓扑学[@hatcher2005algebraic]是一个相对古老的领域，但该领域的应用直到最近才开始具体化[@carlsson2009topology; @edelsbrunner2010computational]。 事实上，人们发现拓扑结构是解决许多领域长期存在的问题的天然工具。 例如，持久同源（persistent homology）[@edelsbrunner2010computational]已在各种复杂数据问题的解决中被成功应用[@boyell1963hybrid; @kweon1994extracting; @bajaj1997contour; @attene2003shape； @carr2004simplifying; @LeeChungKang2011; @DabaghianMemoliFrank2012; @LeeChungKang2011b; @nicolau2011topology; @LeeKangChung2012; @LeeKangChung2012b; @lum2013extracting;@giusti2016two;@curto2017can;@rosen2017using]。近年来，人们对拓扑学在机器学习和数据科学中的作用越来越感兴趣[@hensel2021survey; @DW22]。

基于拓扑结构的机器学习模型已被应用于许多领域，包括数据的拓扑签名（topological signatures）[@biasotti2008describing; @carlsson2005persistence; @rieck2015persistent]、神经科学[@LeeChungKang2011； @LeeChungKang2011b; @DabaghianMemoliFrank2012; @LeeKangChung2012; @LeeKangChung2012b; @giusti2016two; @curto2017can], 生物科学 [@dewoskin2010applications; @nicolau2011topology; @chan2013topology; @taylor2015topological； @topaz2015topological; @lo2016modeling]、图的研究[@HorakMaleticRajkovic2009; @ELuYao2012; @BampasidouGentimis2014; @CarstensHoradam2013; @PetriScolamieroDonato2013; @PetriScolamieroDonato2013b； @rieck2019persistent；@hajij2020efficient]、时间序列预测[@zeng_topological_2021]、木马检测[@hu_trigger_2022]、图像分割[@hu_topology-preserving_2019]、三维重建[@waibel_capturing_2022]、
以及时变设置（time-varying setups ） [@edelsbrunner2004time; @perea2015sw1pers; @maletic2016persistent; @rieck_uncovering_2020]。


拓扑数据分析（Topological data analysis ，TDA）[@carlsson2009topology；@edelsbrunner2010computational；@ghrist2014elementary；@love2020topological；@DW22]已成为一个利用拓扑工具分析数据和开发机器学习算法的科学领域。TDA 在机器学习领域有许多应用，包括增强现有的机器学习模型 [@bentaieb2016topology; @hofer2017deep; @clough2019explicit; @bruel2019topology; @wangtopogan； @leventhal2023exploring]、提高深度学习模型的可解释性[@carlsson2020topological; @elhamdadi2021affectivetda; @love2023topological]、降维[@moor2020topological]、过滤学习（ filtration learning ）[@hofer2020graph]和拓扑层构造[@kim2020pllay]。一个值得注意的研究趋势是持久图（persistence diagrams）的矢量化，构建持久图的矢量表示是为了在下游机器学习任务中加以利用。这些方法包括贝蒂曲线 （Betti curves）[@umeda2017time]、持久性景观（persistence landscapes） [@bubenik2015statistical]、持久性图像 （persistence images）[@adams2017persistence]，以及其他矢量化构造 [@chen2015statistical；@kusano2016persistence；@berry2020functional]。最近，文献[@carriere_perslay_2020]也提出了这些方法的一种统一框架。


我们的工作引入了组合复形（combinatorial complexes，CC），将其作为一种广义的高阶网络，在此网络上可以统一的方式定义和研究深度学习模型。因此，我们的工作扩展了 TDA，用拓扑术语形式化了深度学习概念，并用我们的 TDL 框架实现了 TDA 中的构造，如 mapper [@singh2007topological]。CC 和组合复形神经网络（combinatorial complex neural networks，CCNNs，定义在 CC 上的神经网络），其构建受到代数拓扑学[@hatcher2005algebraic]和拓扑量子场论（topological quantum field theory ）[@turaev2016quantum]中经典概念的启发，也受到 了TDA [@collins2004barcode； @carlsson2005persistence; @carlsson2006algebraic; @carlsson2008local; @carlsson2008persistent; @carlsson2009theory; @carlsson2009topology]在机器学习中应用的最新进展的启发[@pun2018persistent; @DW22]。


