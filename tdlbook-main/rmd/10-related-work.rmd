# Related work

Topological deep learning (TDL) has recently emerged as a new research frontier that lies at the intersection of several areas, including geometric and topological machine learning, and network science. To demonstrate where TDL fits in the existing literature, we review a broad spectrum of prior works, and categorize them into graph-based models, higher-order deep learning models, graph-based pooling, attention-based models, and applied algebraic topology.

## Graph-based models

Graph-based models have been widely used for modeling pairwise interactions (edges) between elements (vertices) of different systems, including social systems (e.g., social network analysis) and biological systems (e.g., protein-protein interactions), see [@knoke2019social; @jha2022prediction]. Based on their edge or vertex properties, graphs can be classified as unweighted graphs (unweighted edges), weighted graphs (weighted edges), signed graphs (signed edges), undirected or directed graphs (undirected or directed edges), and spatio-temporal graphs (spatio-temporal vertices), as discussed in [@goyal2018graph; @wu2020comprehensive]. Each of these graph types can be combined with neural networks to form graph neural networks and model different interactions in various systems [@goyal2018graph; @wu2020comprehensive]. For example, unweighted and undirected graph-based models have been used for omic data mapping [@amar2014constructing] and mutual friendship detection in social networks [@tabassum2018social]; weighted graph-based models have been widely used with systems related to traffic forecasting [@halaoui2010smart; @zhang2018kernel] and epidemiological modeling/forecasting [@linka2020outbreak; @manriquez2021protection]; signed graph-based models are suitable for tasks such as segmentation [@bailoni2022gasp] and clustering [@kunegis2010spectral; @gallier2016spectral]; spatio-temporal graph-based models can describe systems that are spatio-temporal in nature, such as human activity and different types of motion [@yan2018spatial; @bhattacharya2020step; @plizzari2021spatial].

As graph-based approaches that utilize single-layer or monolayer graphs cannot model multiple types of relations between vertices in a network [@goyal2018graph; @wu2020comprehensive], multilayer or multiplex networks have been proposed [@kivela2014multilayer; @zhang2020multiplex; @chang2022graphrr]. Similar to monolayer graphs, multiplex networks contain vertices and edges, but the edges exist in separate layers, where each layer represents a specific type of interaction or relation. Multiplex networks have been used in various applications, including multilayer modeling of the human brain [@de2017multilayer; @anand2023hodge] and online gaming [@chang2022graphrr]. All these types of networks can only model pairwise relations between vertices, motivating the need for higher-order networks, as discussed in Section \@ref(the-utility-of-topology).

## Higher-order deep learning models

In recent years, there has been an increasing interest in higher-order networks [@mendel1991tutorial; @battiston2020networks; @bick2021higher] due to the ability of these networks to adequately capture higher-order interactions. Hodge-theoretic approaches, message passing schemes, and skip connections have been developed for higher-order networks in the signal processing and deep learning literature.

A Hodge-theoretic approach [@lim2020hodge] over simplicial complexes has been introduced by [@barbarossa2020topological; @schaub2021signal]. This effort has been extended to hypergraphs by [@barbarossa2016introduction; @schaub2021signal] and to cell complexes by [@roddenberry2021signal; @sardellitti2021topological]. The work of [@roddenberry2019hodgenet] has defined an edge-based convolutional neural network by exploiting the 1-Hodge Laplacian operator for linear filtering [@barbarossa2018learning; @schaub2018denoising; @barbarossa2020topological; @barbarossa2020topologicalmag; @schaub2021signal].

Convolutional operators and message-passing algorithms have been developed for higher-order neural networks. For example, a convolutional operator on hypergraphs has been proposed by [@arya2018exploiting; @feng2019hypergraph; @jiang2019dynamic] and has been investigated further by [@jiang2019dynamic; @gao2020hypergraph; @bai2021hypergraph; @bai2021multi; @giusti2022cell; @wu2022hypergraph; @gong2023generative]. A unifying framework for learning on graphs and hypergraphs has been proposed recently in [@huang2021unignn]. The authors in [@gao2022hgnn] have introduced the so-called general hypergraph neural networks, which constitute a multi-modal/multi-type data correlation modeling framework. As for message passing on complexes, the work of [@hajijcell] has introduced a higher-order message-passing framework that encompasses those proposed by [@gilmer2017neural; @bunch2020simplicial; @ebli2020simplicial; @hayhoe2022stable] and has utilized various local neighborhood aggregation schemes. In [@mitchell2022topological], recurrent simplicial neural networks have been proposed and applied to trajectory prediction. The authors in [@calmon2022higher] have addressed the challenge of processing signals supported on multiple cell dimensions concurrently, by introducing a coupling multi-signal approach on higher-order networks that utilizes the Dirac operator.  Several simplicial and cellular neural networks have been introduced recently, including [@burnssimplicial; @bodnar2021weisfeiler; @roddenberry2021signal; @sardellitti2021topological; @sardellitti2022topological; @battiloro2023topological; @yang2023convolutional]. For more details, the reader is referred to the recent survey of [@mathilde2023] on TDL.

A generalization of skip connections [@ronneberger2015u; @he2016deep] to simplicial complexes has been introduced by [@hajij2022high], which allows the training of higher-order deep neural networks. The authors in [@morris2019weisfeiler] have proposed a higher-order graph neural network that takes into account higher-order graph structures at multiple scales. While these methods allow for multi-way hierarchical coupling, the coupling is isotropic and weight differences within a particular multi-way connection can not be learned. These limitations can be alleviated by attention-based models.

Higher-order models have achieved promising performance in several real-world applications, including link prediction [@hajij2022high; @piaggesi2022effective; @chen2021bscnets], action recognition [@wang2023survey], visual classification [@shi2018hypergraph], optimal homology generator detection [@keros2021dist2cycle], time series [@santoro2023higher], dynamical systems [@majhi2022dynamics], spectral clustering [@reddy2023clustering], node classification [@hajij2022high], and trajectory prediction [@benson2018simplicial; @roddenberry2021principled].

## Attention-based models

Real-world relational data is large, unstructured, sparse and noisy. As a result, graph neural networks (GNNs) may learn suboptimal data representations, and therefore may exhibit compromised performance [@wu2020comprehensive; @asif2021graph; @dai2021nrgnn]. To address these issues, various attention mechanisms [@chaudhari2021attentive] have been incorporated in GNNs, which allow to learn neural architectures that detect the most relevant parts of a given graph while ignoring irrelevant parts. Based on the used attention mechanism, existing graph attention approaches can be divided into weight-based attention, similarity-based attention, and attention-guided walk [@boaz2019].

The majority of attention-based mechanisms, with the exception of [@bai2021hypergraph; @kim2020hypergraph; @georgiev2022heat; @giusti2022cell; @giusti2022simplicial; @goh2022simplicial], are designed for graphs. For example, the attention model proposed by [@goh2022simplicial] is a  generalization of the graph attention model of [@velickovic2017graph]. In [@giusti2022simplicial], the authors have utilized a model based on Hodge decomposition, similar to the one suggested in [@roddenberry2021principled], to introduce an attention model for simplicial complexes. The hypergraph attention models introduced in [@kim2020hypergraph; @bai2021hypergraph] provide alternative generalizations of the graph attention model of [@velickovic2017graph]. The aforementioned attention models neither allow nor combine higher-order attention blocks of entities of different dimensions. This limits the space of neural architectures and the scope of applications of existing attention models.

## Graph-based pooling

Several attempts have been made to emulate the success of image-based pooling layers in the context of graphs. Some of the early work employs popular graph clustering algorithms [@kushnir2006fast; @dhillon2007weighted] to achieve graph-based pooling architectures [@bruna2013spectral]. Coarsening operations have been applied to graphs
to attain the invariance properties needed in learning tasks [@ying2018hierarchical; @mesquita2020rethinking; @gao2021topology]. The current state-of-the-art graph-based pooling approaches mostly rely on dynamically learning the pooling needed for the learning task [@grattarola2022understanding]. This includes spectral methods [@ma2019graph], clustering methods such as DiffPool [@ying2018hierarchical] and MinCut [@bianchi2020spectral], top-K methods [@gao2019graph; @lee2019self; @zhang2021hierarchical], and hierarchical graph pooling [@huang2019attpool; @lee2019self; @zhang2019hierarchical; @li2020graph; @pang2021graph; @zhang2021hierarchical]. Pooling on higher-order networks remains unstudied, with the exception of a general simplicial complex pooling strategy developed by [@cinque2022pooling] along the lines of the proposal made by [@grattarola2022understanding].

## Applied algebraic topology

Although algebraic topology [@hatcher2005algebraic] is a relatively old field, applications of this field have only recently started to crystallize [@carlsson2009topology; @edelsbrunner2010computational]. Indeed, topological constructions have been found to be natural tools for the formulation of longstanding problems in many fields. For instance, persistent homology [@edelsbrunner2010computational] has been successful at finding solutions to various complex data problems [@boyell1963hybrid; @kweon1994extracting; @bajaj1997contour; @attene2003shape; @carr2004simplifying; @LeeChungKang2011; @DabaghianMemoliFrank2012; @LeeChungKang2011b; @nicolau2011topology; @LeeKangChung2012; @LeeKangChung2012b; @lum2013extracting; @giusti2016two; @curto2017can; @rosen2017using]. Recent years have witnessed increased interest in the role of topology in machine learning and data science [@hensel2021survey; @DW22].
Topology-based machine learning models have been applied in many areas, including topological signatures of data [@biasotti2008describing; @carlsson2005persistence; @rieck2015persistent], neuroscience [@LeeChungKang2011; @LeeChungKang2011b; @DabaghianMemoliFrank2012; @LeeKangChung2012; @LeeKangChung2012b; @giusti2016two; @curto2017can], bioscience [@dewoskin2010applications; @nicolau2011topology; @chan2013topology; @taylor2015topological; @topaz2015topological; @lo2016modeling], the study of graphs [@HorakMaleticRajkovic2009; @ELuYao2012; @BampasidouGentimis2014; @CarstensHoradam2013; @PetriScolamieroDonato2013; @PetriScolamieroDonato2013b; @rieck2019persistent; @hajij2020efficient], time series forcasting [@zeng_topological_2021], Trojan detection [@hu_trigger_2022], image segmentation [@hu_topology-preserving_2019], 3D reconstruction [@waibel_capturing_2022],
and time-varying setups [@edelsbrunner2004time; @perea2015sw1pers; @maletic2016persistent; @rieck_uncovering_2020].

Topological data analysis (TDA) [@carlsson2009topology; @edelsbrunner2010computational; @ghrist2014elementary; @love2020topological; @DW22] has emerged as a scientific area that harnesses topological tools to analyze data and develop machine learning algorithms. TDA has found many applications in machine learning, including enhancing existing machine learning models [@bentaieb2016topology; @hofer2017deep; @clough2019explicit; @bruel2019topology; @wangtopogan; @leventhal2023exploring], improving the explainability of deep learning models [@carlsson2020topological; @elhamdadi2021affectivetda; @love2023topological], dimensionality reduction [@moor2020topological], filtration learning [@hofer2020graph], and topological layers constructions [@kim2020pllay]. A notable research trend has been the vectorization of persistence diagrams. Vector representations of persistence diagrams are constructed in order to be utilized in downstream machine learning tasks. These methods include Betti curves [@umeda2017time], persistence landscapes [@bubenik2015statistical], persistence images [@adams2017persistence], and other vectorization constructions [@chen2015statistical; @kusano2016persistence; @berry2020functional]. A unification of these methods has been proposed recently in [@carriere_perslay_2020].

Our work introduces combinatorial complexes (CCs) as a generalized higher-order network on which deep learning models can be defined and studied in a unifying manner. Hence, our work expands TDA by formalizing deep learning notions in topological terms and by realizing constructions in TDA, e.g., mapper [@singh2007topological], in terms of our TDL framework. The construction of CCs and of combinatorial complex neural networks (CCNNs), which are neural networks defined on CCs, is inspired by classical notions in algebraic topology [@hatcher2005algebraic] and in topological quantum field theory [@turaev2016quantum], and by recent advances in TDA [@collins2004barcode; @carlsson2005persistence; @carlsson2006algebraic; @carlsson2008local; @carlsson2008persistent; @carlsson2009theory; @carlsson2009topology] as applied to machine learning [@pun2018persistent; @DW22].
